[

{
"title"    : "MySQL学习笔记(十)：count",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/10/22/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%8D%81)-count/",
"date"     : "2021-10-22 00:00:00 +0800",
"content"  : "count(*) 的实现方式不同引擎实现方式不同：      MyISAM：将总行数直接存在磁盘上，查询的时候返回，因此速度很快        Innodb：因为需要支持事务，而事务是由 MVCC 实现的，一行记录需要先判断对查询事务的可见性，所以“应该返回多少行” 是不确定的。因此Innodb的实现方式是把数据一行一行的从引擎中读出来，可见的行才会被累计进来。    举个例子，假设表 t 有 10000 条数据：                            sessionA          sessionB          sessionC                                      begin;                                                select count(*) from t;                                                                      insert into t(插入一行);                                     begin;                                                insert into t(插入一行);                                     select count(*) from t; (返回10000)          select count(*) from t; (返回10002)          select count(*) from t; (返回10001)                      可见同一时刻的并发查询，返回的结果是不一致的。    由于需要遍历全表，因此对于大表，count 会变得很慢。    为 count 提速用缓存保存计数这是第一时间会想到的方式，比如使用 redis。更新数据时顺便更新 redis中的计数值，count 的时候直接查询 redis 即可，速度很快。但是这会有2个问题：      缓存异常宕机会丢失更新          redis持久化有RDB和AOF两种方式；RDB按照备份策略，比如60秒1000个k-v被修改，备份过程中宕机，那么这个阶段的所有更新都会丢失；AOF按照备份策略，比如 appendfsync always 策略，同步记录所执行的指令到日志文件，但是它的日志和mysql的WAL(先写日志)不同，它是后写日志，可能指令执行后写日之前宕机，那这个数据就丢失了，虽然丢失数据较少且概率较低，但依然存在这个可能。        可以在重启后到数据库中执行一次 count(*)获取行数后填到 redis 中，毕竟异常宕机重启不会经常发生，偶尔一次全表扫描的成本还是可以接受的。        结果可能不准    由于更新数据和更新 redis 统计计数不是一个原子操作，可能会出现统计计数和数据不一致的问题。    比如以下场景：                            sessionA          sessionB                                      redis 计数+1                                                读 redis 计数                                     查询近100条记录                          插入一行数据                                 sessionB 在查询计数时已经加了1，但是查不到新插入的数据(将sessionA的两个操作调换顺序也是类似的)。究其原因， redis 和数据库可以看做两个不同的数据源，不能保证两个操作的原子性(不可分割)。这类问题属于分布式一致性问题，虽可通过引入其他手段解决，但会使一个简单的计数查询需求变得很复杂，没有必要。  在数据库保存计数那不存到 redis  中，存到数据库计数表中，会出现上面的问题吗？  首先，Innodb 支持崩溃恢复，所以不存在更新丢失的问题。  将上图中 redis  的操作换成对数据库计数表的操作，不会出现数据不一致的问题。很明显，Innodb的事务保证了操作的原子性。所以，对于大表，推荐使用数据库计数表来提速 count 。进一步优化根据 两阶段锁 协议，可以通过调整事务内更新语句的顺序：将 update 计数表放到最后，来减少计数表行锁等待的时间，提高并发度。  不同 count 用法的区别首先，count 是一个聚合函数，它的逻辑是，对于返回的结果集，一行一行的判断，不为 NULL 就+1。      count(主键id)    遍历整张表，把每一行的 id 取出来返回给 server 层，server 层判断 id 不可能为空，+1。          这里可能会觉得奇怪，id 根本不可能为空，为什么还要多此一举去判断一下。      的确是没什么必要，但类似需要优化的地方太多了，MySQL专门对 count(*) 优化过了，直接使用 count(*)  就好了。            count(1)    遍历整张表，但不取值。Server 层对于返回的每一行，放一个 “1“ 进去，判断不可能为空，+1。          很明显，相较于 count(主键id)，count(1) 效率更高，因为 count(id)  还需要解析数据行、拷贝字段值等操作。            count(字段)    遍历整张表，把每一行对应的字段值取出来返回给 server 层，server 层判断是否为空，不为空+1。所以 count(字段) ≤ count(*)。        count(*)    做了专门的优化，并不会把全部字段取出来，而是直接不取值，认定count(*) 肯定不为空，直接按行累加。  按效率排序：count(字段) lt; count(主键id) lt; count(1) ≈ count(*)。推荐直接使用 count(*)。"
} ,

{
"title"    : "MySQL学习笔记(九)：如何收缩表空间?",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/10/20/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B9%9D)-%E5%A6%82%E4%BD%95%E6%94%B6%E7%BC%A9%E8%A1%A8%E7%A9%BA%E9%97%B4/",
"date"     : "2021-10-20 00:00:00 +0800",
"content"  : "使用 delete 删除数据不会使表空间变小前文有提过，delete 只是将记录标记为删除，并没有真正删除。只有当其对应的 update undo logs 被清除时才会由后台 purge 任务物理删除，即没有事务再需要这些版本的记录时会执行物理删除。即便是物理删除，也不会把磁盘空间返还给操作系统。究其原因，出于性能考虑，物理删除后的空间会被复用。删除记录后，当插入符合范围条件的数据时，原空间会被复用。删除一整个数据页上的所有记录后，当需要使用新页的时候，原数据页会被复用。可通过 show table status like &#39;t&#39; 来查看表碎片大小，如果过大，可通过重建表来释放表空间。  几个字段的含义，都是针对 Innodb，MyISAM略有不同：      data_length：聚簇索引大小近似值，单位是字节。    index_length：非聚簇索引大小近似值，单位是字节。    data_free：已分配但未使用的空间大小近似值，单位是字节。表碎片大小即是查看这个字段。    这三个值之和近似接近 ibd 文件大小。重建表不光删除数据会造成空洞(可被复用但没被使用的空间)，插入和更新也会。因为插入数据往往都是随机的，即基本不可能按索引递增顺序插入，就很有可能造成数据页的分裂。比如当一个数据页满了，此时再插入一行数据到此节点就会造成数据页分裂，原数据页末尾就会产生空洞。更新可理解为先删除后插入，同理。重建表就是通过去掉这些空洞，来达到收缩表空间的目的。命令如下：1alter table A engine=InnoDB5.6版本之前内部流程如下：  新建一张与原表结构相同的临时表  按主键ID递增的顺序，将数据一行一行的从原表读出来再插入到临时表中。这一步就可以去掉原表主键索引上的空洞。  用临时表替换掉原表这个流程有个问题，在第2步中，原表不能有更新操作。也就是说，这个ddl不是online的。从5.6版本开始  建立一个临时文件，扫描原表的所有数据页  用数据页中原表的记录生成B+树，存储到临时文件中  在生成临时文件的过程中，将对原表的操作记录在一个日志文件(row log)中  临时文件生成后，将日志文件中的操作应用到临时文件  用临时文件替换掉原表的数据文件很明显，在该过程执行中是允许对原表做增删改操作的，这也是 Online DDL 名字的由来。注意：对于大表来说，因为需要扫描原表数据和构建临时文件，这个步骤是很消耗IO和CPU资源的。尤其对于线上业务，要很小心的控制操作时间。如果想要安全操作的话，推荐使用 github 的 gh-ost。online 和 inplaceonline ddl 构建的临时文件位于 Innodb 内部，整个过程也都是在 Innodb 内部完成。对于 Server 层来说，没有把数据挪动到临时表(5.6版本之前的操作流程)，相当于是一个 “原地” 操作，因此叫做 inplace。12345alter table A engine=InnoDB;其实隐含的意思是alter table A engine=InnoDB,ALGORITHM=inplace;相对的，就有1alter table A engine=InnoDB,ALGORITHM=copy;对应的就是5.6版本之前的操作流程。所以，如果现在有一个1TB的表，磁盘空间为1.2TB，能不能做一个inplace的ddl呢？答案是不能，因为临时文件也是要占用空间的。总结来说，online 是指在操作过程会不会阻塞对原表的增删改操作。inplace 指的是在 Serve 层建临时表还是直接在存储引擎内建临时文件。两种重建表的方式及区别      alter table A engine=InnoDB    如上文所述        optimize table A    等同于 alter table A engine=InnoDB + analyze table A (analyze：重新统计索引信息)  什么情况下重建表空间反而变大了？      表本身已经没有空洞了，比如刚刚重建过一次，这时候再重建，如果恰好在重建期间有外部的 DML 在执行，就有可能会引入新的空洞。        而且重建表的时候并不是把每个数据页都占满，而是会留下 1/16 的预留空间给后续的更新用，也就是说重建后的表并不是百分百紧凑的。    以下过程就可能会出现这种情况：          重建一次      插入一部分数据，但是这部分数据使用的是预留空间      再重建一次。        这时由于预留空间被使用了，再次重建时就需要再额外留出 1/16 的预留空间，所以空间反而变大了。  "
} ,

{
"title"    : "MySQL学习笔记(八)：MySQL锁",
"category" : "",
"tags"     : "mysql, 锁",
"url"      : "/blog/2021/10/18/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%85%AB)-MySQL%E9%94%81/",
"date"     : "2021-10-18 00:00:00 +0800",
"content"  : "MySQL中的锁有三类：全局锁、表级锁、行级锁全局锁即给整个数据库实例上锁，让整个库都处于只读状态，除查询以外的操作都会被阻塞。server层实现。  如果加上全局锁后，客户端由于异常断开，MySQL会自动释放这个锁。加锁：1flush tables with read lock;解锁：1unlock tables;使用场景：做全库逻辑备份隐患：  由于只能查询，所以在此期间业务基本停摆  如果在主库上备份，业务停摆；如果在备库上备份，在此期间备库不能执行从主库同步过来的 binlog，会导致主从延迟那如果备份的时候不加全局锁会发生什么情况呢？不加锁，会导致备份出来的库不是同一个逻辑时间点的，数据从业务逻辑上看是不一致的。比如在备份过程中先备份了A表，然后执行了一个业务操作，再备份B表，这个业务操作会同时更新A表和B表。那么这个时候备份出来的数据A表还是老版本，而B表已经被更新了，这个备份就是有问题的，是逻辑不一致的。为了既不影响业务，也要保证备份视图的逻辑一致性，推荐采用另一种全库备份的方法：mysqldump -single-transaction。导数据之前会启动一个事务，来确保拿到一致性视图。而且由于 MVCC 的支持，在此期间是可以正常更新的。当然显而易见，这种方法只适用于支持事务的存储引擎，所以这也是为什么推荐使用 Innodb 而不是 MyISAM 的一个原因。表级锁server层实现，分两种：表锁和元数据锁(MDL)。表锁  与全局锁一样，也会在客户端断开时自动释放加锁：1lock tables t1 read,t2 write;解锁：1unlock tables;以上述加锁语句为例，t1加了读锁，t2加了写锁：  任何线程都不能写 t1 ，包括本线程  只有本线程能读写 t2  本线程甚至不能访问除 t1、t2 之外的任何表，这点很奇怪，不懂为什么这么设计元数据锁Meta data lock，MDL不需要显示使用，访问表的时候会自动加上。从MySQL5.5开始引入，当做增删查改时，会加MDL读锁；当变更表结构时，会加MDL写锁。      读锁不互斥，因此可以有多个线程同时对一张表增删改查        读写互斥，即不能有多个线程同时更改表结构，或一个线程在增删改查而另一个线程在更改表结构        MDL锁在事务提交时才会释放，在变更表结构时要特别小心，以免锁住线上的查询和更新，导致整张表不能读写。下面是一个示例：                            session A          session B          session C          session D                                      begin;                                                           select * from t limit 1;                                                                      select * from t limit 1;                                                                      alter table t add f int;                                                                      select * from t limit 1;                      session A 先开启了一个事务，执行了一次查询，并且没有马上提交，这时会对表 t 加一个 MDL 读锁。    session B 也需要一个 MDL 读锁，读锁之间不互斥，可以正常执行查询。    session C 要加一个字段，需要一个 MDL 写锁，读锁和写锁互斥，所以必须等待表 t 释放读锁之后才能继续。    session D 需要一个读锁，这里需要注意的是，表 t 上会有一个等待获取锁的锁队列，而获取MDL写锁的优先级要比获取读锁的优先级高，所以导致session D 也被阻塞。    最后的结果就是表 t 完全被锁住，完全不可读写了。如果客户端还有重试机制，一直在发起重试请求，MySQL的线程很快就会爆满，最后导致整个实例挂掉。  ​        解决办法：​        1、监控长事务(information_schema.innodb_trx)，要么先暂停DDL，要么kill掉这个长事务​        2、但是对于一些热点表，kill未必管用，可能刚kill掉一个长事务，新的请求立马又来了。这种情况下，理想的办法是为 alter table 语句设定等待时间，如果在此期间能拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再重试这个命令来重试这个过程。MariaDB 和 AliSQL 已经提供了这个功能，MySQL暂时没有。12ALTER TABLE t NOWAIT add column ...ALTER TABLE t WAIT N add column ... MySQL可以通过调整 lock_wait_timeout 值来控制这个超时时间，默认值是 31536000s，即1年，显然这个时间太长了。案例一假如正在备库执行一个 -single-transaction 的逻辑备份，此时在主库上对表 t 执行了一个 DDL，备库会出现什么情况？备库会根据 DDL 的 binlog 到达的时间点不同而出现不同的情况。先拆解一下 mysqldump -single-transaction 在内部的执行逻辑：      set session transaction isolation level repeatable read;        start transaction with consistent snapshot;    (其他表的备份逻辑)        SAVEPOINT sp;    (时刻1)        show create table t;    (时刻2)        select * from t;    (时刻3)        rollback to savepoint sp;    (时刻4)    (其他表的备份逻辑)  在时刻1到达：没有任何影响，此时表 t 上没有任何 MDL 锁，所以可正常执行，备份得到的是新的表结构；在时刻2到达：此时已经先备份完了 t 的表结构，DDL才到达，在执行第5步的时候会报错：Table definition has changed,please retry transaction ，mysqldump 终止；在时刻3到达：此时表结构和数据都已备份完成，但是 MDL 读锁还没释放(会在第6步后才释放)，所以 DDL 操作会阻塞，备份得到的是旧的表结构；在时刻4到达：MDL 读锁已被释放，DDL 可正常执行，备份得到的是旧的表结构。行锁由存储引擎实现，Innodb 支持，MyISAM 不支持。如果不支持行锁，就只能使用表级锁，也就意味着锁的粒度太大并发度就会降低。这也是为什么推荐使用 Innodb 的重要原因之一。两阶段锁之前提到过一个两阶段提交，行锁有一个两阶段锁协议，也被称为 2PL。定义两阶段指的是分为加锁阶段和解锁阶段，在加锁阶段只能加锁不能解锁，在解锁阶段只能解锁不能加锁。单看定义很难理解，换成大白话说就是行锁在需要的时候才加上，但并不是不需要了就立即释放，而是要事务结束后才会释放。为什么需要两阶段锁？重点就是在事务结束后才会释放所有行锁，而不是用完立即释放。任何锁的本质就是保证并发操作的正确性，将并行改为串行。二阶段锁用来保证并发更新操作的正确性，两个并发的更新操作，必须等其中一个提交后另一个才能继续，否则就会发生更新被覆盖的情况。假设不存在两阶段锁协议，会发生如下情况：同时发起2个操作，向同一个账户打200块，账户原余额有100块。  sessionA 发起打款操作，获取到写锁，用 update 更新账户余额为 300  update 完毕，假设不存在两阶段锁，用完立即释放，释放写锁，此时事务尚未提交  sessionB 发起打款操作，获取到写锁，根据一致性视图可见性规则：事务未提交，更新不可见。得到的账户余额仍为100，用 update 更新账户余额为 300  update 完毕，立即释放写锁  sessionA 提交，账户余额为 300  sessionB 提交，账户余额为 300所以，因为存在两阶段锁协议，在第2步结束后，由于事务尚未提交，写锁仍未释放，则第3步的 update 操作必须等待 sessionA 提交后才能继续，此时 sessionA 读到的余额为300，再执行 update 后更新余额为500，这才是符合逻辑的结果。如何优化？锁虽然保证了并发操作的正确性，但是由并行改为串行降低了并发度。所以另一个问题就是如何最大限度的提高并发度？由于行锁是在需要的时候才加上，在事务结束后统一释放。所以针对包含多个更新的事务，可以调整事务内更新语句的顺序，将会产生行锁竞争的语句尽量往后放，从而让等待行锁的时间最小化，以达到提高并发度的目的。示例：假如有一个在线订票业务，订票逻辑可以简化为下列步骤：  从账户余额扣掉票钱  给系统余额加上票钱  记录一条日志假设同时发起2个订票请求，可以看到，在这个事务中，会产生行锁竞争的是第2步(直白说就是会 update 同一行)。按这个顺序的话，系统余额表上的行锁会从第2步开始加上，第3步完成后事务提交时释放。调整下顺序，改为：  记录一条日志  从账户余额扣掉票钱  给系统余额加上票钱这时候系统余额表上的行锁持续时间就缩短为1步了，从第3步开始加上，到第3步完成后事务提交时释放。虽然在这里看就是少了一小步(一条语句的执行时间)，但如果这个业务请求并发量很大的话，这个优化的效果就会非常明显。死锁为什么会出现死锁？简单说就是出现锁的循环等待。示例如下：                   sessionA      sessionB                  1      begin;      begin;              2      update t set k = k+1 where id=1;                     3             update t set k = k+1 where id=2;              4      update t set k = k+1 where id=2;                     5             update t set k = k+1 where id=1;      由于事务都尚未提交，行锁还未释放。第4步要获取 id=2 的行锁，需要等待sessionB提交；第5步要获取 id=1 行锁，需要等待sessionA提交，死锁产生了。解决办法      等待直到超时，然后退出    Innodb中有个参数用于设置这个超时时间：innodb_lock_wait_timeout，默认值为 50s。这个默认值对于业务来说是不能接受的，相当于卡顿50s。但是如果设成较小的值，又很有可能造成误伤：如果不是死锁，而就是普通的锁等待，此时并没有循环等待的情况，但是由于超过了阈值而被当成了死锁而提前退出了。所以这种方法一般不采用。        主动死锁检测    innodb_deadlock_detect 用于控制是否打开主动死锁检测，默认是 ON。    这是一种相对较好的方式，但需要注意的是它的资源消耗有可能会很大。对于每个新加入进来的线程，都要先判断会不会由于自己的加入而导致死锁，这是一个时间复杂度为 O(n^2) 的操作。假设有1000个并发线程同时更新同一行，这个死锁检测就是100万量级的。    虽然最终检测的结果是没有死锁，但此期间需要消耗大量的CPU资源。所以当出现CPU消耗接近100，TPS却很低的话，很有可能就是死锁检测导致的。    那如何优化这种热点行更新问题？          简单粗暴的方法就是如果确认不会出现死锁，直接关闭死锁检测。但这个方法明显危险系数很高，万一还是出现了死锁的话只能依靠超时机制，而如上面所述，超时机制的阈值很难设置。      拆分热点行。将一行拆分为多行，比如一条账户记录可以拆分为10条子账户记录，账户总额就等于10条子账户余额之和，在需要更新账户余额时，随机选择其中一条进行更新。这样就将一行上的死锁检测成本、锁等待个数、冲突概率都降为了原来的1/10。这种方案属于设计层面上的优化，需要结合业务逻辑做详细的设计和测试。      案例二现需要删除前10000行数据，有以下三种方式：      1delete from t limit 10000;            在一个连接中循环执行20次 delete from t limit 500;    在20个连接中同时执行 delete from t limit 500;哪种方式好一些？第一种：执行时间较长，意味着占用锁(MDL读锁、X锁)的时间会比较长；而且大事务在从库上回放的时间也较长，在此期间会导致主从延迟；第三种：人为的制造了行锁冲突，而且大概率会重复删除，达不到删除前10000行数据的目的；第二种方式较好。"
} ,

{
"title"    : "MySQL学习笔记(七)：MySQL索引",
"category" : "",
"tags"     : "mysql, 索引",
"url"      : "/blog/2021/10/11/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%83)-MySQL%E7%B4%A2%E5%BC%95/",
"date"     : "2021-10-11 00:00:00 +0800",
"content"  : "索引的常见实现方式有哪些？  哈希表：O(1) 的时间复杂度，速度最快，但缺点是只适用于等值查询。因为key是无序的，所以区间查询时只能全部遍历一遍。  有序数组：O(logn)的时间复杂度，利用二分法。可用于等值查询和区间查询，但插入删除时间复杂度较高，因为需要移动插入点后面的所有元素。所以有序数组比较适合静态存储引擎，即基本不会变的数据。  搜索树：常用实现是B+树。为什么采用B+树而不是常见的二叉树?二叉树即每个节点只有左右2个子节点，所以显而易见的问题就是当节点变多时树的高度会很高。比如需要存储100万条数据，就需要20层（n层二叉树的节点数为 2^n-1，20层二叉树的节点总数为1048576）。因为一个节点就是一页，那么一次查询很可能就需要进行20次随机IO（大概率会触发随机IO），在传统机械硬盘时代，一次随机IO大约10ms，那么单一次查询可能就需要200ms，这个查询是很慢的。所以解决办法就是增加树的子节点数，由二叉变为N叉。B+树就是一颗N叉树。一个节点就是一页，页是Innodb磁盘IO的基本单位，一页在Innodb中默认是16k，假如索引字段为整数类型占4个字节，每个key还有一个指向下一层节点的指针固定占6字节，再加上一些辅助字节总共差不多占13字节左右（非叶子节点）。16k/13=1260，那么一个节点就可以差不多有1200个分叉，一颗树高为4的B+树，就可以存 1200^3≈17亿 个值。因为根节点总是在内存中，第二层大概率也在内存中，这时17亿数据量的单次查询页只需要进行2-3次磁盘IO，速度大大提高了。顺便说一下 Innodb 中B+树的特点：  N叉树，即每个节点可以有多个key  内部节点不存储数据，只有指针，只在叶子节点存储数据  每一层页与页之间构成一个双向链表  页内 records 之间构成一个单向链表  叶子节点为第0层，从下往上递增，root为最大层数  一个节点就是一个pageInnodb中的页是什么？Innodb表空间  详见：The basics of InnoDB space file layoutInnodb的数据存储模型被称为 space，即“表空间”。表空间是一个逻辑概念，有一个32位的space ID，实际上可能由多个物理文件组成（如ibdata1、ibdata2）。表空间分为系统表空间（system space：ibdata1、ibdata2，space ID 为0）和表对应的表空间(per-table space：ibd文件)。ibd文件实际上是一个功能齐全的space，可以包含多张表，但在MySQL实现中一个ibd只对应一张表。每个space会被划分为多个page，一个page默认16k。page也有一个32位的page number（页号），表示在space内的偏移量（offset），比如page 0 对应 offset 为0，page 1 对应 offset 为16384。注意一个space可能包含多个文件，所以这个offset不一定是文件内的，而是整个space中的。Innodb单表空间最大为64TB，是因为 2^32 * 16k。页的基本结构页包含一个38字节的头部（FIL为File的缩写）和一个8字节的尾部，中间的内容取决于不同的page type，可用大小为 16k-38-8=16338。FIL Header 和 Trailer 结构如下：可以看到，头部包含了  Offset(Page Number)  Page Type  Space ID  指向前一页和后一页的指针，构成一个双向链表（树的同一层中）  最后一次改动页的LSN  当前系统中（所有space）最大的LSN表空间(space file)文件结构系统表空间(system space)文件结构系统表空间(system space)的 space ID 为 0 。它采用了一些固定页号的页来存储一些关键信息。结构如下：单表空间(per-table space file)文件结构Page3为聚簇索引(主键索引)的root，Page4为第一个二级索引的root，如果有多级索引的话以此类推。Innodb索引  详见 The physical structure of InnoDB index pages一切皆索引在Innodb中一切皆索引，意思是：  每张表都有一个主键。如果没有手动指定，会使用第一个 not null 的 unique key。如果仍然没有，会自动分配一个6字节的隐藏 Row ID作为主键。  主键索引树(聚簇索引)叶子节点key是主键值，value是是整行数据。  二级索引key是索引列的值，value是对应的主键值。  一张表有几个索引，就有几棵B+树。且至少有一棵主键B+树，数据存储在主键索引树上。查询不走索引其实是遍历主键索引树。  B+树中一个节点为一页。索引结构因为一个索引就是一棵B+树，B+树中一个节点对应一页，所有索引页具有和上面讲到的页的相同基本结构，都包含一个FIL Header和FIL Trailer，主体部分会有所不同，如下图所示：重点关注其中的 User Records 和 Page Directory。User Records 是实际存储数据的地方：  非叶子节点：存储指向下一层子节点的指针  叶子节点：假设为主键索引树，存储的就是行数据一个page中的所有 User Records 组成了一个单链表，头是一个叫 infimum 的 system record(存储了当前页中最小的key)，尾是一个叫 supremum 的 system record(存储了当前页中最大的key)。Index Header 结构如下：可以看到有 Number of Records 、 Page Level(叶子节点所在层为第0层，从下往上递增，root节点所在层为最大层) 等等。二级索引叶子节点value的排序问题假设存在以下表记录            a      b      c      d                  1      2      3      d              1      3      2      d              1      4      3      d              2      1      3      d              2      2      2      d              2      3      4             存在一个联合主键 (a,b)，三个索引 c(c)、ca(c,a)、cb(c,b)，分析这三个索引是否有冗余？普通索引叶子节点存的值为主键，这里的主键为联合主键。索引 ca 即先对 c 排序，再对 a 排序，因为key已经包含了a，所以value只需要存储 b，ca相同时，b升序。记录如下：            c(key的部分)      a(key的部分)      b(value存的值)                  2      1      3              2      2      2              3      1      2              3      1      4              3      2      1              4      2      3      再看索引 c。key先对 c 排序，再对 a 排序，再对 b 排序，结果和 ca 是一样的。所以索引ca 是多余的。索引 cb 先对 c 排序，再对 b 排序，再对 a 排序，可用于基于c、b 的查询，需要保留。实践一下可以通过 innodb_space 命令直接分析文件，获取文件中存储的page、records等信息。(目前还不支持MySQL8.0)  详见：  innodb_ruby  B+Tree index structures in InnoDB基于主键索引和普通索引的查询有什么区别主键索引树叶子节点直接存储行数据，所以主键索引查询只需要扫描主键索引树即可。而普通索引树叶子节点存储的是主键值，所以需要先扫描普通索引树拿到主键值，再回到主键索引树获取行数据，相较于主键索引查询多扫描了一棵索引树，这个过程称为 回表。选择自增主键还是业务主键可从存储空间大小和性能两个方面来考虑：  占用空间：业务主键相较于自增主键都较长，由于二级索引树叶子节点存储的是主键值，所以采用业务主键的二级索引相较于自增主键会占用更多的空间。  性能：由于自增主键是有序的，所以在维护索引树时直接追加即可(叶子节点所在层即第0层的最后一个节点中的最后一个record后)，当一页写满会自动开辟一个新的页。而业务主键很难保证有序性，维护索引时很可能会在中间插入，就很有可能引起节点分裂(甚至是父索引节点的分裂)，自然性能会受到影响。所以，在大多数情况下，都应优先使用自增主键。当然事无绝对，如果只有一个索引，且该业务字段是唯一的，可以将该字段设为主键。因为不存在其他索引，就不用考虑其他索引的叶子节点大小问题。当然，性能上相较于自增主键还是会有一点影响。一些索引设计原则假设存在表 u(id,id_card,name,age,gender)，id 是主键，另有一个 id_card 索引覆盖索引即索引key中包含了要查找的字段。假如现需要根据身份证查询数据，可直接走 id_card 索引。现又有另一个高频需求，根据身份证查询姓名。目前走的仍然是 id_card 索引，需要先在 id_card 索引树上找到对应的 id_card，然后再回到主索引树上根据 id获取到姓名，也就是 回表。那有没有什么办法可以优化呢？方法就是建立一个 (id_card,name) 的联合索引。这棵索引树节点的key包含了两个字段：id_card 和 name，这样的话直接在 (id_card,name) 索引上搜索便可直接得到 name，而不需要回表查整行记录，减少了语句的执行时间。与此同时，根据最左匹配原则，原先根据身份证查询数据的请求也可以用到这个索引，所以现在可以删除id_card 这个索引了。这就是覆盖索引，即索引key中包含了要查找的字段。可使用 explain 查看是否使用了覆盖索引，如果在 explain 中的 extra 列中出现了 Using index，说明当前查询使用了覆盖索引，即不需要回表查询。当然，索引是有代价的。因为每新建一个索引就相当于新建一棵索引树，虽然可以提高查询速度，但增删改就需要多维护一棵索引树。所以需要权衡使用，任何索引都是这样，数据量小的话就没有什么必要，没有太大区别。最左匹配原则假设现在又有一个低频需求：根据身份证查询地址，那么有必要再建立一个 (id_card,address)的联合索引吗？答案是不需要。因为这是一个低频请求，意味着请求的次数不会太多，上一节的 (id_card,name)索引就够用了。可先通过(id_card,name) 这个索引定位到相应的 id_card，获取到主键后再回表查询。原理很简单，因为联合索引 (a,b)是先根据 a 排序再根据  b 排序，所以对于 a 的检索可以用到这个B+树。所以最左匹配原则的定义就是只要索引满足最左前缀，便可利用该索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左N个字符。如果既有a,b的联合查询，又有基于a、b各自的查询呢？这时考虑的原则就是空间了。如果b字段比a字段大，那么就应该建立一个 (b,a)的联合索引和一个 a 的单独索引。这两个索引可以同时满足 a,b的联合查询和基于a、b各自的查询。除此之外，最左匹配原则当遇到范围匹配时就会失效。比如有一个 (a,b)的联合索引， a=1 and b=2 或 b=1 and a=2 都可以用到索引，顺序无所谓，优化器会调整 where 的顺序。但是当遇到类似 a gt;1 and b=2 时，就只有 a 能用到索引，会先快速定位到 agt;1 的记录，此时 b 是无序的，只能遍历判断 b 是否满足。示例：      如果 sql 为    1SELECT * FROM table WHERE a = 1 and b = 2 and c = 3;         该怎么建立索引？    答：第一反应是直接建立 (a,b,c) 的联合索引，但是这里要注意区分度，区分度高的放在前面。区分度越高，检索效率越高，因为快速定位更精准。像性别、状态这种区分度很低的字段，放到后面。        如果 sql 为    1SELECT * FROM table WHERE a gt; 1 and b = 2;         该怎么建立索引？    答：因为是范围查询，如果建立 (a,b) 的索引，就只有 a 能用上索引。    ​        所以应该建立 (b,a) 的索引，优化器会调整条件的顺序，然后b 就能用上索引，在此基础上，agt;1 也能用上。        如果 sql 为    1SELECT * FROM `table` WHERE a gt; 1 and b = 2 and c gt; 3;         该怎么建立索引？    答：首先肯定要以 b 开头，所以有两种选择：(b,a) 或 (b,c)，至于具体选择哪个，就看区分度和字段长度了。        如果 sql 为    1SELECT * FROM `table` WHERE a = 1 ORDER BY b;        该怎么建立索引？    答：建立 (a,b) 联合索引，当 a=1 时，b 相对有序，可以避免再次排序。    ​        那如果是    1SELECT * FROM `table` WHERE a gt; 1 ORDER BY b;        ​        因为此时是范围查询， agt;1 时 b 是无序的，所以没有必要再建立一个 (a,b) 的联合索引。        如果 sql 为    SELECT * FROM `table` WHERE a IN (1,2,3) and b gt; 1;     该怎么建立索引？    答：还是建立  (a,b) 的联合索引，IN 查询 可视为等值查询，相当于 a=1 or a=2 or a=3，所以还是一样的思路。  索引下推索引下推并不是一个索引设计原则，它是一个索引查找的内部优化。前提：因为范围查询不能使用联合索引，只能使用最左前缀。以下面的 sql 举例：1select * from tuser where name like &#39;张&#39; and age=10 and is_male=1;以 张 开头的记录有 (张三，10)，(张三，10)，(张三，20)，(张六，30)，有一个联合索引 (name,age)。在 MySQL5.6 之前，存储引擎提供的接口对于这种情况只允许传入最左前缀一个参数，即只能传入 name  这个字段，所以需要回表4次用于判断 age 和 is_male 是否满足条件。在 MySQL5.6之后，接口可以传入包含最左前缀的整个联合索引，即name,age字段。这样的话可直接在 (name,age) 索引树上就对 age 进行判断，提前过滤掉不满足条件的记录，最后只需要回表2次。当 explain 的 extra 字段中显示 Using index condition 时则表示本次查询使用到了索引下推。选择唯一索引还是普通索引？从读和写两方面来分析。      读：二者区别就在于是否唯一。唯一索引找到记录后即可返回，普通索引还需继续向后遍历检查是否满足条件。但此时数据页已在内存中，而且很大概率都是页内遍历(通过二分法)，这点差异对于现在的CPU来说可以忽略不计。所以可认为二者在查询方面差异不大。        写：需要考虑目标数据页是否在内存中，下面以 insert 举例说明          目标数据页在内存中：对于唯一索引来说，找到插入位置，判断到没有冲突，插入，语句执行结束；对于普通索引来说，找到插入位置，插入，语句执行结束。这种情况下直接更新内存即可，性能也没有多大差异。      目标数据页不在内存中：                  由于唯一索引需要判断唯一性，所以必须要将数据页从磁盘读到内存。          而普通索引没有这个要求，所以可直接在内存中记录下一条 “insert” 操作，语句就执行结束了。记录这个操作的区域叫 change buffer， 由于只需要写内存，避免了磁盘的随机读(磁盘的随机IO是数据库中成本最高的操作之一)，这种情况下普通索引性能就远远优于唯一索引，尤其如果是机械硬盘的话。                    综上所述，如果业务可以接受的话，从性能角度出发，应该选择普通索引。change bufferchange buffer 是 buffer pool 的一部分，默认占比为 25()，最大占比可设为 50()。1show global variables like &#39;innodb_change_buffer_max_size&#39;;在MySQL5.5之前的版本，只支持缓存insert操作，所以最初叫 insert buffer(很多地方见到的 ibuf 指的就是它，后来也一直延用了下来)。后来也加入了对 update、delete 的支持，便改名为了 change buffer。作用当目标数据页不在内存中时，普通索引更新类操作的提速器。注意：只能作用于普通索引，不能作用于唯一索引。原理上面已经简单分析过，就是通过减少磁盘的随机读来提升写的性能。上面是通过 insert 来举例，update 和 delete 有点点区别。如果update 和 delete  中的 where 条件走普通索引的话，由于需要获取受影响行数，所以还是需要先把数据页读到内存中，然后直接更新内存，这里就用不上 change buffer 了。如果 where 条件走主键索引或其他普通字段(其实最终也是走主键索引树)，并且需要更新普通索引字段的话，通过主键索引树获取受影响行数，对普通索引的更新就直接写到 change buffer 中了，这种情况是能用 change buffer 来提速的。怎么保证数据被正确更新？上面说到，普通索引的更新写到 change buffer 中就结束了，那后续的查询是怎样的？还是对应到上面的2种情况：目标数据页在不在内存中。      如果目标数据页在内存中，意味着更新操作是直接更新的内存。那此时内存中的数据页一定是更新后的数据，虽然磁盘上还是老的数据，所以直接从内存返回即可；        如果目标数据页不在内存中，需要先把数据页从磁盘读入内存，然后应用 change buffer 中的操作日志，生成一个正确的版本后返回，这个过程称为 merge。          merge 的时机：              查询时。这时会把目标数据页从磁盘读到内存中；        作为后台任务定期运行。innodb_io_capacity 和 innodb_io_capacity_max 用于设置 Innodb 后台任务(刷脏页、merge)的 IOPS，可调整该数值来控制 merge 的频率；        在崩溃恢复期间，会从系统表空间(ibdata1)中读取 change buffer，然后当把数据页从磁盘读到内存中时，会进行 merge；        重启后；        slow shutdown 时。可通过 --innodb-fast-shutdown=0 开启 slow shutdown。            怎么保证更新不丢失？如果写完 change buffer 后断电了或意外宕机了，重启后 change buffer 和数据会丢失吗？不会。change buffer 也会被记到 redo log 中(redo log 中包含了数据页的变更和change buffer的变更)，回想之前讲过的两阶段提交协议，redo log 和 binglog  落盘才代表事务成功提交。所以，如果一个事务已提交，则代表 change buffer 已经写到 redo log 中，且 redo log 已落盘，崩溃恢复时会根据 redo log 来恢复 change buffer。适用场景change buffer 简单来说就是把对普通索引的更新缓存了下来，然后在适当的时候进行 merge。所以在 merge 之前， change buffer 中记录的变更越多，收益就越大。所以对于写多读少类业务，数据页在写完之后马上被访问到的概率很小，也就是说不会马上进行 merge，这种情况下 change buffer 的效果最好。比如账单类、日志类等。相反，如果是写后马上进行查询的业务，由于马上要访问数据页，会立即触发 merge。这种情况不仅不会减少随机IO的次数，反而会增加维护 change buffer 的代价，反而起到了副作用，这种情况下可以关闭 change buffer ：1show global variables like &#39;innodb_change_buffering&#39;;默认值为 all ，设为 none 即可关闭 change buffer。官方文档InnoDB Change Buffer给字符串字段创建索引的几种方法      直接创建完整索引，这样可能比较占空间        创建前缀索引    即可以只定义字符串的一部分作为索引    1alter table user add index index_email(email(6));        优点是：节省空间    缺点是：                  可能会增加额外的扫描次数        ​    比如执行这样一句查询：        1select id,name,email from user where email=&#39;zhangssxyzxxx.com&#39;;                ​    对于完整索引，在 email 索引树上定位到 zhangssxyzxxx.com，然后回表取出对于记录即可，只需扫描一行；        ​    对于前缀索引，在 email 前缀索引树上定位到 zhangs，回表判断 email 是否等于 zhangssxyzxxx.com，是的话将记录加入结果集，继续在 email 前缀索引树上遍历下一条记录，再回表判断，重复此过程，直到遍历的下一条记录不等于 zhangs。所以前缀索引可能会增加记录的扫描行数。        如何优化？        关键在于增加前缀的区分度。区分度越高，过滤掉的记录就越多，需要回表的次数就越少。        可以通过统计索引上有多少个不同的值来判断要使用多长的前缀。        先统计索引上有多少个不同的值：        1select count(distinct email) as L from user;                再依次选取不同长度的前缀来对比区分度：        123456select   count(distinct left(email,4)）as L4,  count(distinct left(email,5)）as L5,  count(distinct left(email,6)）as L6,  count(distinct left(email,7)）as L7,from user;                数值越大表示对应长度的前缀区分度越高，效果越好。                    不能使用覆盖索引        比如执行这样一句查询：        1select id,email from user where email=&#39;zhangssxyzxxx.com&#39;;                因为只需要查询 id,email，对于完整索引来说，使用覆盖索引即可，不需要再回表；        对于前缀索引，则必须要回表判断 email 的值，即便前缀索引包含了全部字段(email(18)，假设email 有18个字符)，因为系统不确定前缀索引的定义是否截断了完整信息。                  倒序存储    对于前缀区分度不够好的情况，可以考虑使用倒序存储。    比如身份证，同一个区域内的身份证前面几位都是相同的，如果按照上面的方法建立前缀索引，这个前缀的长度可能会比较长。    这时可把身份证倒过来存，因为身份证的尾部都是不同的，区分度足够高，查的时候转换一下：    1select field_list from t where id_card = reverse(&#39;input_id_card_string&#39;);        这时为身份证建立前缀索引需要的长度就会短很多，具体多长可通过上面的方法来确定。    缺点：不支持范围查询，因为是倒序存储，没办法按顺序遍历。        新增一个 hash 字段    专门新增一个 hash 字段用来做索引。          相较于倒序存储查询性能相对稳定一些，因为倒序存储毕竟还是前缀索引，或多或少还是会增加扫描行数。而crc32(或其他哈希算法)冲突的概率总体还是非常小的，可认为每次查询的平均扫描行数接近1。        1alter table t add id_card_crc int unsigned, add index(id_card_crc);        每次插入新纪录的时候，都用 crc32() 计算出一个哈希值填到这个新字段中。    查询的时候计算一下，同时因为 crc32 会有冲突(虽然概率也非常小)，所以还需要在 where 中校验一下原值：    1select field_list from t where id_card_crc=crc32(&#39;input_id_card_string&#39;) and id_card=&#39;input_id_card_string&#39;        这时索引的长度就只有4个字节(crc32的长度)，相较于身份证长度大大减少了。    缺点：和倒序存储一样，不支持范围查询，因为哈希字段对应的原值完全是无序的，没办法在哈希索引上按顺序遍历。  "
} ,

{
"title"    : "MySQL学习笔记(六)：MySQL事务",
"category" : "",
"tags"     : "mysql, 事务",
"url"      : "/blog/2021/09/19/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%85%AD)-MySQL%E4%BA%8B%E5%8A%A1/",
"date"     : "2021-09-19 00:00:00 +0800",
"content"  : "解决的问题      脏读：读到还没有提交事务的数据        不可重复读：前后读取的记录内容不一致        幻读：前后读取的记录数量不一致          示例：假设存在 person(id,name) 表，含2条数据：(1,”foo”)、(2,”bar”)                                    事务A            事务B                                                select count(1) from person 查到2条数据                                                          insert into person(id,name) values(3,”zoo”) 插入一条数据                                             commit 提交                                insert into person(id,name) values(3,”zoo”) 插入一条数据，报错提示主键重复                                             select * from person where id = 3，又查不到数据，但是又不能insert，就很奇怪，这种现象就被成为“幻读”                                           注：使用 select count(1) 并不能看到幻读现象      隔离级别  读未提交  读提交：可解决脏读问题  可重复读：可解决不可重复读问题  串行化：可解决幻读问题如何查询隔离级别？1show global variables like &#39;isolation&#39;;默认级别为**可重复读**。如何更改隔离级别？1set global(session) transaction isolation level READ UNCOMMITTED ;如何实现？MVCC（Multiple Version Concurrency Control）多版本并发控制简而言之，就是一行记录在数据库中存在多个版本，如下图所示：V1、V2、V3 并不是物理真实存在的，真实存在的是U1、U2、U3，也就是undo日志。当需要上一个版本的数据时，会通过当前记录和undo日志推算出来。其实每行记录都会有一个我们看不到的隐藏字段trx_id。一致性视图（一致性读）有两种开启事务的方式：  begin/start transaction：并不是一个事务的真正起点，执行第一个语句的时候事务才真正开启  start transaction with consistent snapshot：马上开启事务事务开启时，会创建一个一致性视图。不必纠结于视图两个字，可简单理解为这样一个操作：记录下真正开启事务的那一刻，后面会以这个时刻为准来判断一行记录的哪个版本对于当前事务可见。具体实现上，会在开启事务的那一刻生成一个数组，保存了当前系统正在活跃（启动了还没提交）的事务ID，定义一个低水位：数组里的最小事务ID，高水位：当前系统已经创建过的最大的事务ID+1，这个数组和高低水位即构成了一致性视图。后续的操作应该可以猜到，就是拿记录的事务ID和这个高低水位比较来判断是否可见。这是具体的代码逻辑，不方便记忆，可简化为下面的可见性规则：  本事务的更新总是可见  版本未提交，不可见  版本已提交，但是在视图创建之后提交的，不可见  版本已提交，而且是在视图创建之前提交的，可见以上就是事务具体是如何实现的不同隔离级别下的一致性视图生成时机  读未提交：直接读取记录的最新版本，没有视图的概念  读提交：每个语句会生成新的一致性视图  可重复读：在事务开启的时候生成一致性视图，在后续的整个事务期间都使用该视图  串行化：直接用加锁的方式来避免并行访问当前读事务使用的是一致性读，这里针对的是查询类操作，而更新类操作使用的是当前读，即读取记录的最新版本，这个区别这很重要。示例说明千言万语，不如动手验证一下假设存在表 t ( id, k)  values (1,1) ( 2, 2)，事务级别为默认级别，即可重复读。            事务A      事务B      事务C                  start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                                   update t set k=k+1 where id=1;                     update t set k=k+1 where id=1;                            select k from t where id=1;                     select k from t where id=1;                            commit;                                   commit;             事务C没有显示开启事务，所以更新完成以后自动提交了，此时k的值是2。先分析事务A，事务A最先开启，根据可重复读的一致性视图生成时机，在开启的那一刻创建了一致性视图，在此后的整个事务期间都使用该视图。又根据事务可见性规则，事务C的版本虽已提交，但是在视图创建之后提交的，不可见。事务B先不管值是多少，版本尚未提交，仍不可见。所以事务A的查询结果仍是1。事务B首先执行了一次更新操作，这里的重点就是：此时读取的是哪个版本？结论是最新版本，即当前读。因为要保证更新不能丢失。所以读到的值是2，再+1=3。然后再进行一次查询操作，根据事务的可见性规则，本事务的更新总是可见。所以，事务B的查询结果是3。如果换成读提交隔离级别，结果是怎样的？根据读提交的一致性视图生成时机：在每一次执行语句的时候都生成新的视图，所以事务A的视图是在执行到 select 的时候生成。再根据事务的可见性规则，事务C的更新已提交，且是在视图生成之前提交，可见。事务B的更新尚未提交，不可见。所以事务A的查询结果是2。事务B的查询结果还是3，分析过程和可重复读隔离级别下一样。将上面的操作步骤稍微改一下，事务C稍后再提交，隔离级别仍是可重复读，看下结果会怎样            事务A      事务B      事务C                  start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                            update t set k=k+1 where id=1;                     update t set k=k+1 where id=1;                            select k from t where id=1;                     select k from t where id=1;             commit;              commit;                                   commit;             事务A查询结果：事务B和事务C的更新都尚未提交，不可见，结果是1。事务B查询结果：重点来了，由于事务C更新后没有马上提交，所以id=1的写锁一直未释放。而事务B是当前读，必须要读最新版本，若读到的记录最终被回滚了，就产生了脏读，这是不可接受的，所以必须确保当前读读到的数据是最终会被提交的数据。具体实现就是加锁，来确保当前读的数据是最新的且已提交的。所以事务B的更新，要等待事务C提交后才能继续。长事务尽量避免长事务，长事务有以下风险：      会产生大量undo日志，会占用大量存储空间。在MySQL 5.5 及以前版本中，回滚日志是和数据字典一起放在ibdata文件中的，即使长事务最终被提交，回滚日志被清理，文件也不会变小。        还会占用锁，可能拖垮整个库。如之前对当前读的讨论，当前读会加锁来保证读取到的数据是最新且已被提交的数据，若长事务一直不提交，会导致锁一直被占用，存在很大隐患。        甚至可能会导致优化器选错索引。    案例重现：    表 t 定义如下：    12345678CREATE TABLE `t` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `a` int(11) DEFAULT NULL,  `b` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `a` (`a`),  KEY `b` (`b`)) ENGINE=InnoDB;        存储过程，插入100000条数据：    123456789101112delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(ilt;=100000)do    insert into t(a,b) values(i, i);    set i=i+1;  end while;end;;delimiter ;call idata();        插入数据后执行 select * from t where a between 10000 and 20000，查询走的是 a 索引，没有问题    1explain select * from t where a between 10000 and 20000            sessionA 模拟一个长事务，sessionB 先清空表，再调用存储过程插入数据                            sessionA          sessionB                                      start transaction with consistent snapshot;                                                delete from t;                                     call idata();                                     explain select * from t where a between 10000 and 20000;                          commit                                 再查看执行计划，会发现没有选择使用索引 a，走的是全表扫描        为什么？    选择索引是优化器的工作，它会综合考虑扫描行数、索引基数、是否使用了临时文件、是否排序等因素来选择它认为速度最快的索引。    可通过 show index from t 来查看索引基数 cardinality (近似值，并不精确)，结果如下：        索引基数越大，说明区分度越高，选择它的概率就越大。图中可以看出主键和 a 的差距并不大，a 反而是最高的，不选择索引 a 肯定还有其他的原因。    再来看一下扫描行数。查询计划显示走的是全表扫描，rows为105033行。强制使用索引 a 看下扫描行数是多少：    1explain select * from t force index(a) where a between 10000 and 20000;            rows为39940行。这个差距挺大的，那为什么优化器放着 39940行的 a 索引不用而要选择 105033行的全表扫描？    因为使用索引 a 需要回表，而走全表扫描是直接扫描主键索引树，不需要回表，优化器”认为“全表扫描更快。从结果上看，优化器明显选错了，所以优化器也不是百分百可靠的。    到这里又有一个疑问，为什么开启了一个长事务后，删除再插入数据就会导致扫描行数发生变化？    很明显，肯定和这个长事务有关。    先简单说一下undo log，分为两种：insert undo logs 和 update undo logs，对应 insert操作和update、delete操作(delete本质上也是update操作，delete时并不是直接物理删除，而是先做一个 deleted 标记)。insert undo logs 只用于回滚操作，并且在事务提交后便可清除(事务提交即意味着插入已永久持久化了，不会再有回滚操作，也就不再需要 insert undo logs 了)。而 update undo logs 除了用于回滚操作之外，还用于MVCC，且只有当没有任何事务再需要用它来构造数据的早期版本时，方可删除。    上面讲到 delete 时并不是直接物理删除，而是先标记。只有在对应的 update undo logs 被清除时才会进行真正的物理删除，这个过程也成为 purge。    所以在这个案例中，因为先开启了一个长事务 sessionA，为了保证事务的可重复读，在 sessionB 中进行的 delete 和 insert 操作所产生的 upodate undo logs 会一直保留直到长事务提交。也就意味着原先的10万行数据并没有被真正删除，而是保留在了索引树上。因此每一行都会有两个版本，即总共会有20万数据，从 ibd 文件大小也能看出，大小变为了原来的两倍。而优化器统计扫描行数时会将标记为删除的版本也统计在内，导致扫描行数增加。    那为什么只有索引 a 的扫描行数增加了，主键的扫描行数还是10万行？因为主键的扫描行数是直接按照表的行数来估算的，而表的行数优化器取的是 show table status like &#39;t&#39; 中 rows 的值。索引的统计则是通过对数据页采样统计估算出来的，所以会算上老版本的数据。    原因找到了，如何解决？          既然是由长事务引起的，提交或kill掉长事务后便可重新让查询走索引 a      简单粗暴，force index(a)  强制使用索引 a`      analyze table t 重新统计索引信息，优化器会更正扫描行数，然后选择使用索引 a 。当发现 explain 预估的 rows 和实际情况差距较大时，都可以采用这个方法来处理      如何监控长事务？下述语句用于监测持续时间超过60s的长事务1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))gt;60如何避免长事务？应用开发端      确认是否使用了 set autocommit = 0 ，代表手动提交事物。可先开启 general_log，通过 show global variables like &#39;general_log&#39; 查看 general_log 是否开启和日志文件路径。然后随便跑一个业务逻辑，在日志中检查是否有  ` set autocommit = 0`。        确认是否有不必要的只读事务。有些框架会不管什么语句都用 begin/commit 包起来。或者有些业务并没什么必要，也要把一堆 select 放到事务中。上面提到过，对当前读事务会加锁。所以对于单纯的 select 没有必要放到事务中，事务中主要放 update/insert/delete，除非是明确需要事务特性的查询，比如明确需要可重复读的查询，才需要放到事务中。        根据对业务的预估，通过 set max_execution_time 来设置单个语句的最长执行时间，来避免单个语句意外执行过长时间。    数据库端                  监控 information_schema.innodb_trx表，超过阈值就报警或者kill掉。                    推荐使用 percona 的 pt-kill 工具，作用描述如下：                  pt-kill captures queries from SHOW PROCESSLIST, filters them, and then either kills or prints them. This is also known as a “slow query sniper” in some circles. The idea is to watch for queries that might be consuming too many resources, and kill them.                            在测试阶段可要求输出所有的 general_log，分析日志提前发现问题。                    在MySQL5.6或更新版本，可将 innodb_undo_tablespaces 设置为 2 或更大的值，如果真的出现大事务导致回滚段过大，清理起来更方便。                  innodb_undo_tablespaces ：                      0：使用系统表空间，即 ibdata1            不为0：使用独立数量的undo表空间，默认为2，即 undo_001、undo_002                              "
} ,

{
"title"    : "MySQL学习笔记(五)：MySQL计算QPS和TPS",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/09/11/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%94)-MySQL%E8%AE%A1%E7%AE%97QPS%E5%92%8CTPS/",
"date"     : "2021-09-11 00:00:00 +0800",
"content"  : "QPS基于 com_select  基于questions的会统计show命令，mysql设置环境变量的时候也会增加，不太准12345#!/usr/bin/env bashOLD_QPS=`echo show global status where Variable_name=&#39;Com_select&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`sleep 1NEW_QPS=`echo show global status where Variable_name=&#39;Com_select&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`echo (NEW_QPS-OLD_QPS) / 1 | bc1 代表添加到shell的第一个参数值，2 代表第二个，以此类推。0 为shell文件名。获取当前时刻的 qps：1./qps.sh 1TPS12345678910#/usr/bin/env bashOLD_COM_INSERT=`echo show global status where Variable_name=&#39;Com_insert&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`OLD_COM_UPDATE=`echo show global status where Variable_name=&#39;Com_update&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`OLD_COM_DELETE=`echo show global status where Variable_name=&#39;Com_delete&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`sleep 1NEW_COM_INSERT=`echo show global status where Variable_name=&#39;Com_insert&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`NEW_COM_UPDATE=`echo show global status where Variable_name=&#39;Com_update&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`NEW_COM_DELETE=`echo show global status where Variable_name=&#39;Com_delete&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`echo ((NEW_COM_INSERT - OLD_COM_INSERT) + (NEW_COM_UPDATE - OLD_COM_UPDATE) + (NEW_COM_DELETE - OLD_COM_DELETE)) / 1 | bc"
} ,

{
"title"    : "MySQL学习笔记(四)：MySQL定时备份和恢复",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/09/05/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)-MySQL%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/",
"date"     : "2021-09-05 00:00:00 +0800",
"content"  : "定时备份：使用 mysqldump + crontabmysqldump 用法      备份全部数据库的数据和结构    1mysqldump -uroot -p123456 -A gt; /data/mysqlDump/mydb.sql            备份全部数据库的结构（加 -d 参数）    1mysqldump -uroot -p123456 -A -d gt; /data/mysqlDump/mydb.sql            备份全部数据库的数据(加 -t 参数)    1mysqldump -uroot -p123456 -A -t gt; /data/mysqlDump/mydb.sql            备份单个数据库的数据和结构(数据库名mydb)    1mysqldump -uroot-p123456 mydb gt; /data/mysqlDump/mydb.sql            备份单个数据库的结构    1mysqldump -uroot -p123456 mydb -d gt; /data/mysqlDump/mydb.sql            备份单个数据库的数据    1mysqldump -uroot -p123456 mydb -t gt; /data/mysqlDump/mydb.sql            备份多个表的数据和结构（数据，结构的单独备份方法与上同）    1mysqldump -uroot -p123456 mydb t1 t2 gt; /data/mysqlDump/mydb.sql            一次备份多个数据库    1mysqldump -uroot -p123456 --databases db1 db2 gt; /data/mysqlDump/mydb.sql      crontab 用法crontab不同操作系统实现不同，语法是通用的crontab 表达式含义：1* * * * *  表示分钟  表示小时  表示一个月中的第几天  表示月份  表示一个星期中的第几天  * 表示分钟都要执行（以此类推）  a-b 表示从 a 到 b 这段时间内要执行（以此类推）  */n 表示每 n 分钟执行一次（以此类推）  a,b,c 表示在第 a、b、c 分钟执行（以此类推）定时任务脚本12345678910111213141516171819202122232425#!/bin/bashnumber=31backup_dir=/var/lib/mysqldd=`date +Y-m-d_H:M:S`tool=mysqldumpusername=rootpassword=xxxxdatabase_name=testif [ ! -d backup_dir ];then    mkdir -p backup_dir;fitool -uusername -ppassword database_name gt; backup_dir/database_name-dd.sqldelfiles=`ls -l -crt backup_dir/*.sql | awk &#39;print 9 &#39; | head -1`count=`ls -l -crt backup_dir/*.sql | awk &#39;print 9 &#39; | wc -l`if [ count -gt number ];then    rm delfiles;fi利用全量备份和binlog恢复数据  使用全量备份恢复临时库1   mysql -uroot -p database lt; dump.sql  flush logs 重开一个binlog，一是避免操作当前binlog文件防止发生意外情况，二是缩小范围排除干扰，在之前的binlog中定位操作范围1   flush logs      使用 mysqlbinlog 导出 sql，主要是设置 --start-position 和 --stop-position，不需要设置 --base64-output=decode-rows    1mysqlbinlog --start-position 6276 --stop-position 6481 --database test binlog.000011 gt; binlog.sql            使用导出的sql进行增量恢复    1mysql -uroot -p database lt; binlog.sql      参考MySQL 定时备份数据库（全库备份）"
} ,

{
"title"    : "MySQL学习笔记(三)：redo log 和 binlog",
"category" : "",
"tags"     : "mysql, redo, binlog",
"url"      : "/blog/2021/08/28/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%89)-redo-log-%E5%92%8C-binlog/",
"date"     : "2021-08-28 00:00:00 +0800",
"content"  : "binlogMySQL Server层的日志，与存储引擎无关，存储的是逻辑日志，可理解为sql语句。主要作用：  如果是 Innodb 引擎，和 redo log 一起提供崩溃恢复的能力（crash-safe），保证了事务ACID中的持久性。  常被用于主备同步，或接入其他下游系统如 es 用于数据分析，即作为其他需要直接从数据库获取数据且有很高实时性要求的应用的数据来源通道。如何查看？      ​    查看binlog 格式    1show global variables like &#39;binlog_format&#39;;            ​    查看当前使用的binlog文件    1show master status ;            ​    查看所有binlog文件    1show binary logs;            查看binlong文件内容    方式一：show 命令    1show binlog events in &#39;binlog.000008&#39;;        方式二： mysqlbinlog    1mysqlbinlog --base64-output=decode-rows --verbose binlog.000001            binlog 3种格式的区别    STATEMENT：记录具体执行语句，如 update t set name = &#39;newName&#39; where name = &#39;oldName&#39;    ROW：记录每一行记录的变更，一个 update 可能产生多条日志（每条记录都会产生一行日志）    MIXED：当满足某些条件时会自动从 STATEMENT切换到 ROW        选择哪种格式？    STATEMENT直接记录sql，可能会在某些情况下导致执行结果不一致，比如 delete 时走的索引不同会导致删除的记录不同；    ROW 直接记录每一行记录的变更，最安全，但是日志的量会很大，存储空间不紧张的话最好选择该格式。MySQL 8 默认格式就是 ROW；    MIXED 需要检查会不会有不在自动切换的条件中，但可能会导致执行结果不一致的特殊情况，自动其实意味着熟悉        切换当前连接 binlog 格式    1SET SESSION binlog_format = &#39;STATEMENT&#39;;            刷新binlog    1flush logs;      binlog的写入时机binlog会首先写到cache里，cache满了就会暂存到磁盘上，这个参数大小由 binlog_cache_size 控制，默认是32K。注意这个cache是单个线程拥有的，也就是说每个线程都有自己的binlog_cache，但是共用同一份binlog文件。一个事务的binlog是不能被拆开的，无论一个事务多大，也要确保一次性写入binlog文件。binlog会被首先write到操作系统的page cache中，然后才会被 fsync到磁盘中。一般可认为只有fsync才占磁盘的IOPS。write  和 fsync 的时机由参数 sync_binlog控制，默认值是1，总共有3种设置：  0：每次事务提交时都只write，不fsync。由于只保存到操作系统缓存，所以如果主机重启会丢失数据  1：每次事务都 fsync  N(Ngt;1)：每次事务提交都write，但只有累计了 N 个事务以后才会 fsync设为N时，如果主机异常重启会丢失最近N个事务的数据，但是可以提升性能，需评估使用。redo logInnodb 特有的日志，存储的是物理日志，即数据页上的改动。数据页：Innodb 从磁盘读取数据的基本单位，即使只取一行记录，也会将改行所在的整个数据页读入内存。数据页的大小可通过以下参数查看，默认为16k1show global variables like &#39;innodb_page_size&#39;;主要作用：提供事务能力，保证 crash-saferedo log 格式采用循环写的方式，有2个游标：checkpoint、writepos。writepos 表示当前写的位置，checkpoint 表示要擦除的位置，都是边写边往后推移。绿色的部分表示空闲的空间。查看 redo log 相关设置1show global variables like &#39;innodb_log&#39;;innodb_log_buffer_size：redo log 缓冲区大小，默认16Minnodb_log_file_size：redo log文件大小，默认48Minnodb_log_files_in_group：每组有多少个redo log 文件，默认2个：ib_logfile0、ib_logfile1redo log 一般设多大？redo log 太小的话，会导致很快被写满，然后不得不强行刷 redo log，发挥不出 WAL 的作用。所以，磁盘空间不是很紧张的话，设大点吧，比如设置为4个文件，每个文件1G。redo log 的写入时机？同样，redo log 也分为 write 和 fsync，由参数 innodb_flush_log_at_trx_commit控制，默认值为1：  0：每次事务提交只把redo log保留在 redo log buffer 中  1：每次事务提交都 fsync 到磁盘（prepare阶段fsync，commit阶段write就行了）  2：每次事务提交都只将redo log 写到 page cache 中除此之外，后台还会有一个线程，每隔1秒会将redo log buffer中的内容先 write 到 page cache，再 fsync 到磁盘。没提交的事务也会被 被动的 写入磁盘：  redo log buffer 占用空间达到 innodb_log_buffer_size 一半时，后台线程会主动写盘，注意由于没有提交事务，此时只是 write，没有 fsync  如果有并行事务，假如A执行了一半，已经写了一些redo log 到 buffer中，此时 B 提交，如果 innodb_flush_log_at_trx_commit=1，B要把buffer里日志全部fsync到磁盘，此时会带上A的内容一起fsync再次强调，写完内存和rodo log、binlog就算事务提交成功了。所以结合两阶段提交，当默认值为1时，在redo log的 prepare 阶段就会fsync一次。然后再写binlog，再将redo log 的标识设为 commit（但是这个阶段只会write 到page cache 中，不再需要fsync了，因为有后台定时线程：每1秒会fsync一次；和崩溃恢复逻辑：prepare的redo log 加上完整的binlog即可保证事务的持久性和一致性）最终数据落盘的过程是？1、正常运行过程中，MySQL更新完内存后，内存中的数据页被称为脏页，最终数据落盘就是把脏页写盘，这个过程甚至和redo log 毫无关系。（当然，猜测还会向前推进redo log中 checkpoint的位置）2、在崩溃恢复时或启动时（MySQL启动时会自动执行该过程），会先将redo log中的数据更新到内存中，此时该数据页就变成了脏页，接下来的步骤就和第1种情况一样了。什么是MySQL中的两阶段提交？一个事务当写完内存和redo log、binlog以后就算提交成功了，并不需要写真正的数据文件。为了保证2个日志文件都写入成功，采用了两阶段提交的方法，如下图所示两阶段提交并不是MySQL特有的，它是一种分布式事务的策略。其本质说来也很通俗，就是一个人的时候好办，多个人的时候，为了保持大家步调一致，每个人准备好以后吼一声，都准备好了才能继续下一步。两阶段提交如何保证 crash-safe？crash-safe具体提供了什么能力？  只要客户端收到事务成功的消息，事务就一定持久化了  只要客户端收到事务失败的消息，事务就一定失败了  如果客户端收到“执行异常”之类的消息，需要应用重连后查询当前状态来执行后续逻辑。数据库内部只要保证数据和日志、主库和备库之间一致就行了具体如何实现？1、redo log 状态为commit，直接提交事务2、redo log 状态为prepare，查看对于的binlog是否完整：​      a.  不完整，回滚​      b.  完整，提交事务如何知道binlog是否完整？完整的binlog最后会有个 XID EVENT 和  COMMIT此外，还提供了一个 binlog-checksum 参数用于校验binlog的完整性。redo log 是怎么和 binlog 对应起来的？它们都有一个共同的字段 XID为什么要引入两个日志，只用binlog 不就行了？（现在只能理解到这个程度）binlog不具备 crash-safe 的能力1、binlog 中没有checkpoint，不能区分哪些是已经落盘到磁盘数据文件（即最终存储数据的文件），哪些是需要应用到内存中用于崩溃恢复的2、binlog中存储的是逻辑日志，即sql级别的语句。redo-log中存储的是物理日志，即数据页级别的改动。一个sql语句可能会更改到好几个数据页，如果单个数据页损坏，binlog是没有能力恢复单个数据页的，它只能应用整个sql语句。比如一个sql语句同时更改了ABC三个数据页，更新的时候发生了crash，B数据页没有正常更新，AC正常更新。如果使用binlog来恢复，它只能同时恢复ABC三个数据页，结果就是B恢复了，AC又不对了。简而言之，binlog粒度太大。为什么要写2个日志，直接更新数据文件不是更快吗？这种思路叫做 write ahead log，简称 WAL，是数据库的通用技术，主要基于以下两点：  顺序写快于随机写。写日志文件都是顺序写，而直接更新数据文件大多数都是随机写，在机械硬盘时代这个速度差异非常大。  虽然是写2个日志，看似意味着一次事务提交要经历两次刷盘，实际上利用了 组提交 的策略，fsync 的次数会大大减少，而write 到 操作系统page cache 基本上可以认为和写到内存差不多，可认为只有 fsync 会占用磁盘的 IOPS。说下组提交在双1配置下，即innodb_flush_log_at_trx_commit和 sync_binlog 都为1，也就是默认情况下，redo log 会在 prepare 阶段 fsync。结合两阶段提交，写binlog其实也分为两步：write 和 fsync。MySQL做了一个优化：拖时间。即将redo log 的 fsync 放到了 binlog的 write 之后、fsync之前。这样的话，由于redo log write 完之后没有立即 fsync，而是等了一步：等binlog write完，所以有机会可以积累更多的redo log 到 page cache中，然后再一并进行 fsync。与此同时，也给了binlog  组提交的机会，因为binlog fsync 之前要等待redo log fsync。但是由于通常情况下 redo log fsync会很快，所以binlog 组提交的效果不如 redo log 的明显。但是可以通过调整以下参数优化：binlog_group_commit_sync_delay：表示延迟多少毫秒以后才调用 fsyncbinlog_group_commit_no_delay_count：表示积累多少次以后才调用 fsync第一个参数为0时，第二个参数无效。如何提高MySQL的IO性能？从提高写redo log 和 binlog 的性能入手。redo log：  增大redo log buffer 大小  增大redo log file 大小  将 innodb_flush_log_at_trx_commt 设为2，表示每次事务提交都只 write 到 page cache，由后台定时线程来fsync。这样做的风险是主机掉电重启后会丢失数据（正常关闭不影响，MySQL在正常关闭前会完成一系列收尾工作）binlog：  将 sync_binlog 设为大于1的值（通常是100-1000），这样做的风险是主机掉电会丢失binlog日志  调整 binlog_group_commit_sync_delay 和 binlog_group_commit_no_delay_count，提高binlog组提交效率，减少写盘次数。这样做会增加语句的响应时间，但是不会有丢失数据的风险sql语句突然变慢了？有时会遇到这种情况，一条sql平时执行都很快，但有时候不知道为什么突然就变得很慢，而且这个情况还是随机的、持续时间也很短，很难复现。突然变慢的这一瞬间很有可能是MySQL在刷脏页。以下4种情况会引发刷脏页：  当需要读入新的内存页时，如果系统内存不足，就需要淘汰旧的数据页。如果淘汰的是脏页，就需要把脏页刷到磁盘。当一次淘汰的脏页太多时，就会明显影响性能。  redo log 写满了。此时需要把redo log的 checkpoint 往前推进，并把 write pos 到新的 checkpoint 之间对应的脏页刷到磁盘。这种情况是很严重的，此时MySQL将不再接受更新，所有更新都会堵住。  后台任务 purge 会在系统空闲时刷脏页。  MySQL正常关闭时。解决思路：主要解决前两种情况，后两种情况不会影响到系统性能。对于情况二，调整 redo log 的大小即可，默认值明显太小，上文有讲过。对于情况一，需要先了解下Innodb刷脏页的控制策略和相关参数。这里主要的参数是 innodb_io_capacity，从名字就可以看出是控制 IO 能力的，官方描述如下：  The innodb_io_capacity variable defines the number of I/O operations per second (IOPS) available to InnoDB background tasks, such as flushing pages from the buffer pool and merging data from the change buffer.简单说就是设置 IOPS，它用来告诉 Innodb 所在主机的 IO 能力，用于后台任务刷脏页和 change buffer 的 merge。默认值是200，即 Innodb 全力刷脏页可以达到 200 IOPS。这个值明显太低了，即便是对于机械硬盘。当然，Innodb 不会完全按照这个值去刷脏页，因为系统还需要处理服务请求。Innodb的做法是会算出一个百分比，然后按 innodb_io_capacity 定义的能力乘以这个百分比来控制刷脏页的速度。百分比的大致算法：      根据当前脏页比例再结合 innodb_max_dirty_pages_pct(脏页比例上限，默认值是90) 算出一个 0到100 之间的数字          脏页比例可通过下面命令得到：      123select VARIABLE_VALUE into a from global_status where VARIABLE_NAME = &#39;Innodb_buffer_pool_pages_dirty&#39;;select VARIABLE_VALUE into b from global_status where VARIABLE_NAME = &#39;Innodb_buffer_pool_pages_total&#39;;select a/b;            计算方法伪代码如下，M 为当前脏页比例：      123456F1(M)  if Mgt;=innodb_max_dirty_pages_pct then      return 100;  return 100*M/innodb_max_dirty_pages_pct;                  InnoDB 每次写入的日志都有一个序号(LSN)，当前写入的序号跟 checkpoint 对应的序号之间的差值，假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，N 越大，算出来的值越大。        取2值较大者作为百分比  从这个过程可以看出，可以介入的部分就是两个参数：innodb_io_capacity 和 innodb_max_dirty_pages_pct。      innodb_io_capacity    建议设置为磁盘的IOPS，磁盘的IOPS可以通过下面的命令来测试，一般参考测试结果中 write 的能力来设置：    1fio -filename=filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest         注意：一定要熟悉 fio 的使用方法，否则可能会把盘都刷了！！！注意，注意，注意，危险的事情说三遍！！！    参考：fio 命令入门到跑路        innodb_max_dirty_pages_pct    从上面的伪代码可以看出，当脏页比例≥该参数值时，第一个参数为100；否则取脏页比例/该参数值的百分比。MySQL8.0以后该值默认为90，也就是说当脏页比例≥90时，会百分百按 innodb_io_capacity  的能力全力刷脏页。为了尽量避免因刷脏页引起的抖动，应经常关注脏页比例，不要让它经常接近90。或者可以调高一点(其实该值已经调整过了，之前是75，8.0后调为了90。90已经接近100了，可认为该值是一个很合理的值，如果脏页累计过多，刷脏页就会很频繁)。  除此之外，还有一个参数 innodb_flush_neighors，从参数名字可以看出大概意思：刷脏页的时候是只刷自己还是连着附近的脏页也一起刷了，1表示启用，0表示只刷自己。这个机制对于传统机械硬盘很有用，机械硬盘的IOPS一般只有几百，每次多刷一些可以减少很多随机IO。而SSD的随机IO已不是瓶颈，只刷自己反而会更快些。在MySQL8.0中，该参数的默认值已为0，表示只刷自己。"
} ,

{
"title"    : "MySQL学习笔记(二)：MySQL权限控制",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/08/23/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%8C)-MySQL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/",
"date"     : "2021-08-23 00:00:00 +0800",
"content"  : "MySQL权限可分为全局级、数据库级、表级、列级、子程序级（函数、存储过程）全局级1select * from mysql.user;数据库级1select * from mysql.db;表级1select * from mysql.tables_priv;列级1select * from mysql.columns_priv;子程序级1select * from procs_priv;也可通过 show grants for user 查询授权语句示例12345678910111213141516171819202122创建用户：create user &#39;test&#39;&#39;&#39; identified by  &#39;test&#39;;授予全局级查询权限：grant select on *.* to &#39;test&#39;&#39;&#39;;授予数据库级查询权限：grant select on test.* to &#39;test&#39;&#39;&#39;;授予表级查询权限：grant select on mysql.user to &#39;test&#39;&#39;&#39;;授予表级查询权限：grant select on mysql.user to &#39;test&#39;&#39;&#39;;授予列级权限（只能查询person表的name字段）：grant select(name) on test.person to &#39;test&#39;&#39;&#39;;撤销权限：revoke select on *.* from &#39;test&#39;&#39;&#39;;刷新权限：flush privileges;"
} ,

{
"title"    : "MySQL学习笔记(一)：一条sql查询语句是如何执行的？",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/08/18/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)-%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/",
"date"     : "2021-08-18 00:00:00 +0800",
"content"  : "先放一张MySQL架构图，有助理解，一条sql查询语句经历了从上到下的以下过程。整体可分为两大块：Server层和存储引擎层。所有跨存储引擎层的功能都在Server层实现，如内置函数（日期、时间、数学、加密函数等）、存储过程、触发器、视图等。连接器作用：建立连接、检查权限、维持和管理连接建立连接时会去查询是否有相应权限，之后的所有权限判断逻辑都依赖于此时读到的权限。这也就意味着如果在建立连接后再对用户的权限做更改，只会在重新连接后才生效。todo：怎么查询权限？连接建立后如果没有后续动作，该连接就处于空闲状态，可以在 show processlist 中看到它。Sleep表示当前系统中有一个空闲连接。如果长时间没有动作，连接就会断开。由参数wait_timeout控制，默认是8小时。长连接 vs 短连接长连接和短连接是一种行为，并不是由某个参数控制。如果一个连接建立后，一直有后续行为，它就会一直维持连接状态。如果建立连接做出某些动作以后就断开，它就是短连接。长连接后续的所有操作使用的内存都是管理在该长连接的对象里，直到连接断开才会释放。所以有时候执行一些比较大的占用内存的操作后，长连接一直累积，可能会导致OOM，MySQL重启。查询缓存MySQL 8 以后将整个缓存模块删除了，不再提供查询缓存功能。缓存顾名思义是用来提高查询速度的，但是由于缓存失效太频繁，只要对表有更新，会导致整个表的缓存失效，然后又要重新建立缓存，所以最后权衡下来把缓存模块拿掉了。但是对于静态表，读远远多于写的表，比如配置表，可以使用缓存。对于还提供查询缓存的版本，可以将 query_cache_type 设置为 DEMAND，即按需使用，在需要的时候加上 SQL_CAHCE 即可。1select SQL_CACHE * from test.person;分析器主要作用： 词法分析和语法分析。词法分析：分析每个单词代表什么含义，比如 select 代表查询，t 表示表名，id 表示列名。语法分析：分析语句是否符合sql语法规范，经常看见的 You have an error in your SQL syntax 就是在这个阶段。优化器主要作用：决定选择哪个索引或者join的连接顺序。不同的索引、不同的表连接顺序会对执行效率有很大影响。执行器主要作用：调用存储引擎提供的接口完成操作，如 取 id=1 的数据。存储引擎是以插件的形式接入MySQL的，可以把存储引擎看作一个黑盒，它对外提供了很多接口，只需要调用即可。mysql.slow_log 中会看到两个值，一个是 row_examined，表示调用存储引擎接口的次数，Server层面。另一个是引擎内部的扫描行数。这两个值并不相同。"
} ,

{
"title"    : "自己写一个starter",
"category" : "",
"tags"     : "springboot, starter",
"url"      : "/blog/2021/08/15/%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AAstarter/",
"date"     : "2021-08-15 00:00:00 +0800",
"content"  : "一个 starter 就是一个提供特定功能的库，它主要做了几件事：  添加了一些依赖  定义了一些属性（在application.properties 或 application.yml 中使用），可以理解为一些功能的开关，或者是初始值  根据某些条件动态的定义一些bean，这些 bean 就是提供功能的对象，给客户端使用目标：实现一个starter，提供 json 序列化功能，有两种序列化方式可供选择：fastjson 和 gson使用场景1： 由用户在属性文件中选择使用哪一种使用场景2： 由用户添加某个具体实现的依赖来选择使用哪一种使用场景1： 由用户在属性文件中选择使用哪一种      新建一个maven项目，命名为 json-spring-boot-starter（所有第三方库都命名为xxx-spring-boot-starter，spring 自己的 starter 命名为 spring-boot-starter-xxx）    pom 文件如下，重点是引入的依赖：spring-boot-autoconfigure、fastjson、gson    123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263lt;?xml version=1.0 encoding=UTF-8?gt;lt;project xmlns=http://maven.apache.org/POM/4.0.0 xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance         xsi:schemaLocation=http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsdgt;    lt;modelVersiongt;4.0.0lt;/modelVersiongt;    lt;groupIdgt;com.examplelt;/groupIdgt;    lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;    lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;    lt;namegt;json-spring-boot-starterlt;/namegt;    lt;descriptiongt;Json Serializer Starterlt;/descriptiongt;       lt;propertiesgt;        lt;java.versiongt;11lt;/java.versiongt;    lt;/propertiesgt;       lt;dependencyManagementgt;        lt;dependenciesgt;            lt;dependencygt;                lt;groupIdgt;org.springframework.bootlt;/groupIdgt;                lt;artifactIdgt;spring-boot-dependencieslt;/artifactIdgt;                lt;versiongt;2.5.2lt;/versiongt;                lt;typegt;pomlt;/typegt;                lt;scopegt;importlt;/scopegt;            lt;/dependencygt;        lt;/dependenciesgt;    lt;/dependencyManagementgt;       lt;dependenciesgt;        lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-autoconfigurelt;/artifactIdgt;        lt;/dependencygt;           lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;        lt;/dependencygt;           lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-starter-testlt;/artifactIdgt;            lt;scopegt;testlt;/scopegt;        lt;/dependencygt;    lt;/dependenciesgt;       lt;buildgt;        lt;pluginsgt;            lt;plugingt;                lt;groupIdgt;org.apache.maven.pluginslt;/groupIdgt;                lt;artifactIdgt;maven-compiler-pluginlt;/artifactIdgt;                lt;versiongt;3.8.1lt;/versiongt;                lt;configurationgt;                    lt;sourcegt;11lt;/sourcegt;                    lt;targetgt;11lt;/targetgt;                lt;/configurationgt;            lt;/plugingt;        lt;/pluginsgt;    lt;/buildgt;lt;/projectgt;            定义一个 JsonSerializer 接口，提供两种实现    123public interface JsonSerializer     String serialize(Object obj);        123456public class FastjsonSerializer implements JsonSerializer     Override    public String serialize(Object obj)         return Fastjson:  + JSON.toJSONString(obj);            1234567public class GsonSerializer implements JsonSerializer     Override    public String serialize(Object obj)         Gson gson = new Gson();        return Gson:  + gson.toJson(obj);            ​        定义一个 JsonTemplate，这是最终用户要使用的工具类  1234567891011   public class JsonTemplate        private final JsonSerializer serializer;          public JsonTemplate(JsonSerializer serializer)            this.serializer = serializer;                 public String serialize(Object obj)            return this.serializer.serialize(obj);          ​  实现自动配置，这里根据属性来选择，属性名定义为 json.serializer.type，用到ConditionalOnProperty 注解。没有指定属性时使用 fastjson（matchIfMissing = true）1234567891011121314151617181920212223   Configuration   public class JsonSerializerAutoConfiguration        Bean       public JsonTemplate jsonTemplate(JsonSerializer serializer)            return new JsonTemplate(serializer);                 Bean       ConditionalOnProperty(               prefix = json.serializer,               name = type,               havingValue = fastjson,               matchIfMissing = true)       public JsonSerializer fastjsonSerializer()            return new FastjsonSerializer();                 Bean       ConditionalOnProperty(prefix = json.serializer, name = type, havingValue = gson)       public JsonSerializer gsonSerializer()            return new GsonSerializer();          ​  在 resources 下创建一个文件 META-INF/spring.factories12   org.springframework.boot.autoconfigure.EnableAutoConfiguration=#92;   com.example.json.JsonSerializerAutoConfigurationmvn install 到本地，接下来写客户端验证一下12345        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;12345678910111213141516171819SpringBootApplicationSlf4jpublic class StarterClientApplication implements CommandLineRunner     Autowired private JsonTemplate jsonTemplate;    public static void main(String[] args)         SpringApplication.run(StarterClientApplication.class, args);        Override    public void run(String... args) throws Exception         Person person = new Person();        person.setName(张三);        person.setAge(30);        log.info(jsonTemplate.serialize(person));    没有指定属性时，输出为：1Fastjson: age:30,name:张三指定属性为json时：123json:  serializer:    type: gson1Gson: name:张三,age:30为属性文件加上智能提示      为属性创建一个类，打上 ConfigurationProperties 注解    12345678910111213141516171819ConfigurationProperties(prefix = json.serializer)public class JsonSerializerProperties        /** 序列化类型：fastjson 和 gson */    private Type type;       public Type getType()         return type;           public void setType(Type type)         this.type = type;           public enum Type         FASTJSON,        GSON                改造自动配置类    12345678910111213ConfigurationEnableConfigurationProperties(JsonSerializerProperties.class)public class JsonSerializerAutoConfiguration     Bean    public JsonTemplate jsonTemplate(JsonSerializerProperties serializerProperties)         JsonSerializerProperties.Type type = serializerProperties.getType();        JsonSerializer serializer =                (type == null || JsonSerializerProperties.Type.FASTJSON.equals(type))                        ? new FastjsonSerializer()                        : new GsonSerializer();        return new JsonTemplate(serializer);                加入spring-boot-configuration-processor依赖，注意一定不要搞错了，之前我没注意写成 spring-boot-autoconfigure-processor，害我调半天    12345        lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-configuration-processorlt;/artifactIdgt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;        这个注解会在 META-INF 下生成一个元数据文件 spring-configuration-metadata.json    123456789101112131415161718  groups: [          name: json.serializer,      type: com.example.json.JsonSerializerProperties,      sourceType: com.example.json.JsonSerializerProperties      ],  properties: [          name: json.serializer.type,      type: com.example.json.JsonSerializerPropertiesType,      description: 序列化类型：fastjson 和 gson,      sourceType: com.example.json.JsonSerializerProperties      ],  hints: []        description 来自字段上的注释    12/** 序列化类型：fastjson 和 gson */    private Type type;        可以手动编辑该json文件，加上 defaultValue。  使用场景2：由用户添加某个具体实现的依赖来选择使用哪一种      改造自动配置类，用到 ConditionalOnClass注解。注意这里bean定义的顺序，JsonTemplate一定要放在最后，因为它依赖于JsonSerializer    12345678910111213141516171819202122ConfigurationEnableConfigurationProperties(JsonSerializerProperties.class)public class JsonSerializerAutoConfiguration        Bean    ConditionalOnClass(JSON.class)    Primary    public JsonSerializer fastjsonSerializer()         return new FastjsonSerializer();           Bean    ConditionalOnClass(Gson.class)    public JsonSerializer gsonSerializer()         return new GsonSerializer();           Bean    public JsonTemplate jsonTemplate(JsonSerializer serializer)         return new JsonTemplate(serializer);                将 fastjson 和 gson 依赖设置成可选项    1234567891011        lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;      测试一把，客户端先添加fastjson的依赖12345678910        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;        lt;/dependencygt;输出为：1Fastjson: age:30,name:张三换成gson123456789        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;        lt;/dependencygt;输出为：1Gson: name:张三,age:30如果两个依赖都没有添加，会报错提示：123Action:Consider defining a bean of type &#39;com.example.json.JsonSerializer&#39; in your configuration.这个信息也可以自定义成更友好的提示信息，这一part暂时不研究了，结束！"
} ,

{
"title"    : "正确理解浮点数",
"category" : "",
"tags"     : "浮点数, 数据类型",
"url"      : "/blog/2021/04/25/float/",
"date"     : "2021-04-25 00:00:00 +0800",
"content"  : "首先，浮点数是一种数字的表示方式，而不是指小数很多人一提到浮点数，就说是小数，这是不对的。浮点数只是一种数字的表示方式，数字是多少就是多少，它就在那里不会改变，改变的只是人类表示它们的方式。比如给定一个数 10，能确定它是浮点数还是定点数吗？不能！我们必须知道这个数是如何存储的，即底层是如何来表示这个数的，才能说它是定点数还是浮点数。这其实类似于十进制中的科学计数法，比如 1234.5678这个数，这个数本身不会改变，是多少永远是多少，变的是表示它的方式，我们可以以多种不同的方式来表示这个数：  1.2345678 #92;times 10^3  123.45678 #92;times 10  0.12345678 #92;times 10^4  12345678 #92;times 10^-4可以看到，小数点的位置是浮动的，这才是浮点数名字的由来。再次强调，浮点数指的是表示数字的方式，而不是数字本身。至于大家常提到的IEEE754，它是一个标准，即规定了大家统一采用哪种方式来表示和存储数据，它规定尾数部分格式为 1.xxxx，具体存储时1省略，其他细节不再赘述。为什么要使用浮点数简单来说，同样的位数，浮点数表示的范围更大比如 int 和 float 同是 32bitint的最大值为 2^31-1 = 2.147483647E9 = 2147483647float的最大值为 (2-2^23)#92;cdot2^127 = 0#92;mboxx1.fffffeP+127f = 3.4028235E38f         这表示范围可比 int 大多了为什么float的精度是6~7位？因为float的尾数部分有23位，即可以用23位二进制来表示一个数，指数部分不影响精度，只影响范围，因为有效数字是从第一个非0位算起。事实上还要加上规格化尾数省略的1，所以问题就变成24位二进制位可以表示多少位十进制？#92;[#92;beginalign*2^24  amp;= 10^x#92;#92;x  amp;= log_102^24 = 24 #92;cdot log_102 = 7.22471989594#92;endalign*#92;]12float f = 0.123456789f;System.out.println(f); // 0.12345679  只能保证7位精度"
} ,

{
"title"    : "Git基本原理",
"category" : "",
"tags"     : "Git",
"url"      : "/blog/2021/04/13/git/",
"date"     : "2021-04-13 00:00:00 +0800",
"content"  : "把 Git 看作是一个文件系统，这很重要，事实上它就是一个小型的文件系统。先创建一个空目录my_repo，进入该目录，接下来的所有操作都在这个目录内，建议按顺序阅读。git init 做了什么git init 会在当前目录内创建一个 .git 目录，包含如下内容1234567891011121314151617181920212223242526272829❯ cd .git❯ tree ..├── branches├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── info│   └── exclude├── objects│   ├── info│   └── pack└── refs    ├── heads    └── tags看看 git status 的输出1234567❯ cd ..❯ git status位于分支 master尚无提交无文件要提交（创建/拷贝文件并使用 git add 建立跟踪）告诉我们当前位于分支 master，git 是如何知道的，其实是存储在 HEAD 文件中12❯ cat .git/HEADref: refs/heads/master再看 git branch，结果是空，因为 .git/refs/heads 下还没有任何东西，这里就是存储分支的地方git add 做了什么创建一个文件1234❯ echo 123 gt; a.txt❯ ll总用量 4-rw-r--r-- 1 head head 4  4月 13 23:51 a.txt此时 .git 中的内容没有发生任何变化执行 git add a.txt 后再查看 .git 中的内容123456789101112131415161718192021222324252627282930313233❯ git add a.txt❯ tree .git.git├── branches├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── info│   └── pack└── refs    ├── heads    └── tagsobjects 中有新增的东西，用 git cat-file 看一下1234❯ git cat-file -t 190a18037c64c43e6b11489df4bf0b9eb6d2c9bfblob❯ git cat-file -p 190a18037c64c43e6b11489df4bf0b9eb6d2c9bf123可以看到该文件类型是 blob，文件内容是 123，就是 a.txt 的内容。blob是 git 文件系统 中的一种文件类型，即二进制大对象，用于存储文件的内容 (注意，只保存文件的内容，不保存metadata如文件名、创建时间等信息)，文件内容使用 zlib的deflat 压缩算法，文件名采用 sha-1 哈希算法(20个字节，160个bit，40个十六进制)，取第1个字节作目录名，剩下19个字节作文件名。所以 objects 下最多会有 256 个目录，这样做是为了提高查找速度。除此之外，还多了一个 index 文件，也就是常说的 stage area (暂存区)，它们是一个意思。  The index is a binary file (generally kept in . git/index ) containing a sorted list of path names, each with permissions and the SHA1 of a blob object; git ls-files can show you the contents of the index. Please note that words index, stage, and cache are the same thing in Git: they are used interchangeably.可以用 git ls-files  查看12❯ git ls-filesa.txt在这里存储了相关的metadata，可以家 --debug 查看1234567❯ git ls-files --debuga.txt  ctime: 1618345384:304625740  mtime: 1618345384:304625740  dev: 65024 ino: 20849651  uid: 1000  gid: 1000  size: 8 flags: 0查看 git status12345678❯ git status位于分支 master尚无提交要提交的变更：  （使用 git rm --cached lt;文件gt;... 以取消暂存）        新文件：   a.txt试试取消暂存1234567891011121314❯ git rm --cached a.txtrm &#39;a.txt&#39;❯ ❯ ❯ git status位于分支 master尚无提交未跟踪的文件:  （使用 git add lt;文件gt;... 以包含要提交的内容）        a.txt提交为空，但是存在尚未跟踪的文件（使用 git add 建立跟踪）再次查看 index 文件12❯ git ls-files没有内容所以常说的 git add 把文件添加到暂存区就是这个意思其他部分没有变化git commit 做了什么1234❯ git commit -m add a.txt[master（根提交） bb97804] add a.txt 1 file changed, 1 insertion(+) create mode 100644 a.txt查看 .git123456789101112131415161718192021222324252627282930313233343536373839404142❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tagsobjects 中多了2个文件，分别看下是什么12345678❯ git cat-file -t bb978046c355c1fa2875c6a8473cea4a60d55814commit❯ git cat-file -p bb978046c355c1fa2875c6a8473cea4a60d55814tree c4903888e91347c58a530c1f1987dfc4d203960aauthor lxh lt;452228391qq.comgt; 1618338183 +0800committer lxh lt;452228391qq.comgt; 1618338183 +0800add a.txt可以看到该文件类型是 commit，commit 是 git 中另一种文件类型，包含了 author、commit message 等信息。还包含了另一种叫作 tree 的文件类型，也就是多出来的另一个文件1234❯ git cat-file -t c4903888e91347c58a530c1f1987dfc4d203960atree❯ git cat-file -p c4903888e91347c58a530c1f1987dfc4d203960a100644 blob 190a18037c64c43e6b11489df4bf0b9eb6d2c9bf    a.txttree 即目录树的意思，它可以包含 blob 和 tree ，即可以包含文件和子目录，就和文件系统是一样的。所以 commit 可以理解为当前时刻的文件系统快照 snapshot，这一点很重要，所以 git 可以回到任一时刻的状态。除此之外，refs/heads 下也多出来一个文件 master，看下是什么12❯ cat .git/refs/heads/masterbb978046c355c1fa2875c6a8473cea4a60d55814所以 master 只是一个存储了 commit 文件名的文件，表示指向当前最新的 commit。所以 master 只是一个别名而已，同理任何分支都是这样，比如 dev 分支。它的作用是 HEAD 会指向它，用来表示当前是在哪个分支上。12❯ cat .git/HEADref: refs/heads/master改动文件会发生什么往 a.txt 中新增内容12345678910❯ echo 456 gt;gt; a.txt❯ less a.txt❯ git status位于分支 master尚未暂存以备提交的变更：  （使用 git add lt;文件gt;... 更新要提交的内容）  （使用 git restore lt;文件gt;... 丢弃工作区的改动）        修改：     a.txt修改尚未加入提交（使用 git add 和/或 git commit -a）执行 add1234567891011121314151617181920212223242526272829303132333435363738394041424344454647❯ git add a.txt❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── ce│   │   └── 8c77db7f732ddc56661bc5f5cae2e1198978b1│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tags16 directories, 26 filesobjects 中多出了一个文件12345❯ git cat-file -t ce8c77db7f732ddc56661bc5f5cae2e1198978b1blob❯ git cat-file -p ce8c77db7f732ddc56661bc5f5cae2e1198978b1123456就是 a.txt 最新的内容，而原来的 a.txt 依然还在(190a18037c64c43e6b11489df4bf0b9eb6d2c9bf)，所以 git 保存的是整个文件，而不是只是变化的部分，这也是 commit 的基础，所以 git 能回到任一时刻的状态。执行提交123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051❯ git commit -m modify a.txt[master 294a462] modify a.txt 1 file changed, 1 insertion(+)❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── 29│   │   └── 4a462e98ac84b47522beb5a6c5b795bad599c2│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── ce│   │   └── 8c77db7f732ddc56661bc5f5cae2e1198978b1│   ├── e9│   │   └── bbf146022722173fb1c459daf3a03f211ad3ad│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tagsobjects 下又多了两个文件，分别看一下123456789❯ git cat-file -p 294a462e98ac84b47522beb5a6c5b795bad599c2tree e9bbf146022722173fb1c459daf3a03f211ad3adparent bb978046c355c1fa2875c6a8473cea4a60d55814author lxh lt;452228391qq.comgt; 1618340817 +0800committer lxh lt;452228391qq.comgt; 1618340817 +0800modify a.txt❯ git cat-file -t 294a462e98ac84b47522beb5a6c5b795bad599c2commit这是我们刚刚的提交，里面又多了一个 tree1234❯ git cat-file -t e9bbf146022722173fb1c459daf3a03f211ad3adtree❯ git cat-file -p e9bbf146022722173fb1c459daf3a03f211ad3ad100644 blob ce8c77db7f732ddc56661bc5f5cae2e1198978b1    a.txt感觉似曾眼熟，这不跟 c4903888e91347c58a530c1f1987dfc4d203960a 重复了吗？事实上，基于 hash 的算法，只要内容有任何改动，哈希结果都会发生变化。原tree  中包含了 a.txt，a.txt 发生了变化会导致原 tree 的哈希结果也发生变化，所以产生了一个新的 tree。可能你觉得这里有点多余，我们换个场景就明白了。( 另外可以自行查看一下，refs/heads/master 中的内容，现在应该是指向了这个新的 commit)删除 a.txt12rm a.txtgit add .此时 objects 没有发生任何变化，只有 index 发生了变化12❯ git ls-filesindex 里没有文件了，即暂存区里没有文件了，因为我们把它删除了，显而易见，工作区里也不会有这个文件了1234❯ pwd/home/head/code/my_repo❯ ll总用量 0提交123456❯ git commit -m delete a.txt[master 138a68a] delete a.txt 1 file changed, 2 deletions(-) delete mode 100644 a.txt❯ ll总用量 0objects 下又多了两个文件，这里就不展示目录树了123456789101112❯ git cat-file -t 138a68ac42b5fde646849df01c1339cfaec874c3commit❯ git cat-file -p 138a68ac42b5fde646849df01c1339cfaec874c3tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904parent 294a462e98ac84b47522beb5a6c5b795bad599c2author lxh lt;452228391qq.comgt; 1618342629 +0800committer lxh lt;452228391qq.comgt; 1618342629 +0800delete a.txt❯ git cat-file -t 4b825dc642cb6eb9a060e54bf8d69288fbee4904tree❯ git cat-file -p 4b825dc642cb6eb9a060e54bf8d69288fbee4904一个是最新的提交，一个是最新的tree，这个tree里没有任何东西现在我们来找回这个文件12345678910111213141516171819202122232425262728293031323334353637383940gt; git logcommit 138a68ac42b5fde646849df01c1339cfaec874c3 (HEAD -gt; master)Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 03:37:09 2021 +0800    delete a.txtcommit 294a462e98ac84b47522beb5a6c5b795bad599c2Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 03:06:57 2021 +0800    modify a.txtcommit bb978046c355c1fa2875c6a8473cea4a60d55814Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 02:23:03 2021 +0800    add a.txt    ❯ git reset 294a462e98ac84b47522beb5a6c5b795bad599c2重置后取消暂存的变更：D       a.txt❯ git status位于分支 master尚未暂存以备提交的变更：  （使用 git add/rm lt;文件gt;... 更新要提交的内容）  （使用 git restore lt;文件gt;... 丢弃工作区的改动）        删除：     a.txt修改尚未加入提交（使用 git add 和/或 git commit -a）❯ git restore .❯ ll总用量 4-rw-r--r-- 1 head head 8  4月 14 03:43 a.txt❯ cat a.txt123456所以，由于保存了任一时刻的快照，便能很容易的恢复到任一时刻的文件系统状态。所以，得把 tree理解为系统快照，是带状态的一种对象，不能以普通文件系统的视角，虽然是同一个目录，但是有文件和没文件是两种状态；即便是同一个文件，文件改变了，那整个目录也是一种新的状态。这就是时光穿梭机！git checkout 做了什么git checkout -b dev 会在 refs/heads 下新增一个 dev 文件，文件内容为当前最新 commit ，此时应该和 master 内容是一样的，两个分支此时的状态完全一致，objects 下没有任何变化。除此之外，HEAD 的内容会更改为 ref refs/heads/dev ，表示当前分支为 dev 分支。这就是 checkout 所做的全部内容了。所以分支只是一个别名而已，指向不同的 commit，以表示不同时刻的文件系统状态。从 git 可以学到什么      链表(指针)          branch -gt; commit -gt; tree -gt; blob (subtree)      branch 只是一个 commit 的别名，或者叫指针也行      commit 如果不是第一个提交的话，还有一个 parent 属性，表示之前一个提交      git 的整个操作过程简化来说就是操作 HEAD 指针在各个 commit 之间跳转，然后根据 commit 中的 tree “渲染” 出整个文件系统            哈希          基于哈希的消息摘要能快速的标识出整个文件系统，而且 SHA-1 的碰撞概率在版本控制这个用途上来说基本可以认为不会发生      See Also Git是否考虑到SHA1碰撞的问题了？            存储整体而不是存储变化，好像 React 基本思想也是这样(当状态变化时重新渲染组件，React也可以进行时光穿梭)          这里的整体指的是文件和相关联的tree这个整体，而不是整个文件系统。其他未发生变化的blob和tree直接引用就行。      常用命令本地创建分支并推送到远程分支1git push origin branch:branch --- 本地分支与远程分支要同名删除远程分支1git push origin --delete branchSee AlsoGit internals"
} ,

{
"title"    : "Manjaro安装配置踩坑",
"category" : "",
"tags"     : "Manjaro",
"url"      : "/blog/2021/04/12/manjaro-install/",
"date"     : "2021-04-12 00:00:00 +0800",
"content"  : "LVM方式的安装我安装的版本是Manjaro 21 KDE，常规安装按图形界面方式很快，没有任何问题。但 calamares当前版本对 lvm 的支持还不是很好，直接在图形界面采用 lvm 点下一步后会马上闪退，或者进入安装过程后报各种卷创建的错误，实在无招，只能曲线救国。最开始手动创建卷，然后进图形界面选择文件系统和挂载点，安装倒是没问题了，最后也提示安装成功。但重启后找不到系统，还是会继续进入U盘。推测是 calamares 在 lvm 方式下创建 efi 分区有bug，gitHub上也有很多相关issue。试了N遍之后，最后的办法是先按常规方式安装一遍，保证重启后能正常进入系统。再用U盘进入live environment，在已分区的基础上手动创建物理卷，卷组和逻辑卷。efi 和 swap 不要动(注意在后续图形安装界面中 efi 千万别选择格式化，要的就是保留 efi 之前的内容)，其他分区可自由分配卷组和逻辑卷。最后进入图形安装界面选择挂载点和文件系统安装即可。Intel核显 + NVIDIA独显 的问题  Linus Torvalds:  NVIDIA, fuck u!安装完之后一开始正常使用没什么问题，但是一竖屏后就有撕裂现象。解决办法：安装 optimus-manager 和 optimus-manager-qt ，切换为 NVIDIA 之后就没问题了。大黄蜂方案已经过时且不再维护了，不建议使用。optimus-manager ( A Linux program to handle GPU switching on Optimus laptops)文档中有关于 Gnome and GDM users 和 IMPORTANT : Manjaro KDE users 的注意事项，一定要看，否则可能会导致切换后黑屏、无法启动的问题，我在这浪费了好多时间…换源sudo pacman-mirros -i -c China -m rank       选中一个sudo pacman -Syy      更新软件源sudo pacman -Syu      更新全部软件安装搜狗拼音添加 archlinuxcn 软件源，/etc/pacman.conf 在最后加入SigLevel = Optional TrustAll Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/archsudo pacman -Syy      更新软件源sudo pacman -S archlinux-keyring     更新软件密钥sudo pacman -S fcitx-im     安装输入法框架sudo pacman -S fcitx-configtool     安装输入法配置工具在软件中心找到并安装 fcitx-sogoupinyin( 注意：一定要按顺序安装，不能直接安装最后一个，会导致依赖缺失，不能配置等问题)在 /etc/environment 中加入123GTK_IM_MODULE=fcitxQT_IM_MODULE=fcitxXMODIFIERS=im=fcitx重启完成安装微信文档写的很详细：deepin-wine-wechat-arch中文字体显示为框框：  下载宋体字符集文件：https://www.freefonts.io/downloads/simsun/  将解压后的文件复制到如下路径：cp    SIMSUN.ttf    ~/.deepinwine/Deepin-WeChat/drive_c/windows/Fonts/  重启微信使用 oh-my-zshhttps://gist.github.com/yovko/becf16eecd3a1f69a4e320a95689249e主题美化IDEA Ultimate 输入法候选框不跟随鼠标，一直在左下角这是一个 idea 古老又没人管的bug解决办法参考：https://ld246.com/article/1601280084643下载修改过的  JetBrainsRuntime ，在 idea.sh 启动脚本开头添加export IDEA_JDK=xport IDEA_JDK=目录/java-11.0.7-jetbrain但是有个新的问题，markdown文件 的预览按钮不见了，也懒得折腾了，写 markdown 用专门的编辑器吧，比如 TypoarIDEA 不会显示全局系统菜单Manjaro安装了 Application Title 插件后所有应用的菜单都会在顶部的全局系统菜单显示，但 IDEA 不会。官方解决办法： idea中安装 JavaFX Runtime for Plugins 插件。但是有点延迟，打开 idea 后要过一会菜单才会出来，也是很烦。美化Make Your KDE Plasma Desktop Look Better参考着来，不需要完全照搬pamac 安装软件超时查看安装超时的地址，手动下载后放到 /var/tmp/pamac-build- lt; usernamegt; / lt; AppNamegt; ` 目录下，再重新安装即可。VMware Workstation第一次启动虚拟机时报错Cannot open /dev/vmmon: No such file or directory. Please make sure that the kernel module vmmon&#39; is loaded。 原因是内核中没有vmmon模块。首先推荐使用LTS版本，其次较新的版本可能还没有 vmmon模块的补丁。执行以下命令安装 linux-headers：1  sudo pacman -S (pacman -Qsq ^linux | grep ^linux[0-9]*[-rt]* | awk &#39;print 1-headers&#39; ORS=&#39; &#39;)重启后执行：1  sudo vmware-modconfig --console --install-all参考链接1参考链接2 　　打开虚拟网络编辑器报错Fail Network configuration is missing. Ensure that /etc/vmware/-networking exists.执行以下命令启动 vmware-networks-configuration 服务：1  systemctl start vmware-networks-configuration.service虚拟机无网络报错 Could not connect &#39;Ethernet0&#39; to virtual network &#39;/dev/vmnet0&#39;。重装 linux-headers：123  sudo pacman -R linux510-headers    sudo pacman -S linux510-headers重置网卡：1  sudo touch /etc/vmware/x amp;amp; sudo vmware-networks --migrate-network-settings /etc/vmware/x amp;amp; sudo rm /etc/vmware/x amp;amp; sudo modprobe vmnet amp;amp; sudo vmware-networks --start查看网卡状态：1  vmware-networks --status虚拟机设置里选择连接方式和网卡即可。若后面再次 could not connect，执行12systemctl start vmware-networkssystemctl enable vmware-networks保证 vmware-networks 是 loaded 的就行，状态是 inactive 也没关系。重新在虚拟机设置里选择连接方式和网卡即可。"
} 

]