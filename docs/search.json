[

{
"title"    : "SpringMVC方法映射原理解析",
"category" : "",
"tags"     : "Spring",
"url"      : "/blog/2022/03/11/SpringMVC%E6%96%B9%E6%B3%95%E6%98%A0%E5%B0%84%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/",
"date"     : "2022-03-11 00:00:00 +0800",
"content"  : "HandlerMapping接口package：spring-webmvc。顶级接口。用于定义请求和处理对象之间的映射关系。主要方法：1HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception;主要实现类：HandlerMapping（接口）-gt; AbstractHandlerMapping(抽象类) -gt; AbstractHandlerMethodMapping(抽象类) -gt;...省略... -gt; RequestMappingHandlerMapping(实现类)。WebMvcAutoConfiguration自动配置类声明了一个 RequestMappingHandlerMapping bean1234567891011BeanPrimaryOverridepublic RequestMappingHandlerMapping requestMappingHandlerMapping(      Qualifier(mvcContentNegotiationManager) ContentNegotiationManager contentNegotiationManager,      Qualifier(mvcConversionService) FormattingConversionService conversionService,      Qualifier(mvcResourceUrlProvider) ResourceUrlProvider resourceUrlProvider)    // Must be Primary for MvcUriComponentsBuilder to work   return super.requestMappingHandlerMapping(contentNegotiationManager, conversionService,         resourceUrlProvider);RequestMappingHandlerMappingRequestMappingHandlerMapping 同时实现了 InitializingBean 接口，看它的 afterPropertiesSet 方法：1234567OverrideSuppressWarnings(deprecation)public void afterPropertiesSet()    //省略，一些属性设置   super.afterPropertiesSet();看它的父类 AbstractHandlerMethodMapping12345678910111213141516171819Overridepublic void afterPropertiesSet()    initHandlerMethods();/** * Scan beans in the ApplicationContext, detect and register handler methods. * see #getCandidateBeanNames() * see #processCandidateBean * see #handlerMethodsInitialized */protected void initHandlerMethods()    for (String beanName : getCandidateBeanNames())       if (!beanName.startsWith(SCOPED_TARGET_NAME_PREFIX))          processCandidateBean(beanName);            handlerMethodsInitialized(getHandlerMethods());进 processCandidateBean 方法：123456789101112131415protected void processCandidateBean(String beanName)    Classlt;?gt; beanType = null;   try       beanType = obtainApplicationContext().getType(beanName);      catch (Throwable ex)       // An unresolvable bean type, probably from a lazy bean - let&#39;s ignore it.      if (logger.isTraceEnabled())          logger.trace(Could not resolve type for bean &#39; + beanName + &#39;, ex);            if (beanType != null amp;amp; isHandler(beanType))       detectHandlerMethods(beanName);   当 bean isHandler 时，检测该bean内的方法。isHandler 就是看有没有打了 Controller 或 RequestMapping  注解：12345Overrideprotected boolean isHandler(Classlt;?gt; beanType)    return (AnnotatedElementUtils.hasAnnotation(beanType, Controller.class) ||         AnnotatedElementUtils.hasAnnotation(beanType, RequestMapping.class));接着看 detectHandlerMethods：1234567891011121314151617181920212223242526protected void detectHandlerMethods(Object handler)    Classlt;?gt; handlerType = (handler instanceof String ?         obtainApplicationContext().getType((String) handler) : handler.getClass());   if (handlerType != null)       Classlt;?gt; userType = ClassUtils.getUserClass(handlerType);      Maplt;Method, Tgt; methods = MethodIntrospector.selectMethods(userType,            (MethodIntrospector.MetadataLookuplt;Tgt;) method -gt;                try                   return getMappingForMethod(method, userType);                              catch (Throwable ex)                   throw new IllegalStateException(Invalid mapping on handler class [ +                        userType.getName() + ]:  + method, ex);                           );             //省略                    methods.forEach((method, mapping) -gt;          Method invocableMethod = AopUtils.selectInvocableMethod(method, userType);         registerHandlerMethod(handler, invocableMethod, mapping);      );   MethodIntrospector.selectMethods 会根据第二个参数返回的metadata(这里是RequestMappingInfo)，当metadata不为null时，将该method加入到maplt;Method,RequestMappingInfogt;中返回。getMappingForMethod 方法的主要逻辑就是拿到method上RequestMapping注解的信息：123456private RequestMappingInfo createRequestMappingInfo(AnnotatedElement element)    RequestMapping requestMapping = AnnotatedElementUtils.findMergedAnnotation(element, RequestMapping.class);   RequestConditionlt;?gt; condition = (element instanceof Class ?         getCustomTypeCondition((Classlt;?gt;) element) : getCustomMethodCondition((Method) element));   return (requestMapping != null ? createRequestMappingInfo(requestMapping, condition) : null);所以最终只会返回打了RequestMapping注解的方法，因为类如GetMapping类的注解其实也是加了RequestMapping的复合注解。接下来就进入到了 registerHandlerMethod，注册mapping关系：123protected void registerHandlerMethod(Object handler, Method method, T mapping)    this.mappingRegistry.register(mapping, handler, method);123456public void register(T mapping, Object handler, Method method)    // 省略      this.registry.put(mapping,            new MappingRegistrationlt;gt;(mapping, handlerMethod, directPaths, name, corsConfig != null));   // 省略registry 就是一个map：1private final Maplt;T, MappingRegistrationlt;Tgt;gt; registry = new HashMaplt;gt;();至此，在 RequestMappingHandlerMapping 中维护了一个RequestMappingInfo和Method关系的map。请求匹配springmvc本质就是一个servlet，直接看DIspatcherServlet的doDispatch方法：12345678protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception    // 省略         // Determine handler for the current request.         mappedHandler = getHandler(processedRequest);            // 省略123456789101112protected HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception    if (this.handlerMappings != null)        // 熟悉的 HandlerMapping 接口      for (HandlerMapping mapping : this.handlerMappings)          HandlerExecutionChain handler = mapping.getHandler(request);         if (handler != null)             return handler;                     return null;mapping.getHandler(request) 调用链为：mapping.getHandler(request) -gt; AbstractHandlerMaping#getHandler -gt; AbstractHandlerMethodMapping#getHandlerInternal -gt; AbstractHandlerMethodMapping#lookupHandlerMethod。lookupHandlerMethod :12345protected HandlerMethod lookupHandlerMethod(String lookupPath, HttpServletRequest request) throws Exception    // 省略    Listlt;Tgt; directPathMatches = this.mappingRegistry.getMappingsByDirectPath(lookupPath);   // 省略又看到熟悉的 mappingRegistry 了。"
} ,

{
"title"    : "Mybatis-plus 源码解析二：一级缓存和二级缓存",
"category" : "",
"tags"     : "mybatis",
"url"      : "/blog/2022/03/07/Mybatis-plus-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%BA%8C-%E4%B8%80%E7%BA%A7%E7%BC%93%E5%AD%98%E5%92%8C%E4%BA%8C%E7%BA%A7%E7%BC%93%E5%AD%98/",
"date"     : "2022-03-07 00:00:00 +0800",
"content"  : "mybatis-plus本身不提供缓存功能，一级缓存和二级缓存是mybatis中的实现一级缓存又称本地缓存。在同一个 SqlSession 内可以对相同sql的执行结果进行缓存。1234567Test    public void testFirstLevelCacheOnlyQueryOnce()         SqlSession session = sqlSessionFactory.openSession(true);        UserMapper userMapper = session.getMapper(UserMapper.class);        System.out.println(userMapper.selectById(1));        System.out.println(userMapper.selectById(1));    从日志中可看到，两次相同的查询只会查询一次数据库。12345678Test    public void testFirstLevelCacheQueryAfterUpdate()         SqlSession session = sqlSessionFactory.openSession(true);        UserMapper userMapper = session.getMapper(UserMapper.class);        System.out.println(userMapper.selectById(1));        userMapper.updateById(new User().setId(10086L).setName(foo).setAge(100).setEmail(foofoo.com));        System.out.println(userMapper.selectById(1));    更新后，会重新查询一次数据库。1234567891011121314151617181920Test    public void testFirstLevelCacheInTwoSession()         SqlSession session1 = sqlSessionFactory.openSession(true);        SqlSession session2 = sqlSessionFactory.openSession(true);        UserMapper userMapper1 = session1.getMapper(UserMapper.class);        UserMapper userMapper2 = session2.getMapper(UserMapper.class);        System.out.println(userMapper1.selectById(1):  + userMapper1.selectById(1));        System.out.println(userMapper1.selectById(1):  + userMapper1.selectById(1));        // 会重新查询一次数据库        System.out.println(userMapper2.selectById(1):  + userMapper2.selectById(1));        // session2 更新了数据        userMapper2.updateById(new User().setId(1L).setName(foo).setAge(100).setEmail(foofoo.com));        // session1查到了脏数据        System.out.println(userMapper1.selectById(1):  + userMapper1.selectById(1));        // session2会重新查询一次，读到的是最新的数据        System.out.println(userMapper2.selectById(1):  + userMapper2.selectById(1));    可看出，一级缓存的范围是同一个SqlSession内。源码分析先认识几个顶级接口：SqlSession：mybatis的主要接口。通过这个接口来执行增删改查、获取mapper、管理事务等。Executor：顾名思义，执行器。具体干活的接口，包括update、query、commit、rollback等。每个 SqlSession 有一个 Executor，Executor 有个抽象实现类 BaseExecutor，BaseExecutor 中有个cache：1protected PerpetualCache localCache;PerpetualCache 内部就是个 HashMap123456789public class PerpetualCache implements Cache   private final String id;  private final Maplt;Object, Objectgt; cache = new HashMaplt;gt;();  public PerpetualCache(String id)     this.id = id;  所以一级缓存的范围就是SqlSession，同一个session内共享同一个cache，不同session之间缓存互不影响。缓存key查询会走到 BaseExecutor 的这个方法：123456Override  public lt;Egt; Listlt;Egt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException     BoundSql boundSql = ms.getBoundSql(parameter);    CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);    return query(ms, parameter, rowBounds, resultHandler, key, boundSql);  createCacheKey 会生成一个cache key，实现如下：12345678910111213141516171819202122232425262728293031323334Override  public CacheKey createCacheKey(MappedStatement ms, Object parameterObject, RowBounds rowBounds, BoundSql boundSql)     ...省略...    CacheKey cacheKey = new CacheKey();    cacheKey.update(ms.getId());    cacheKey.update(rowBounds.getOffset());    cacheKey.update(rowBounds.getLimit());    cacheKey.update(boundSql.getSql());    Listlt;ParameterMappinggt; parameterMappings = boundSql.getParameterMappings();    TypeHandlerRegistry typeHandlerRegistry = ms.getConfiguration().getTypeHandlerRegistry();    // mimic DefaultParameterHandler logic    for (ParameterMapping parameterMapping : parameterMappings)       if (parameterMapping.getMode() != ParameterMode.OUT)         Object value;        String propertyName = parameterMapping.getProperty();        if (boundSql.hasAdditionalParameter(propertyName))           value = boundSql.getAdditionalParameter(propertyName);         else if (parameterObject == null)           value = null;         else if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass()))           value = parameterObject;         else           MetaObject metaObject = configuration.newMetaObject(parameterObject);          value = metaObject.getValue(propertyName);                cacheKey.update(value);              if (configuration.getEnvironment() != null)       // issue #176      cacheKey.update(configuration.getEnvironment().getId());        return cacheKey;  CacheKey 的 update 方法就是往一个List里add1234567891011public void update(Object object)     int baseHashCode = object == null ? 1 : ArrayUtil.hashCode(object);    count++;    checksum += baseHashCode;    baseHashCode *= count;    hashcode = multiplier * hashcode + baseHashCode;    updateList.add(object);  所以一个CacheKey是由以下部分组成的：MappedStatement的id(即mapper方法全路径，如a.b.c.UserMapper.selectById)、offset和limit、sql、参数。当两个mapper方法以上部分都匹配时，会直接从缓存中取出对应的值。刷新缓存除查询以外的方法，包括增删改，都会进入 BaseExecutor 的 update 方法：123456789Override  public int update(MappedStatement ms, Object parameter) throws SQLException     ErrorContext.instance().resource(ms.getResource()).activity(executing an update).object(ms.getId());    if (closed)       throw new ExecutorException(Executor was closed.);        clearLocalCache();    return doUpdate(ms, parameter);  调用了 clearLocalCache：1234567Override  public void clearLocalCache()     if (!closed)       localCache.clear();      localOutputParameterCache.clear();      所以，增删改会简单粗暴的清空所有缓存，之后的查询会再次查询数据库。缓存范围一级缓存可选范围有两个：SESSION、STATEMENT。配置项为：123mybatis-plus:  configuration:    local-cache-scope: session(或statement)默认为session，之前的分析都是在该配置下。在 BaseExecutor 的 query 方法中有个判断：123456789101112131415161718192021222324252627282930Overridepublic lt;Egt; Listlt;Egt; query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql) throws SQLException     ...省略...    try         queryStack++;        // 先从缓存中获取数据        list = resultHandler == null ? (Listlt;Egt;) localCache.getObject(key) : null;        if (list != null)             handleLocallyCachedOutputParameters(ms, key, parameter, boundSql);         else             // 未从缓存中获取到数据时直接从数据库中查询            list = queryFromDatabase(ms, parameter, rowBounds, resultHandler, key, boundSql);             finally         queryStack--;        if (queryStack == 0)         for (BaseExecutor.DeferredLoad deferredLoad : deferredLoads)             deferredLoad.load();                // issue #601        deferredLoads.clear();        // 如果参数localCacheScope值为STATEMENT，则每次查询之后都清空缓存        if (configuration.getLocalCacheScope() == LocalCacheScope.STATEMENT)             // issue #482            clearLocalCache();                return list;当范围为statement时，每次查询时会先查询缓存或直接从数据库查询，之后都会清空缓存。所以个人理解，statement范围的一级缓存其实相当于是关闭了一级缓存，因为每次查询后都会清空缓存，增删改操作也会清空缓存(如上文所述)，那就用不上缓存了嘛。二级缓存二级缓存就是为了解决一级缓存不能跨session的问题。既然一级缓存是存在于每个SqlSession内，那就弄一个全局的cache不就行了。使用方法：      开启二级缓存：    123mybatis-plus:  configuration:    cache-enabled: true        默认即为true        在要开启二级缓存的mapper上加上注解：CacheNamespace 即可  cache-enabled 设为true后，在生成Executor实例时会用 CachingExecutor 包装一下(装饰器模式)，以提供全局缓存功能。二级缓存是以 namespace 作为一个分组，同一个namespace内的缓存都可以共享，不再局限于单个SqlSession内。CacheNamespace 即表示为该mapper开启一个namespace，该mapper内的方法共享一个缓存，当然，底层还是CacheKey，即key的生成和匹配还是按照上一节讲的逻辑。commit后二级缓存才会生效1234567891011Test    public void testSecondaryCache()         SqlSession session1 = sqlSessionFactory.openSession(true);        SqlSession session2 = sqlSessionFactory.openSession(true);        UserWithCacheMapper userMapper1 = session1.getMapper(UserWithCacheMapper.class);        UserWithCacheMapper userMapper2 = session2.getMapper(UserWithCacheMapper.class);        System.out.println(userMapper1.selectById(1):  + userMapper1.selectById(1));        System.out.println(userMapper1.selectById(1):  + userMapper1.selectById(1));        System.out.println(userMapper2.selectById(1):  + userMapper2.selectById(1));    从日志观察发现，上面3次 selectById 查询了2次数据库(第一个和第三个查询了数据库，第二个使用了一级缓存)，二级缓存并没有生效。在第二次查询后加入 session1.commit()，3次查询只查询了一次数据库，二级缓存生效。源码分析直接从CachingExecutor看起，查询会走以下方法：12345678910111213141516171819Override  public lt;Egt; Listlt;Egt; query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)      throws SQLException     Cache cache = ms.getCache();    if (cache != null)       flushCacheIfRequired(ms);      if (ms.isUseCache() amp;amp; resultHandler == null)         ensureNoOutParams(ms, boundSql);        SuppressWarnings(unchecked)        Listlt;Egt; list = (Listlt;Egt;) tcm.getObject(cache, key);        if (list == null)           list = delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);          tcm.putObject(cache, key, list); // issue #578 and #116                return list;              return delegate.query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);  当开启了二级缓存并使用CaheNamespace标注mapper后，mapper中的每个 MappedStatement 会有一个cache属性，会进入到 if(cache!=null)  的代码逻辑。首先尝试从缓存中查询：tcm.getObject(cache, key)，tcm 是 TransactionCacheManager，即带有事务性质的缓存管理器，管理的是 TransactionCache，TransactionCache 最底层仍是上节提到的PerpetualCache，采用HashMap实现。这些都可以在 CaheNamespace 中自定义。缓存中没查询到的话，会走数据库查询，然后将结果放到缓存中 tcm.putObject(cache, key, list)，最终实现如下：1234Override  public void putObject(Object key, Object object)     entriesToAddOnCommit.put(key, object);  1private final Maplt;Object, Objectgt; entriesToAddOnCommit;从名字可以看出来：当commit时要提交的项。可以看到，tcm.putObject 只是将结果放到了 TransactionCache 中的 entriesToAddOnCommit 里面，而查询逻辑 tcm.getObject(cache, key) 是1234567891011121314Overridepublic Object getObject(Object key)   // issue #116  Object object = delegate.getObject(key);  if (object == null)     entriesMissedInCache.add(key);    // issue #146  if (clearOnCommit)     return null;   else     return object;  可看到 getObject 是委托到给了 delegate 去做，不用管 delegate 具体是什么，从最底层的PerpetualCache到 TransactionCache 经历了层层包装(装饰器模式)。上文讲到 putObject 只是把结果放到了 entriesToAddOnCommit 里，而 getObject 根本就没有查询 entriesToAddOnCommit，那这个缓存到底是怎么用的呢？从  sqlSession.commit 看起，最终会进入到 TransactionCache 的 commit 方法：1234567public void commit()   if (clearOnCommit)     delegate.clear();    flushPendingEntries();  reset();接着看 flushPendingEntries：12345678910private void flushPendingEntries()   for (Map.Entrylt;Object, Objectgt; entry : entriesToAddOnCommit.entrySet())     delegate.putObject(entry.getKey(), entry.getValue());    for (Object entry : entriesMissedInCache)     if (!entriesToAddOnCommit.containsKey(entry))       delegate.putObject(entry, null);      从名字也可以看出来，flush pending entries，这里的pendingEntries 就是 entriesToAddOnCommit。delegate.putObject(entry.getKey(), entry.getValue()) ：将entriesToAddOnCommit 添加到 delegate 中去。所以，只有当 commit 时，才会将查询结果缓存起来，后续才能利用到缓存。增删改操作同样会刷新缓存，也就是 delegate.clear()，将delegate中的缓存清空。"
} ,

{
"title"    : "Springboot中自定义Jackson命名转换策略",
"category" : "",
"tags"     : "jackson",
"url"      : "/blog/2022/02/04/Springboot%E4%B8%AD%E8%87%AA%E5%AE%9A%E4%B9%89Jackson%E5%91%BD%E5%90%8D%E8%BD%AC%E6%8D%A2%E7%AD%96%E7%95%A5/",
"date"     : "2022-02-04 00:00:00 +0800",
"content"  : "这篇文章来源于这么一个需求：前端传递过来的json格式不统一，有下划线格式的、驼峰格式的，需要都能正确反序列化。而序列化则统一采用驼峰格式。如果反序列化和序列化都采用同一种格式，则直接可以使用内置的 com.fasterxml.jackson.databind.PropertyNamingStrategy，它有如下几种预定义的属性命名策略：  SNAKE_CASE  UPPER_CAMEL_CASE  LOWER_CAMEL_CASE  LOWER_CASE  KEBAB_CASE  LOWER_DOT_CASE在 application.yml 中配置示例如下：1spring.jackson.property-naming-strategy: SNAKE_CASE但由于需求中要求反序列化和序列化的格式不一致，这时就需要定制一下了。自定义一个类继承 PropertyNamingStrategy，重写 nameForSetterMethod 方法：1234567public class CustomPropertyNamingStrategy extends PropertyNamingStrategy     Override    public String nameForSetterMethod(MapperConfiglt;?gt; config, AnnotatedMethod method, String defaultName)         // 驼峰转小写下划线        return CaseFormat.LOWER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, defaultName);    Jackson 反序列化逻辑是这样的：首先为目标类型（即 RequestBody 后的POJO）生成一个 BeanDeserializer，反射获取其字段和对应的getter/setter方法封装成一个 SettableBeanProperty（这种情况下是它的子类 MethodProperty，当然还有其他类型的子类）。然后构造一个 map，以此为 value，以 nameForSetterMethod 返回的值为 key（其中 defaultName 为 POJO 中字段名），这样就把json中的字段名和POJO中的setter方法对应起来了，最终完成反序列化。默认情况下没有配置自定义的命名策略时，map 中的 key 就是 defaultName，没作任何处理。而我们的需求json是下划线的，POJO是驼峰的。因此在 nameForSetterMethod  方法中将驼峰格式的defaultName转为了小写下划线格式的作为key。比如POJO中有一个属性为 userId，json对象中为 user_id，使用此自定义命名策略后，内部的map中就会存在这么一个映射：user_id -gt;  MethodProperty(field:userId,getter:getUserId,setter:setUserId)。在反序列化 user_id 字段时，就会反射调用POJO的 setUserId 方法将值设置进去。除 nameForSetterMethod 外，还有一个 nameForGetterMethod 方法。同样的逻辑，nameForGetterMethod 是用于构造序列化时map的key的。key 为该方法返回的值，value为对应POJO的getter方法。不同的是，此时的key被用来作为序列化后json对象的字段名。默认也是不作任何处理，直接返回defaultName，即驼峰格式。我们的需求也是序列化返回驼峰格式，因此该方法不用重写。上述情况都是有 getter/setter 方法的情况，在没有getter/setter时，命名策略用到了另一个方法：nameForField。此时Jackson序列化/反序列化时由于没有getter/setter 方法，会直接反射得到field来获取值或设置值，当然前提是字段是public的。nameForField 的返回值便作为了反序列化时json对象的字段名，和序列化时json对象的字段名。同样，在内部会维护一个map，key 是 nameForField 的返回值，value是 FieldProperty（SettableBeanProperty  的另一个子类）。可以查看 SNAKE_CASE 对应的实现 SnakeCaseStrategy，它对上述三个方法都做了重写，全部将 defaultName 转为了小写下划线的格式。所以它的效果是反序列化和序列化都使用下换线的json格式。最后，在要应用此命名转换策略的POJO类上加上注解 JsonNaming(CustomPropertyNamingStrategy.class) 即可，它会覆盖全局的命名策略，优先级更高。  注意：如果有内部类，应该在每个内部类上都加上注解。"
} ,

{
"title"    : "Springboot中校验Enum",
"category" : "",
"tags"     : "springboot",
"url"      : "/blog/2022/01/18/Springboot%E4%B8%AD%E6%A0%A1%E9%AA%8CEnum/",
"date"     : "2022-01-18 00:00:00 +0800",
"content"  : "方式一：全局异常处理 + JsonCreator直接监听 HttpMessageNotReadableException 异常，在全局异常处理器中判断、整理异常信息，简单粗暴，如下所示：12345678910111213141516171819ExceptionHandler(HttpMessageNotReadableException.class)    public Resultlt;Stringgt; handleHttpMessageNotReadableException(HttpMessageNotReadableException e)         String errorMsg;        if (e.getCause() instanceof ValueInstantiationException)             ValueInstantiationException ex = ((ValueInstantiationException) e.getCause());            Reference path = ex.getPath().get(0);            Classlt;?gt; rawClass = ex.getType().getRawClass();            String rightValue = ;            if (rawClass.isEnum())                 rightValue = 值应为： + Arrays.toString(rawClass.getEnumConstants());                        String errorValue = ex.getOriginalMessage().split(problem: )[1];            errorMsg = String.format(s参数(s) 错误！ s, path.getFieldName(), errorValue.substring(errorValue.lastIndexOf(.) + 1), rightValue);         else             errorMsg = e.getCause().getLocalizedMessage();                return new Resultlt;gt;(1, errorMsg, );  返回结果类似如下：12345    code: 1,    message: searchScope参数(PRIVAT) 错误！ 值应为：[ALL, PLATFORM, PRIVATE],    content: 结合 JsonCreator 可忽略大小写，空值转默认值等操作，如下所示：12345678910111213141516public enum SearchScopeQry     ALL(全部), PLATFORM(平台), PRIVATE(私有);    private String scope;    SearchScopeQry(String scope)         this.scope = scope;        JsonCreator    public static SearchScopeQry getSearchScope(String scope)         if (StringUtils.isEmpty(scope))             return ALL;                return SearchScopeQry.valueOf(scope.toUpperCase());    方式二：自定义校验器主要思路是自定一个枚举校验注解，结合 Constraint 使用自定义的校验器进行校验，校验不通过会抛出 MethodArgumentNotValidException 异常，网上有很多资料，这里不再详细说明。"
} ,

{
"title"    : "Mybatis-plus 源码解析一：@MapperScan、@Mapper、基础增删改查",
"category" : "",
"tags"     : "mybatis",
"url"      : "/blog/2021/12/30/Mybatis-plus-%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E4%B8%80-@Mapper%E6%8E%A5%E5%8F%A3%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/",
"date"     : "2021-12-30 00:00:00 +0800",
"content"  : "MapperScan使用场景：打在配置类或启动类上，从指定路径或当前包所在路径开始，扫描mapper接口，动态生成bean注册到容器中。MapperScannerRegistrarMapperScan 包含了一个元注解 Import(MapperScannerRegistrar.class)。Import 可以和 ImportBeanDefinitionRegistrar 实现类配合使用，ImportBeanDefinitionRegistrar 可以动态注册BeanDefinition：1void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)importingClassMetadata 为导入类的注解元信息，比如在Application类上打了 MapperScan 注解，因为 MapperScan 中包含了 Import，所以 importingClassMetadata 就代表 Application 类的注解元信息。MapperScannerRegistrar 中实现如下：1234567891011Override  public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)       // 从导入类注解元信息中获取MapperScan中的属性值    AnnotationAttributes mapperScanAttrs = AnnotationAttributes        .fromMap(importingClassMetadata.getAnnotationAttributes(MapperScan.class.getName()));    if (mapperScanAttrs != null)         // 接着往下看      registerBeanDefinitions(importingClassMetadata, mapperScanAttrs, registry,          generateBaseBeanName(importingClassMetadata, 0));      1234567891011121314151617181920212223242526272829303132void registerBeanDefinitions(AnnotationMetadata annoMeta, AnnotationAttributes annoAttrs,      BeanDefinitionRegistry registry, String beanName)     // 构造一个 MapperScannerConfigurer 的 BeanDefinitionBuilder    BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class);        ...省略...        // 从 MapperScan 中的 value、basePackages、basePackageClasses 读取要扫描的路径    Listlt;Stringgt; basePackages = new ArrayListlt;gt;();    basePackages.addAll( Arrays.stream(annoAttrs.getStringArray(value)).filter(StringUtils::hasText).collect(Collectors.toList());    basePackages.addAll(Arrays.stream(annoAttrs.getStringArray(basePackages)).filter(StringUtils::hasText)        .collect(Collectors.toList()));                                                                              basePackages.addAll(Arrays.stream(annoAttrs.getClassArray(basePackageClasses)).map(ClassUtils::getPackageName)        .collect(Collectors.toList()));    // 如果都没指定，取当前包名    if (basePackages.isEmpty())       basePackages.add(getDefaultBasePackage(annoMeta));        ...省略...    // 将扫描路径设置到 MapperScannerConfigurer 的 basePackage属性                      builder.addPropertyValue(basePackage, StringUtils.collectionToCommaDelimitedString(basePackages));    // 注册BeanDefinition    registry.registerBeanDefinition(beanName, builder.getBeanDefinition());  所以 MapperScannerRegistrar 的作用就是注册 MapperScannerConfigurer 的 BeanDefinition，将 MapperScan 的属性值设置进去，然后由 MapperScannerConfigurer来完成 mapper 接口的扫描注册工作。MapperScannerConfigurerMapperScannerConfigurer 实现了 BeanDefinitionRegistryPostProcessor 接口，postProcessBeanDefinitionRegistry 方法实现如下：1234567891011Override  public void postProcessBeanDefinitionRegistry(BeanDefinitionRegistry registry)     ...省略...    // class路径扫描器        ClassPathMapperScanner scanner = new ClassPathMapperScanner(registry);    ...省略...    scanner.registerFilters();    scanner.scan(        StringUtils.tokenizeToStringArray(this.basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS));  registerFilters  注册扫描器的过滤器，可根据注解、接口等进行过滤。ClassPathMapperScanner  是 mybatis-spring 的类，继承自 spring-context 中的 ClassPathBeanDefinitionScanner。重写了 doScan 方法和  isCandidateComponent 方法。isCandidateComponent  指定了只要接口且不是内部类。doScan 方法调用了父类的 doScan 后(这一步已经将扫描到的BeanDefinition注册到容器中了 )，对扫描到的 beanDefinition 做了进一步处理：123456789101112131415Override  public Setlt;BeanDefinitionHoldergt; doScan(String... basePackages)     // 这一步已经将扫描到的BeanDefinition注册到容器中了      Setlt;BeanDefinitionHoldergt; beanDefinitions = super.doScan(basePackages);    if (beanDefinitions.isEmpty())       LOGGER.warn(() -gt; No MyBatis mapper was found in &#39; + Arrays.toString(basePackages)          + &#39; package. Please check your configuration.);     else         // 往下看      processBeanDefinitions(beanDefinitions);        return beanDefinitions;  1234567891011121314151617181920212223private void processBeanDefinitions(Setlt;BeanDefinitionHoldergt; beanDefinitions)     AbstractBeanDefinition definition;    BeanDefinitionRegistry registry = getRegistry();    // 遍历扫描到的BeanDefinition    for (BeanDefinitionHolder holder : beanDefinitions)       definition = (AbstractBeanDefinition) holder.getBeanDefinition();      ...省略...      // 得到beanClassName,比如 a.b.c.UserMapper      String beanClassName = definition.getBeanClassName();      LOGGER.debug(() -gt; Creating MapperFactoryBean with name &#39; + holder.getBeanName() + &#39; and &#39; + beanClassName          + &#39; mapperInterface);        // the mapper interface is the original class of the bean      // but, the actual class of the bean is MapperFactoryBean      // 设置构造函数值为beanClassName，MapperFactoryBean的构造函数为：public MapperFactoryBean(Classlt;Tgt; mapperInterface) ，需要传递mapper的class      definition.getConstructorArgumentValues().addGenericArgumentValue(beanClassName); // issue #59      // 将mapper的 BeanDefinition 的class属性设为 MapperFactoryBean      definition.setBeanClass(this.mapperFactoryBeanClass);      ...省略...      所以这时每个 mapper 对应的 BeanDefinition 实际上都是 MapperFactoryBean 类型了，是一个工厂bean，并持有一个 mapperInterface  属性，对应mapper的class。  ps：FactoryBean  是懒加载，只有在第一次使用到对应类型的对象时，才会调用 getObject  获取对象。如果要急加载，可实现 SmartFactoryBean 接口，有一个 isEagerInit  方法。Mapper使用场景：单独定义在mapper接口上MybatisPlusAutoConfiguration#AutoConfiguredMapperScannerRegistrar在 MybatisPlusAutoConfiguration 中有一个内部配置类 MapperScannerRegistrarNotFoundConfiguration：1234    Configuration(proxyBeanMethods = false)    Import(AutoConfiguredMapperScannerRegistrar.class)    ConditionalOnMissingBean(MapperFactoryBean.class, MapperScannerConfigurer.class)    public static class MapperScannerRegistrarNotFoundConfiguration 当没有 MapperFactoryBean 和 MapperScannerConfigurer 类型的bean时，该配置类会生效。而这两种类型的bean只有当使用了 MapperScan 注解时才会被动态注入，详见上一节。所以，换言之，没使用 MapperScan 时会使用这个配置。该配置类import了另一个配置类 AutoConfiguredMapperScannerRegistrar：1public static class AutoConfiguredMapperScannerRegistrar implements BeanFactoryAware, ImportBeanDefinitionRegistrar 可发现也是一个 ImportBeanDefinitionRegistrar 实现类，基本套路也和 MapperScannerRegistrar 一致，看它的 registerBeanDefinitions 方法：123456789101112131415161718192021222324        Override        public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry)             ...省略...            logger.debug(Searching for mappers annotated with Mapper);            // 获取自动配置类所在包路径，即打了 SpringbootApplication 的启动类，因为 Maper 不需要指定扫描路径，那么就从            // 根路径开始扫描            Listlt;Stringgt; packages = AutoConfigurationPackages.get(this.beanFactory);            if (logger.isDebugEnabled())                 packages.forEach(pkg -gt; logger.debug(Using auto-configuration base package &#39;&#39;, pkg));                        // 同样，构建一个 MapperScannerConfigurer 的 BeanDefinition            BeanDefinitionBuilder builder = BeanDefinitionBuilder.genericBeanDefinition(MapperScannerConfigurer.class);            ...省略...            // 给MapperScannerConfigurer设置annotationClass属性，相当于 MapperScan 中的annotationClass属性                 builder.addPropertyValue(annotationClass, Mapper.class);            // 设置basePackage            builder.addPropertyValue(basePackage, StringUtils.collectionToCommaDelimitedString(packages));            ...省略...            // 注册 MapperScannerConfigurer 的 BeanDefinition               registry.registerBeanDefinition(MapperScannerConfigurer.class.getName(), builder.getBeanDefinition());        后面的处理逻辑就是 MapperScannerConfigurer 的处理逻辑了，和 MapperScan 一样。mybatis-plus 增删改查上文说到，mapper 都是 MapperFactoryBean，所以入口就在  getObject：1234Override  public T getObject() throws Exception     return getSqlSession().getMapper(this.mapperInterface);  对应 SqlSession 接口的 lt;Tgt; T getMapper(Classlt;Tgt; type) 方法。SqlSession 有两个主要实现类：DefaultSqlSession(mybatis) 和 SqlSessionTemplate(mybatis-spring)。此时的实现类是 SqlSessionTemplate。MapperFactoryBean 除了 mapperInterface 外，还有两个属性：sqlSessionFactory 和 sqlSessionTemplate。这两个属性是在何时设置进去的？回到 MybatisPlusAutoConfiguration，该配置类还生成了两个bean：1234567    Bean    ConditionalOnMissingBean    public SqlSessionFactory sqlSessionFactory(DataSource dataSource)             Bean    ConditionalOnMissingBean    public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) 上文讲到 ClassPathMapperScanner 的  processBeanDefinitions 方法，用于给每个扫描到的 mapper BeanDefinition 设置属性，这一步会将 mapper 的 class 都设为 MapperFactoryBean，除此之外，还设置了 autowireMode：1definition.setAutowireMode(AbstractBeanDefinition.AUTOWIRE_BY_TYPE);表示对 MapperFactoryBean 启用属性按类型自动注入，在spring容器初始化过程中，会初始化每个 MapperFactoryBean，会把 MybatisPlusAutoConfiguration 中配置的bean设置进去。回到 getMapper 方法，经过层层嵌套，最终到了 com.baomidou.mybatisplus.core.MybatisMapperRegistry#getMapper：1234567891011121314    Override    public lt;Tgt; T getMapper(Classlt;Tgt; type, SqlSession sqlSession)         // TODO 这里换成 MybatisMapperProxyFactory 而不是 MapperProxyFactory        final MybatisMapperProxyFactorylt;Tgt; mapperProxyFactory = (MybatisMapperProxyFactorylt;Tgt;) knownMappers.get(type);        if (mapperProxyFactory == null)             throw new BindingException(Type  + type +  is not known to the MybatisPlusMapperRegistry.);                try             // 往下看            return mapperProxyFactory.newInstance(sqlSession);         catch (Exception e)             throw new BindingException(Error getting mapper instance. Cause:  + e, e);              mybatis-plus 继承了 mybatis 很多类，由于这些类mybatis在设计时并没有打算可扩展，所以mybatis-plus的做法是直接全部copy过来，然后进行扩展。接着点进去：12345678    protected T newInstance(MybatisMapperProxylt;Tgt; mapperProxy)         return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[]mapperInterface, mapperProxy);        public T newInstance(SqlSession sqlSession)         final MybatisMapperProxylt;Tgt; mapperProxy = new MybatisMapperProxylt;gt;(sqlSession, mapperInterface, methodCache);        return newInstance(mapperProxy);    可看到最终是使用了Jdk的动态代理，mapper最终对应的是 mapperInterface 的一个动态代理类。注入基本增删改查那么为何只需要继承 BaseMapper 接口就能实现基本的增删改查呢？mapper基本调用逻辑上文已讲过，撇去层层嵌套后，最终是归功于 com.baomidou.mybatisplus.core.MybatisConfiguration#getMappedStatement(java.lang.String)：123public MappedStatement getMappedStatement(String id)     return this.getMappedStatement(id, true);  id 为 mapper 的方法全路径，比如：a.b.c.UserMapper.insert，MappedStatement 里包含了对应要执行的sql(SqlSource类-gt;DynamicSqlSource)。MybatisConfiguration 中维护了一个map，存储的就是这样的键值对类型，每个mapper的每个方法都对应了一个MappedStatement。所以执行的逻辑基本上就是，根据方法找到对应的MappedStatement，结合传递进来的参数，生成最终的sql，生成 PreparedStatement 对象，后面就是jdbc的流程了。那么这个map是如何生成的？MapperFactoryBean 继承了 DaoSupport 接口，DaoSupport 是 Spring-tx 中 DAO 的基类，实现类有 JdbcDaoSupport 等。DaoSupport 实现了 InitializingBean 接口，afterPropertiesSet 实现如下：12345678910111213    Override    public final void afterPropertiesSet() throws IllegalArgumentException, BeanInitializationException         // Let abstract subclasses check their configuration.        checkDaoConfig();        // Let concrete implementations initialize themselves.        try             initDao();                catch (Exception ex)             throw new BeanInitializationException(Initialization of DAO failed, ex);            MapperFactoryBean 重写了 checkDaoConfig  方法12345678910111213141516171819  Override  protected void checkDaoConfig()     super.checkDaoConfig();    notNull(this.mapperInterface, Property &#39;mapperInterface&#39; is required);    Configuration configuration = getSqlSession().getConfiguration();    if (this.addToConfig amp;amp; !configuration.hasMapper(this.mapperInterface))       try         // 在这一步添加mapper，最终会生成该mapperInterface中所有方法的键值对，放到map中去        configuration.addMapper(this.mapperInterface);       catch (Exception e)         logger.error(Error while adding the mapper &#39; + this.mapperInterface + &#39; to configuration., e);        throw new IllegalArgumentException(e);       finally         ErrorContext.instance().reset();            configuration.addMapper(this.mapperInterface) 详细流程很复杂，简单来说就是会利用反射获取到 BaseMapperlt;Tgt; 中的泛型参数，以此为表名，获取到表相关信息，封装为 TableInfo 对象。Mybatis-plus为基本增删改查操作定义了一系列的类，如Insert、Delete、Update 等等，以 Insert类为例：12345public class Insert extends AbstractMethod     public Insert()         super(SqlMethod.INSERT_ONE.getMethod());    SqlMethod 是一个枚举：12345public enum SqlMethod     /**     * 插入     */    INSERT_ONE(insert, 插入一条数据（选择字段插入）, lt;scriptgt;#92;nINSERT INTO s s VALUES s#92;nlt;/scriptgt;),相当于定义了一个sql模板字符串，第一个占位符是表名，第二个占位符是列名字符串，第三个占位符是值字符串。未完待续！！！"
} ,

{
"title"    : "ThreadLocal",
"category" : "",
"tags"     : "多线程",
"url"      : "/blog/2021/12/21/ThreadLocal/",
"date"     : "2021-12-21 00:00:00 +0800",
"content"  : "作用同一个 ThreadLocal 对象可以管理多个线程中的对象，且线程之间数据是隔离的，互不干扰。有两个主要应用场景：  每个线程需要自己拥有一份独立的对象，比如 SimpleDateFormat，它是线程不安全的  在同一线程中传递全局变量，比如会话信息、用户权限等等，就不需要为每个方法都加一个入参了示例演示一下全局变量的用法。1234567891011121314151617181920public class UserHolder    public static final ThreadLocallt;Usergt; USER_HOLDER = new ThreadLocallt;gt;();//以下方法在不同类中public void method1(Long id)    User user = findUserById(id);    UserHolder.USER_HOLDER.set(user);public void method2()    User currentUser = UserHolder.USER_HOLDER.get();    //业务逻辑public void method3()    User currentUser = UserHolder.USER_HOLDER.get();    //业务逻辑只要是在同一个线程中，method2、method3  取到的都是同一个用户。不同线程取到的是不同用户。原理本质：每个线程会维护自己的一个 map：ThreadLocalMap：1ThreadLocal.ThreadLocalMap threadLocals = null;看下 ThreadLocal set() 方法的源码：12345678public void set(T value)         Thread t = Thread.currentThread();        ThreadLocalMap map = getMap(t);        if (map != null)            map.set(this, value);        else            createMap(t, value);    123ThreadLocalMap getMap(Thread t)         return t.threadLocals;    set 时会先获取当前线程，然后取当前线程中的 ThreadLocalMap，把值 set 进去。每个线程使用 threadLocal.get() 时是在自己的map中获取，就是这样做到线程隔离的。可看到set时，key 是 this，即 threadLocal 对象，这么设计是因为 threadLocalMap 每个线程只有一个，而一个线程可以有多个 threadLocal 对象，如果需要多个全局变量的话。再继续看 threadLocalMap 中的 set 方法：1234567891011121314151617181920212223242526272829303132private void set(ThreadLocallt;?gt; key, Object value)             // We don&#39;t use a fast path as with get() because it is at            // least as common to use set() to create new entries as            // it is to replace existing ones, in which case, a fast            // path would fail more often than not.            Entry[] tab = table;            int len = tab.length;            int i = key.threadLocalHashCode amp; (len-1);            for (Entry e = tab[i];                 e != null;                 e = tab[i = nextIndex(i, len)])                 ThreadLocallt;?gt; k = e.get();                if (k == key)                     e.value = value;                    return;                                if (k == null)                     replaceStaleEntry(key, value, i);                    return;                                        tab[i] = new Entry(key, value);            int sz = ++size;            if (!cleanSomeSlots(i, sz) amp;amp; sz gt;= threshold)                rehash();        ThreadLocalMap 类似于 HashMap，不同在于 ThreadLocalMap 只有数组(初始容量为16)，没有链表，通过 key.threadLocalHashCode amp; (len-1) 计算出数组下标存储位置。那没有链表，如何解决冲突的呢？其实很简单，就往后顺延就可以了。如果下标位置中没值，直接存储。如果有值，判断 key 是否相等，相等的话刷新，不等的话顺延一个位置重复之前的步骤。ThreadLocalMap 有一个阈值：数组容量的2/3，当超过这个阈值的时候会进行扩容，扩容为原来容量的2倍。由于 ThreadLocalMap 解决冲突的办法是顺延法，相对于 HashMap 的链表法在大数据量时效率较低。所以它不适合用来存储大量的值，它的设计初衷也只是为了解决全局变量传递的问题，并不是用来做一个容器。内存泄露问题内存泄露指的是已分配出去的内存，回收不了，就不能再利用，这块内存就像凭空消失了一样。内存泄露积累下来，还会造成内存空间耗尽，导致 OOM。先看一下 ThreadLocalMap 的 Entry 的数据结构：12345678910111213141516171819static class ThreadLocalMap         /**         * The entries in this hash map extend WeakReference, using         * its main ref field as the key (which is always a         * ThreadLocal object).  Note that null keys (i.e. entry.get()         * == null) mean that the key is no longer referenced, so the         * entry can be expunged from table.  Such entries are referred to         * as stale entries in the code that follows.         */        static class Entry extends WeakReferencelt;ThreadLocallt;?gt;gt;             /** The value associated with this ThreadLocal. */            Object value;            Entry(ThreadLocallt;?gt; k, Object v)                 super(k);                value = v;                    可看到 Entry 是一个 WeakReference，在介绍 WeakReference  之前先说下垃圾回收。JVM 的垃圾回收算法有两种：引用计数法和可达性分析算法，从各自名字就能看出大概是什么意思。目前主流垃圾回收器采取的是可达性分析算法，该算法的本质是会从一系列 “GC Roots” 合集出发，探索所有能被该合集引用到的对象，并将其加入到该集合中，这个过程称为 “标记”，然后继续探索。最终，未被探索到的对象便是 “死亡” 对象，便会被垃圾回收器回收。所以一个对象会不会被 GC 就看它能不能被探索到，换句话说有没有引用指向它，是不是可达的。GC 的工作会由 JVM 自动来完成，但程序员也可以通过代码的方式（通过定义不同类型的对象引用 ）来影响一个对象的生命周期，这就是 Reference 的作用。Java 中的四种引用：  强引用  软引用  弱引用  虚引用threadlocalmap 的 key 是一个弱引用，发生gc就会被回收，这样就出现了 key 为 null 的 entry，而如果使用的是线程池，线程一直存在，就会存在一个强引用：thread -gt; threadlocalmap -gt; entry -gt; value，导致 value 不能被回收。使用完以后记得 remove，清理 key 为 null 的 entry"
} ,

{
"title"    : "Redis学习笔记之缓存雪崩、击穿、穿透",
"category" : "",
"tags"     : "redis",
"url"      : "/blog/2021/12/19/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E7%BC%93%E5%AD%98%E9%9B%AA%E5%B4%A9-%E5%87%BB%E7%A9%BF-%E7%A9%BF%E9%80%8F/",
"date"     : "2021-12-19 00:00:00 +0800",
"content"  : "缓存雪崩表现为由于大量的应用请求无法在缓存中处理，导致数据库压力骤增。造成缓存雪崩的原因主要有两个：      大量缓存同时过期                  尽量避免为大批量缓存数据设置相同的过期时间，如确有这个需求，可为每个过期时间增加一点随机值(比如随机增加1~3分钟)                    如果已经发生了，可考虑服务降级                  服务降级指负载过大时，为了保证重要、基本、核心的服务能正常运行，将一些不重要的、不紧急的服务延迟使用或暂停使用。          在缓存雪崩场景下，对于非核心数据可直接返回预定义值，不再查询缓存，等缓存恢复后再恢复相关服务。对于核心数据，仍可查询缓存或数据库，保留服务。          扩展一下，服务降级的方式常见有以下几种：                      延迟服务：可采用定时任务或mq延迟处理。比如可在凌晨流量较小的时候再处理，或提示用户多长时间内会处理完等等；            暂停服务：关闭一些非核心服务。比如在双十一时候，买东西都不允许修改收货地址，不允许退货退款等等，保证核心服务下单支付正常就好；            写降级：不需要实时写到数据库中，写到缓存中即可，由 mq 异步更新数据库，保证最终一致性即可；            读降级：针对一些对读实时性、一致性要求不高的场景，可降级为只查询缓存。                                    redis宕机了                  事前预防：部署主从哨兵集群，主节点挂了，哨兵会把从节点提升为主节点。                    如果已经发生了，可考虑服务熔断或服务限流                  服务熔断可看做是服务降级的一种特殊情况，其初衷都差不多。          区别在于服务降级是从整体负载考虑，从业务层面考虑，人为控制。而服务熔断是为了避免由于某个服务不可用而导致整条服务链路崩溃，当触发熔断条件时自动熔断，返回 fallback 方法，是框架层面的，自动熔断自动恢复。                    关于服务限流，这篇文章讲的很好          限流的实现方式                    缓存击穿指的是针对热点数据的请求，无法在缓存中处理，导致数据库压力骤增。这种情况经常发生在热点数据过期失效时，解决办法就是对于热点数据，不要设置过期时间。缓存穿透指的是在缓存和数据库中都没有数据，缓存成了摆设，如果持续有大量请求涌入，会同时给缓存和数据库带来巨大压力。主要原因有两个：  误操作，删除了缓存和数据库中的数据  恶意攻击，专门访问不存在的数据解决办法有三个：      缓存空值或缺省值，其实相当于服务降级        使用布隆过滤器判断数据存不存在，如果不存在，就不用再去数据库查询了          布隆过滤器是基于位图的一种改进。      位图是一种用来判断存在性的很好用的数据结构。比如这么一个问题：有1千万个整数，范围是1到1亿，如何快速的判断某个整数是否存在这1千万个整数中？      可初始化一个长度为1亿、数据类型为 bit 的数组，每个数对应下标位置，比如5对应到array[5]=1。要判断整数 x 是否在这1千万个整数中，只要判断 array[x] 是否等于 1 即可。基于数组随机访问的特性，查找效率也比较高。而且相较于其他数据结构所需空间更少，1亿个 bit 只需 12M，如果采用散列表则需要 1亿 * 4byte = 40M 左右的空间。      可看到位图存储空间和范围相关，如果上述问题的范围是1到10亿，则需120M存储空间。布隆过滤器就是为了解决这个问题的进一步优化。      还以上面的问题为例，范围扩大到1至10亿。布隆空滤器的做法是还是申请一个长度为1亿、数据类型为 bit 的数组，定义 n 个哈希函数，对每个整数进行n次哈希运算得到n个哈希值，将这n个哈希值对于下标的位置都设为 1 。进行多个哈希运算的目的是为了尽可能的降低哈希冲突的概率。      判断的时候如果这n个下标对应的位置有一个不为1，则说明该数据肯定不存在。但如果都为1，也不一定说明该数据肯定存在，因为这几个位置对应的1可能是由其他数据进行哈希运算后填进去的，比如2、4、6 这3个位置都为1，有可能是由其他3个整数分别填进去的。可通过调整哈希函数的个数、位图大小与要存储的数字之间的比例，来降低误判的概率。      所以，布隆过滤器的特点是判定为不存在的肯定不存在，而判定为存在的情况可能有误判(判定为存在，实则不存在)。通过多个哈希函数来共同决定存储位置，这种方法其实误判的概率已经非常低了。所以布隆过滤器特别适合这种场景：判定不存在，或者对存在误判有一定容忍度的应用场景。      具体实现有 Java 的 BitSet（位图）、Redis 的 BitMap（位图）、Guava 的 BloomFilter（布隆过滤器）。            在前端入口进行合法性检测，把恶意请求(比如请求参数不合理、请求参数是非法值、请求字段不存在等)过滤掉  "
} ,

{
"title"    : "MySQL学习笔记(十一)：order by",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/10/25/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%8D%81%E4%B8%80)-order-by/",
"date"     : "2021-10-25 00:00:00 +0800",
"content"  : "order by  的内部实现有两种： 全字段排序 和 rowid排序。按是否需要 磁盘临时文件 辅助排序可分为：内存排序 和 外部排序。按是否需要 临时表 辅助排序可分为：内存临时表 和 磁盘临时表。查看是否需要排序当 explain 中的 Extra 字段显示 Using filesort，表示该查询需要排序。  注：只是表示需要排序，这里不区分是在内存还是磁盘，别被 filesort 误导。根据数据量和 sort_buffer 大小，可能是内存排序，也可能是外部排序(需要借助磁盘临时文件辅助排序)。MySQL 会为每个线程分配一块内存用于排序，称为 sort_buffer。对应的参数为 sort_buffer_size ，顾名思义为 sort_buffer 的大小。默认值为256k，最小可设为32k。排序实现可通过 optimizer_trace 中的 filesort_summary 中的 sort_mode 来确定排序采用了哪种实现逻辑。1234# 只对当前session生效1.set optimizer_trace = &#39;enabled=on&#39;;# 这两句必须一起执行才能看到 `OPTIMIZER_TRACE` 输出，而且只能在命令行中使用，datagrip中看不到输出2.执行sql;SELECT * FROM `information_schema`.`OPTIMIZER_TRACE`#92;G截取 optimizer_trace 部分输出(在末尾部分)如下：  为了演示外部排序，将 sort_buffer_size 设为了最小值 32768(32k)，123456789101112filesort_summary:               memory_available: 32768,              key_size: 8,              row_size: 39,              max_rows_per_buffer: 840,              num_rows_estimate: 18446744073709551615,              num_rows_found: 20211,              num_initial_chunks_spilled_to_disk: 21,              peak_memory_used: 35384,              sort_algorithm: std::sort,              sort_mode: lt;fixed_sort_key, packed_additional_fieldsgt;            memory_available 即为排序可使用的内存大小，和上面设置的 sort_buffer_size一致。peak_memory_used 表示排序过程中的峰值内存使用大小，可见大于 sort_buffer_size，内存装不下了，只能使用外部排序，借助磁盘临时文件来辅助排序。num_initial_chunks_spilled_to_disk 即为排序过程中使用的临时文件数，可见此次排序使用了21个临时文件，使用归并排序算法。num_rows_found 表示有 20211 行数据参与了排序。sort_mode 总共有3种情况，说明了在 sort_buffer 中包含了哪些内容：      lt;sort_key, rowidgt;: This indicates that sort buffer tuples are pairs that contain the sort key value and row ID of the original table row. Tuples are sorted by sort key value and the row ID is used to read the row from the table.          即 rowid排序，意思是排序时只使用了排序字段和rowid，目的是为了装入更多数据，尽量使用内存排序。自然，为了获取其他字段内容，还需要根据rowid回表查询。      对于有主键的Innodb表，rowid 就是主键；对于没有主键的Innodb表，系统会生成一个6字节的 rowid 作为主键。      在 8.0.20 之前，还有一个参数：max_length_for_sort_data，默认值为4096，用于控制排序的行数据长度。如果单行数据长度超过这个值，MySQL就会由 全字段排序 转为使用 rowid排序。      从 8.0.20 开始，max_length_for_sort_data 由于优化器的调整已经过时了，设置这个参数将不会有任何作用。      MySQL的一个设计思想是能用内存就用内存，对于Innodb表来说，会优先选择全字段排序，因为不需要回表(当数据页不在内存中时，回表还是需要读磁盘)，结果集直接就从 sort_buffer 返回了。rowid排序是第二选择。如果转成 rowid 排序内存还放不下的话，最后会转成外部排序。            lt;sort_key, packed_additional_fieldsgt;: Like the previous variant, but the additional columns are packed tightly together instead of using a fixed-length encoding.          即全字段排序，sort_buffer 中包含了排序的字段和查询引用到的全部字段(因此叫做全字段排序)，结果集会直接从 sort_buffer 中返回。      packed 表示排序过程由于 sort_buffer 紧张(见上图)，对字符串做了“紧凑”处理，在排序过程中是按照字段实际长度来分配空间的。            lt;sort_key, additional_fieldsgt;: This indicates that sort buffer tuples contain the sort key value and columns referenced by the query. Tuples are sorted by sort key value and column values are read directly from the tuple.          同上，只是没有了 packed。      示例：      将 sort_buffer_size 恢复为默认值：262144(256k)，再看看有什么变化：      12345678910111213filesort_summary:               memory_available: 262144,              key_size: 8,              row_size: 35,              max_rows_per_buffer: 1001,              num_rows_estimate: 18446744073709551615,              num_rows_found: 20211,              num_initial_chunks_spilled_to_disk: 0,              peak_memory_used: 43043,              sort_algorithm: std::stable_sort,              unpacked_addon_fields: using_priority_queue,              sort_mode: lt;fixed_sort_key, additional_fieldsgt;                        可看到 peak_memory_used lt; memory_available，说明 sort_buffer 完全够用了，直接在内存排序即可，num_initial_chunks_spilled_to_disk 为0，表示不需要借助磁盘临时文件来辅助排序。      而且 sort_mode 中也没有了 packed，因为内存空间已经完全够用了，不再需要做“紧凑”处理了。      排序提速从上面分析可以看出，排序是个代价比较高的操作，尤其是当转成外部排序时，会涉及到多次磁盘IO。所以尽量使用内存排序是最优解，对应可调整的参数是 sort_buffer_size。那还有没有更进一步的优化？很明显，借助索引，可以完全避免排序。比如这样一句sql：1select * from t where city = &#39;xxx&#39; order by name limit 1000;便可以建立一个 (city,name)  的联合索引，那么在 (city,name) 这棵索引树上便是先按 city 排序，再按name 排序(即 city 相同时 name 局部有序)。这样的话只需先快速定位到 city，然后依次往后遍历取出 id，再回表取出整行数据，作为结果集的一部分直接就返回了，完全避免了排序。而且，由于是有序的，对于 limit 来说更友好，不再需要遍历所有 city=xxx 的数据了，遍历到第1000条便可直接结束了(explain 中的 rows 不准)。更进一步，如果查询没必要获取全部字段，则可利用到覆盖索引，直接从索引树获取到全部字段，连回表查询都不用了。如上面的 sql 可改为：1select city,name,age from t where city = &#39;xxx&#39; order by name limit 1000;此时建一个 (city,name,age) 的联合索引，即可满足排序需求，又不需要回表查询，此时是最优解。当然，还是那句老话，维护索引有代价，需综合考虑。实例假设维护一个人员信息表，现已有索引 (city,name) ，问以下查询是否需要排序？1select * from t where city in (A,B) order by name limit 100;虽然已有 (city,name) 索引，但只能保证同一 city 内 name 有序，不同 city 之间 name 仍然是无序的。所以该查询在数据库内部仍然需要排序。如果出于种种原因想要避免在数据库内部排序，有什么办法？那就在应用端排序，把一条sql拆为两句：12select * from t where city = A order by name limit 100;select * from t where city = B order by name limit 100;拆开的查询可以利用到索引，不需要排序。在应用端便可得到两个有序数组，使用归并排序合并为一个有序数组后，取前100即为结果集。如果是分页查询呢，比如：1select * from t where city in (A,B) order by name limit 10000,100;还是和上面的思路一样，拆分为2个查询：12select * from t where city = A order by name limit 10100;select * from t where city = B order by name limit 10100;同样使用归并算法得到一个有序数组，取第 10000 ~ 10100 的值即为结果集。这时的问题是数据量太大，如果应用端内存紧张的话，可考虑只查询 id 和 name：12select id,name from t where city = A order by name limit 10100;select id,name from t where city = B order by name limit 10100;该查询不需排序，且会用到覆盖索引，也不用回表。合并得到有序数组后，取出第 10000 ~ 10100 的 id，再拿这100个 id 去数据库查出所有记录即可。  随机排序order by rand()随机排序在现实中的应用场景还是比较多的，可能会首先想到使用order by rand() 。比如：1select * from words order by rand() limit 3;表示从单词表中随机抽取3个单词。虽然用法简单明了，但它的执行效率怎样(对于数据量不是特别大的表，其实也完全够用了)，做个实验。先构造一张 words 表和实验数据：12345678910111213141516171819CREATE TABLE `words` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `word` varchar(64) DEFAULT NULL,  PRIMARY KEY (`id`)) ENGINE=InnoDB;delimiter ;;create procedure idata()begin  declare i int;  set i=0;  while ilt;10000 do    insert into words(word) values(concat(char(97+(i div 1000)), char(97+(i  1000 div 100)), char(97+(i  100 div 10)), char(97+(i  10))));    set i=i+1;  end while;end;;delimiter ;call idata();explain 下看看：Extra 显示 Using temporary; Using filesort ，表示需要临时表，需要排序。  注：需要和上面讲到的临时文件区分开来，临时表和临时文件是两个东西。为什么需要临时表?order by rand() 的实现逻辑是为每一行数据生成一个随机数，然后按这个随机数来排序，以达到随机排序的目的。所以需要一张临时表来存放这个新生成的随机数字段。再次强调，Using filesort 只是表示需要排序，有可能是内存排序，也有可能是外部排序。不要看它的名字是 filesort，就认为一定是外部排序。它的整个流程是这样的：  生成一张内存临时表。扫描 words 表，将 word 字段和生成的随机数存入临时表；  将 word 字段和随机数放入 sort_buffer 排序；  从 sort_buffer 中取出前3个 word，作为结果集返回。可通过 optimizer_trace 验证一下：12set optimizer_trace=&#39;enabled=on&#39;;select * from words order by rand() limit 3;select * from information_schema.OPTIMIZER_TRACE#92;G可看到该排序使用了全字段排序，且没有使用到磁盘临时文件。sort_buffer 中同时最多有4行数据存在，每一行长度为 8+279=287。为什么只有4行？难道不应该是对10000行数据排序吗？答案是filesort_priority_queue_optimization中 chosen 为 true，表示使用了优先级队列排序(即堆排序)。因为只需要 limit 3，相当于求 top3，剩下9997条数据并不需要排序，堆排序完美适合解决这个问题。  大顶堆求 top k 小，小顶堆求 top k 大。而且最后堆顶元素就是第k大(小)元素。  比如数据集为[0.9,0.4,0.5,0.7,0.3,0.8]，利用大顶堆求top3小：      建立初始堆：[0.9,0.4,0.5]，0.9为堆顶元素；    插入0.7，比0.9小，交换，0.7为最大元素，堆无需调整，此时堆为 [0.7,0.4,0.5]；    插入0.3，比0.7小，交换，0.3下沉堆化，此时堆为 [0.5,0.4,0.3]；    插入0.8，比0.5大，无需交换，堆不变，此时堆为 [0.5,0.4,0.3]；    遍历结束，top3小即为 [0.5,0.4,0.3]，且0.5为第3小的元素。  自己实现随机排序方法一随机取一行的算法如下：  取得主键 id 的最大值M和最小值N  用随机函数生成一个介于M和N之间的数 X=(M-N)*rand()+N  取 ≥X 的第一个 id 的行用sql表示如下：123select max(id),min(id) into M,N from t;set X = floor(M-N)*rand()+N;select * from t where id gt;= x limit 1;max(id) 和 min(id) 都不需要扫描索引，第三步的 select 也可以用索引快速定位，速度很快。但是有个致命缺陷，它不是真正的随机。如果 id 是连续的没问题，但如果 id 之间有空洞，则选择不同行的概率不一样。如 [1,2,4,5]，按这个算法，4 被选中的概率是其他的两倍。但是它速度最快。方法二  取得整个表的行数，记做 C  Y = floor(C*rand())  limit Y,1limit Y,1 的处理逻辑是按顺序一行行读出来，丢掉前Y个，取下一个作为结果返回(可通过慢查询日志的 Rows_examined 验证)。  这里可能会有点疑问，为什么不走主键索引树，直接快速定位到 Y不就行了。  这是想当然了，因为并不知道第 Y 行的主键id是多少呀，除非只有当主键id从1开始连续递增才行(第Y行的主键id就是Y)。所以该算法总共扫描的行数为：C+Y+1 ，执行代价比方法一要高一些，但跟 order by rand() 比起来还是快得多，而且严格随机。那么用该算法解决随机获取3个单词的步骤如下：  取得整个表的行数，记做 C  Y1 = floor(Crand()), Y2 = floor(Crand()), Y3 = floor(C*rand())  limit Y1,1；limit Y2,1；limit Y3,1总共需扫描  【C+(Y1+Y2+Y3)+3】 行 。其实还有进一步优化的空间，因为Y1、Y2、Y3 重复扫描了：取得 Y1、Y2、Y3后，设最大值为M，最小值为N，执行1select * from t limit N,M-N+1从结果集中取第一条、最后一条、(Y2-Y1)条即可。改进后扫描行数为 C+M+1，大大减少了扫描行数。不可一味盲目追求极致解，够用就好！！！官方优化指南ORDER BY Optimization"
} ,

{
"title"    : "MySQL学习笔记(十)：count",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/10/22/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%8D%81)-count/",
"date"     : "2021-10-22 00:00:00 +0800",
"content"  : "count(*) 的实现方式不同引擎实现方式不同：      MyISAM：将总行数直接存在磁盘上，查询的时候返回，因此速度很快        Innodb：因为需要支持事务，而事务是由 MVCC 实现的，一行记录需要先判断对查询事务的可见性，所以“应该返回多少行” 是不确定的。因此Innodb的实现方式是把数据一行一行的从引擎中读出来，可见的行才会被累计进来。    举个例子，假设表 t 有 10000 条数据：                            sessionA          sessionB          sessionC                                      begin;                                                select count(*) from t;                                                                      insert into t(插入一行);                                     begin;                                                insert into t(插入一行);                                     select count(*) from t; (返回10000)          select count(*) from t; (返回10002)          select count(*) from t; (返回10001)                      可见同一时刻的并发查询，返回的结果是不一致的。    由于需要遍历全表，因此对于大表，count 会变得很慢。    为 count 提速用缓存保存计数这是第一时间会想到的方式，比如使用 redis。更新数据时顺便更新 redis中的计数值，count 的时候直接查询 redis 即可，速度很快。但是这会有2个问题：      缓存异常宕机会丢失更新          redis持久化有RDB和AOF两种方式；RDB按照备份策略，比如60秒1000个k-v被修改，备份过程中宕机，那么这个阶段的所有更新都会丢失；AOF按照备份策略，比如 appendfsync always 策略，同步记录所执行的指令到日志文件，但是它的日志和mysql的WAL(先写日志)不同，它是后写日志，可能指令执行后写日之前宕机，那这个数据就丢失了，虽然丢失数据较少且概率较低，但依然存在这个可能。        可以在重启后到数据库中执行一次 count(*)获取行数后填到 redis 中，毕竟异常宕机重启不会经常发生，偶尔一次全表扫描的成本还是可以接受的。        结果可能不准    由于更新数据和更新 redis 统计计数不是一个原子操作，可能会出现统计计数和数据不一致的问题。    比如以下场景：                            sessionA          sessionB                                      redis 计数+1                                                读 redis 计数                                     查询近100条记录                          插入一行数据                                 sessionB 在查询计数时已经加了1，但是查不到新插入的数据(将sessionA的两个操作调换顺序也是类似的)。究其原因， redis 和数据库可以看做两个不同的数据源，不能保证两个操作的原子性(不可分割)。这类问题属于分布式一致性问题，虽可通过引入其他手段解决，但会使一个简单的计数查询需求变得很复杂，没有必要。  在数据库保存计数那不存到 redis  中，存到数据库计数表中，会出现上面的问题吗？  首先，Innodb 支持崩溃恢复，所以不存在更新丢失的问题。  将上图中 redis  的操作换成对数据库计数表的操作，不会出现数据不一致的问题。很明显，Innodb的事务保证了操作的原子性。所以，对于大表，推荐使用数据库计数表来提速 count 。进一步优化根据 两阶段锁 协议，可以通过调整事务内更新语句的顺序：将 update 计数表放到最后，来减少计数表行锁等待的时间，提高并发度。  不同 count 用法的区别首先，count 是一个聚合函数，它的逻辑是，对于返回的结果集，一行一行的判断，不为 NULL 就+1。      count(主键id)    遍历整张表，把每一行的 id 取出来返回给 server 层，server 层判断 id 不可能为空，+1。          这里可能会觉得奇怪，id 根本不可能为空，为什么还要多此一举去判断一下。      的确是没什么必要，但类似需要优化的地方太多了，MySQL专门对 count(*) 优化过了，直接使用 count(*)  就好了。            count(1)    遍历整张表，但不取值。Server 层对于返回的每一行，放一个 “1“ 进去，判断不可能为空，+1。          很明显，相较于 count(主键id)，count(1) 效率更高，因为 count(id)  还需要解析数据行、拷贝字段值等操作。            count(字段)    遍历整张表，把每一行对应的字段值取出来返回给 server 层，server 层判断是否为空，不为空+1。所以效率 count(字段) ≤ count(*)。        count(*)    做了专门的优化，并不会把全部字段取出来，而是直接不取值，认定count(*) 肯定不为空，直接按行累加。  按效率排序：count(字段) lt; count(主键id) lt; count(1) ≈ count(*)。推荐直接使用 count(*)。"
} ,

{
"title"    : "MySQL学习笔记(九)：如何收缩表空间?",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/10/20/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B9%9D)-%E5%A6%82%E4%BD%95%E6%94%B6%E7%BC%A9%E8%A1%A8%E7%A9%BA%E9%97%B4/",
"date"     : "2021-10-20 00:00:00 +0800",
"content"  : "使用 delete 删除数据不会使表空间变小前文有提过，delete 只是将记录标记为删除，并没有真正删除。只有当其对应的 update undo logs 被清除时才会由后台 purge 任务物理删除，即没有事务再需要这些版本的记录时会执行物理删除。即便是物理删除，也不会把磁盘空间返还给操作系统。究其原因，出于性能考虑，物理删除后的空间会被复用。删除记录后，当插入符合范围条件的数据时，原空间会被复用。删除一整个数据页上的所有记录后，当需要使用新页的时候，原数据页会被复用。可通过 show table status like &#39;t&#39; 来查看表碎片大小，如果过大，可通过重建表来释放表空间。  几个字段的含义，都是针对 Innodb，MyISAM略有不同：      data_length：聚簇索引大小近似值，单位是字节。    index_length：非聚簇索引大小近似值，单位是字节。    data_free：已分配但未使用的空间大小近似值，单位是字节。表碎片大小即是查看这个字段。    这三个值之和近似接近 ibd 文件大小。重建表不光删除数据会造成空洞(可被复用但没被使用的空间)，插入和更新也会。因为插入数据往往都是随机的，即基本不可能按索引递增顺序插入，就很有可能造成数据页的分裂。比如当一个数据页满了，此时再插入一行数据到此节点就会造成数据页分裂，原数据页末尾就会产生空洞。更新可理解为先删除后插入，同理。重建表就是通过去掉这些空洞，来达到收缩表空间的目的。命令如下：1alter table A engine=InnoDB5.6版本之前内部流程如下：  新建一张与原表结构相同的临时表  按主键ID递增的顺序，将数据一行一行的从原表读出来再插入到临时表中。这一步就可以去掉原表主键索引上的空洞。  用临时表替换掉原表这个流程有个问题，在第2步中，原表不能有更新操作。也就是说，这个ddl不是online的。从5.6版本开始  建立一个临时文件，扫描原表的所有数据页  用数据页中原表的记录生成B+树，存储到临时文件中  在生成临时文件的过程中，将对原表的操作记录在一个日志文件(row log)中  临时文件生成后，将日志文件中的操作应用到临时文件  用临时文件替换掉原表的数据文件很明显，在该过程执行中是允许对原表做增删改操作的，这也是 Online DDL 名字的由来。注意：对于大表来说，因为需要扫描原表数据和构建临时文件，这个步骤是很消耗IO和CPU资源的。尤其对于线上业务，要很小心的控制操作时间。如果想要安全操作的话，推荐使用 github 的 gh-ost。online 和 inplaceonline ddl 构建的临时文件位于 Innodb 内部，整个过程也都是在 Innodb 内部完成。对于 Server 层来说，没有把数据挪动到临时表(5.6版本之前的操作流程)，相当于是一个 “原地” 操作，因此叫做 inplace。12345alter table A engine=InnoDB;其实隐含的意思是alter table A engine=InnoDB,ALGORITHM=inplace;相对的，就有1alter table A engine=InnoDB,ALGORITHM=copy;对应的就是5.6版本之前的操作流程。所以，如果现在有一个1TB的表，磁盘空间为1.2TB，能不能做一个inplace的ddl呢？答案是不能，因为临时文件也是要占用空间的。总结来说，online 是指在操作过程会不会阻塞对原表的增删改操作。inplace 指的是在 Serve 层建临时表还是直接在存储引擎内建临时文件。两种重建表的方式及区别      alter table A engine=InnoDB    如上文所述        optimize table A    等同于 alter table A engine=InnoDB + analyze table A (analyze：重新统计索引信息)  什么情况下重建表空间反而变大了？      表本身已经没有空洞了，比如刚刚重建过一次，这时候再重建，如果恰好在重建期间有外部的 DML 在执行，就有可能会引入新的空洞。        而且重建表的时候并不是把每个数据页都占满，而是会留下 1/16 的预留空间给后续的更新用，也就是说重建后的表并不是百分百紧凑的。    以下过程就可能会出现这种情况：          重建一次      插入一部分数据，但是这部分数据使用的是预留空间      再重建一次。        这时由于预留空间被使用了，再次重建时就需要再额外留出 1/16 的预留空间，所以空间反而变大了。  "
} ,

{
"title"    : "MySQL学习笔记(八)：MySQL锁",
"category" : "",
"tags"     : "mysql, 锁",
"url"      : "/blog/2021/10/18/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%85%AB)-MySQL%E9%94%81/",
"date"     : "2021-10-18 00:00:00 +0800",
"content"  : "MySQL中的锁有三类：全局锁、表级锁、行级锁全局锁即给整个数据库实例上锁，让整个库都处于只读状态，除查询以外的操作都会被阻塞。server层实现。  如果加上全局锁后，客户端由于异常断开，MySQL会自动释放这个锁。加锁：123flush tables with read lock;或flush table t with read lock;解锁：1unlock tables;使用场景：做全库逻辑备份隐患：  由于只能查询，所以在此期间业务基本停摆  如果在主库上备份，业务停摆；如果在备库上备份，在此期间备库不能执行从主库同步过来的 binlog，会导致主从延迟那如果备份的时候不加全局锁会发生什么情况呢？不加锁，会导致备份出来的库不是同一个逻辑时间点的，数据从业务逻辑上看是不一致的。比如在备份过程中先备份了A表，然后执行了一个业务操作，这个业务操作会同时更新A表和B表，再备份B表。那么这个时候备份出来的数据A表还是老版本，而B表已经被更新了，这个备份就是有问题的，是逻辑不一致的。为了既不影响业务，也要保证备份视图的逻辑一致性，推荐采用另一种全库备份的方法：mysqldump -single-transaction。导数据之前会启动一个事务，来确保拿到一致性视图。而且由于 MVCC 的支持，在此期间是可以正常更新的。当然显而易见，这种方法只适用于支持事务的存储引擎，所以这也是为什么推荐使用 Innodb 而不是 MyISAM 的一个原因。表级锁server层实现，分两种：元数据锁(MDL)和表锁。表锁  与全局锁一样，也会在客户端断开时自动释放加锁：1lock tables t1 read,t2 write;解锁：1unlock tables;以上述加锁语句为例，t1加了读锁，t2加了写锁：  任何线程都不能写 t1 ，包括本线程  只有本线程能读写 t2  本线程甚至不能访问除 t1、t2 之外的任何表，这点很奇怪，不懂为什么这么设计，访问其他表时会报错 Table &#39;xxx&#39; was not locked with LOCK TABLES元数据锁Meta data lock，MDL不需要显示使用，访问表的时候会自动加上。从MySQL5.5开始引入，当做增删查改时，会加MDL读锁；当变更表结构时，会加MDL写锁。      读锁不互斥，因此可以有多个线程同时对一张表增删改查        读写互斥，即不能有多个线程同时更改表结构，或一个线程在增删改查而另一个线程在更改表结构        MDL锁在事务提交时才会释放，在变更表结构时要特别小心，以免锁住线上的查询和更新，导致整张表不能读写。下面是一个示例：                            session A          session B          session C          session D                                      begin;                                                           select * from t limit 1;                                                                      select * from t limit 1;                                                                      alter table t add f int;                                                                      select * from t limit 1;                      session A 先开启了一个事务，执行了一次查询，并且没有马上提交，这时会对表 t 加一个 MDL 读锁。    session B 也需要一个 MDL 读锁，读锁之间不互斥，可以正常执行查询。    session C 要加一个字段，需要一个 MDL 写锁，读锁和写锁互斥，所以必须等待表 t 释放读锁之后才能继续。    session D 需要一个读锁，这里需要注意的是，表 t 上会有一个等待获取锁的锁队列，而获取MDL写锁的优先级要比获取读锁的优先级高，所以导致session D 也被阻塞。    最后的结果就是表 t 完全被锁住，完全不可读写了。如果客户端还有重试机制，一直在发起重试请求，MySQL的线程很快就会爆满，最后导致整个实例挂掉。  ​        解决办法：​        1、监控长事务(information_schema.innodb_trx)，要么先暂停DDL，要么kill掉这个长事务​        2、但是对于一些热点表，kill未必管用，可能刚kill掉一个长事务，新的请求立马又来了。这种情况下，理想的办法是为 alter table 语句设定等待时间，如果在此期间能拿到MDL写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后再重试这个命令来重试这个过程。MariaDB 和 AliSQL 已经提供了这个功能，MySQL暂时没有。12ALTER TABLE t NOWAIT add column ...ALTER TABLE t WAIT N add column ... MySQL可以通过调整 lock_wait_timeout 值来控制这个超时时间，默认值是 31536000s，即1年，显然这个时间太长了。  获取元数据锁的超时时间，注意与 innodb_lock_wait_timeout 区分开来案例一假如正在备库执行一个 -single-transaction 的逻辑备份，此时在主库上对表 t 执行了一个 DDL，备库会出现什么情况？备库会根据 DDL 的 binlog 到达的时间点不同而出现不同的情况。先拆解一下 mysqldump -single-transaction 在内部的执行逻辑：      set session transaction isolation level repeatable read;        start transaction with consistent snapshot;    (其他表的备份逻辑)        SAVEPOINT sp;    (时刻1)        show create table t;    (时刻2)        select * from t;    (时刻3)        rollback to savepoint sp;    (时刻4)    (其他表的备份逻辑)  在时刻1到达：没有任何影响，此时表 t 上没有任何 MDL 锁，所以可正常执行，备份得到的是新的表结构；在时刻2到达：此时已经先备份完了 t 的表结构，DDL才到达，在执行第5步的时候会报错：Table definition has changed,please retry transaction ，mysqldump 终止；在时刻3到达：此时表结构和数据都已备份完成，但是 MDL 读锁还没释放(会在第6步后才释放)，所以 DDL 操作会阻塞，备份得到的是旧的表结构；在时刻4到达：MDL 读锁已被释放，DDL 可正常执行，备份得到的是旧的表结构。行锁由存储引擎实现，Innodb 支持，MyISAM 不支持。如果不支持行锁，就只能使用表级锁，也就意味着锁的粒度太大并发度就会降低。这也是为什么推荐使用 Innodb 的重要原因之一。两阶段锁之前提到过一个两阶段提交，行锁有一个两阶段锁协议，也被称为 2PL。定义两阶段指的是分为加锁阶段和解锁阶段，在加锁阶段只能加锁不能解锁，在解锁阶段只能解锁不能加锁。单看定义很难理解，换成大白话说就是行锁在需要的时候才加上，但并不是不需要了就立即释放，而是要事务结束后才会释放。为什么需要两阶段锁？重点就是在事务结束后才会释放所有行锁，而不是用完立即释放。任何锁的本质就是保证并发操作的正确性，将并行改为串行。二阶段锁用来保证并发更新操作的正确性，两个并发的更新操作，必须等其中一个提交后另一个才能继续，否则就会发生更新被覆盖的情况。假设不存在两阶段锁协议，会发生如下情况：同时发起2个操作，向同一个账户打200块，账户原余额有100块。  sessionA 发起打款操作，获取到写锁，用 update 更新账户余额为 300  update 完毕，假设不存在两阶段锁，用完立即释放，释放写锁，此时事务尚未提交  sessionB 发起打款操作，获取到写锁，根据一致性视图可见性规则：事务未提交，更新不可见。得到的账户余额仍为100，用 update 更新账户余额为 300  update 完毕，立即释放写锁  sessionA 提交，账户余额为 300  sessionB 提交，账户余额为 300所以，因为存在两阶段锁协议，在第2步结束后，由于事务尚未提交，写锁仍未释放，则第3步的 update 操作必须等待 sessionA 提交后才能继续，此时 sessionA 读到的余额为300，再执行 update 后更新余额为500，这才是符合逻辑的结果。如何优化？锁虽然保证了并发操作的正确性，但是由并行改为串行降低了并发度。所以另一个问题就是如何最大限度的提高并发度？由于行锁是在需要的时候才加上，在事务结束后统一释放。所以针对包含多个更新的事务，可以调整事务内更新语句的顺序，将会产生行锁竞争的语句尽量往后放，从而让等待行锁的时间最小化，以达到提高并发度的目的。示例：假如有一个在线订票业务，订票逻辑可以简化为下列步骤：  从账户余额扣掉票钱  给系统余额加上票钱  记录一条日志假设同时发起2个订票请求，可以看到，在这个事务中，会产生行锁竞争的是第2步(直白说就是会 update 同一行)。按这个顺序的话，系统余额表上的行锁会从第2步开始加上，第3步完成后事务提交时释放。调整下顺序，改为：  记录一条日志  从账户余额扣掉票钱  给系统余额加上票钱这时候系统余额表上的行锁持续时间就缩短为1步了，从第3步开始加上，到第3步完成后事务提交时释放。虽然在这里看就是少了一小步(一条语句的执行时间)，但如果这个业务请求并发量很大的话，这个优化的效果就会非常明显。死锁为什么会出现死锁？简单说就是出现锁的循环等待。示例如下：                   sessionA      sessionB                  1      begin;      begin;              2      update t set k = k+1 where id=1;                     3             update t set k = k+1 where id=2;              4      update t set k = k+1 where id=2;                     5             update t set k = k+1 where id=1;      由于事务都尚未提交，行锁还未释放。第4步要获取 id=2 的行锁，需要等待sessionB提交；第5步要获取 id=1 行锁，需要等待sessionA提交，死锁产生了。解决办法      等待直到超时，然后退出    Innodb中有个参数用于设置这个超时时间：innodb_lock_wait_timeout，默认值为 50s。这个默认值对于业务来说是不能接受的，相当于卡顿50s。但是如果设成较小的值，又很有可能造成误伤：如果不是死锁，而就是普通的锁等待，此时并没有循环等待的情况，但是由于超过了阈值而被当成了死锁而提前退出了。所以这种方法一般不采用。        主动死锁检测    innodb_deadlock_detect 用于控制是否打开主动死锁检测，默认是 ON。    这是一种相对较好的方式，但需要注意的是它的资源消耗有可能会很大。对于每个新加入进来的线程，都要先判断会不会由于自己的加入而导致死锁，这是一个时间复杂度为 O(n^2) 的操作。假设有1000个并发线程同时更新同一行，这个死锁检测就是100万量级的。    虽然最终检测的结果是没有死锁，但此期间需要消耗大量的CPU资源。所以当出现CPU消耗接近100，TPS却很低的话，很有可能就是死锁检测导致的。    那如何优化这种热点行更新问题？          简单粗暴的方法就是如果确认不会出现死锁，直接关闭死锁检测。但这个方法明显危险系数很高，万一还是出现了死锁的话只能依靠超时机制，而如上面所述，超时机制的阈值很难设置。      拆分热点行。将一行拆分为多行，比如一条账户记录可以拆分为10条子账户记录，账户总额就等于10条子账户余额之和，在需要更新账户余额时，随机选择其中一条进行更新。这样就将一行上的死锁检测成本、锁等待个数、冲突概率都降为了原来的1/10。这种方案属于设计层面上的优化，需要结合业务逻辑做详细的设计和测试。      案例二：删除大量数据现需要删除前10000行数据，有以下三种方式：      delete from t limit 10000;        在一个连接中循环执行20次 delete from t limit 500;        在20个连接中同时执行 delete from t limit 500;  哪种方式好一些？第一种：执行时间较长，意味着占用锁(MDL读锁、X锁)的时间会比较长；而且大事务在从库上回放的时间也较长，在此期间会导致主从延迟；第三种：人为的制造了行锁冲突，而且大概率会重复删除，达不到删除前10000行数据的目的；第二种方式较好。案例三：查询长时间不返回主要思路：通过 show processlist 查看当前语句处于什么状态，一般是被锁住了。等MDL锁当有其他线程持有MDL写锁时，就会把查询堵住。比如以下场景：            sessionA      sessionB                  lock table write;                            select * from t where id=1;      show processlist 表现如下：可看到状态为 Waiting for table metadata lock。解决办法很简单，谁持有MDL写锁，就把它 kill 掉。但有时候从 show processlist 中不容易看出，可查询 sys.schema_table_lock_waits表，前提是 performance_schema=on，这个选项在MySQL8中默认开启，相对于不开启大约有 10 的性能损失。或者也可以通过 sys.innodb_lock_waits 查询：需要注意 kill query 4  和  kill 4 的区别：  kill query 4  ：表示停止4号线程当前正在执行的语句。有时候这个命令可能没用，因为语句可能已经执行完了，但事务没提交导致锁不能释放。  kill 4 ：直接断开连接，连接断开时会自动释放锁。等行锁这比较普遍：            sessionA      sessionB                  begin;                     update t set c=c+1 where id=1;                            select * from t where id=1 lock in share mode;      由于 sessionA 事务未提交，导致 id=1 的写锁不能释放，而 sessionB 的查询需要读锁，所以会被堵住。next-key lock目的：解决幻读问题什么是幻读？  The so-called phantom problem occurs within a transaction when the same query produces different sets of rows at different times. For example, if a SELECT is executed twice, but returns a row the second time that was not returned the first time, the row is a “phantom” row.即一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。幻读专指新插入的行。一个幻读示例：假设存在 person(id,name) 表，含2条数据：(1,”foo”)、(2,”bar”)            事务A      事务B                  select count(1) from person 查到2条数据                            insert into person(id,name) values(3,”zoo”) 插入一条数据                     commit 提交              insert into person(id,name) values(3,”zoo”) 插入一条数据，报错提示主键重复                     select * from person where id = 3，又查不到数据，但是又不能insert，就很奇怪             解决办法：使用 for update 加 next-key lock。next-key lock 定义：行锁 +  间隙锁 ，前开后闭区间  To prevent phantoms, InnoDB uses an algorithm called next-key locking that combines index-row locking with gap locking. InnoDB performs row-level locking in such a way that when it searches or scans a table index, it sets shared or exclusive locks on the index records it encounters. Thus, the row-level locks are actually index-record locks. In addition, a next-key lock on an index record also affects the “gap” before the index record. That is, a next-key lock is an index-record lock plus a gap lock on the gap preceding the index record. If one session has a shared or exclusive lock on record R in an index, another session cannot insert a new index record in the gap immediately before R in the index order间隙锁间隙锁即用来锁住行之间的间隙，换句话说，可以锁住 “不存在” 的行，也就是 insert 操作。间隙锁之间并不冲突，与间隙锁冲突的是 insert 这个操作。两个会话可以同时执行 select * from t where c=7 for update，说明两个会话都获取到了同一个间隙锁，但如果此时有 insert 操作就会被阻塞。  注： insert 操作并不单指 sql 中的 insert，而是广义上的索引树上的所有插入行为。RC 和 RR 在锁问题上的区别RC可以禁用间隙锁，RC下只有行锁。而且RC级别下在语句执行完毕后，就会把“不满足条件”的行锁释放，不需要等到事务提交。RR级别下所有的加锁资源都是在事务提交或回滚时统一释放。比如表 t(id,c)，id是主键，c 是普通字段。将隔离级别设为RC，执行 select * from t where c = 5 for update，由于需要扫描主键索引树，会给所有扫描到的行都加上行锁。但是在这句语句执行完毕后，无需事务提交便会把所有 c!=5 的行锁都释放，此时其他事务便可对所有 c!=5 的行进行更新操作。同样的实验如果将隔离级别设为RR便会发生阻塞，因为RR级别下所有的锁都是在事务提交时才会释放。所以 RC 级别下，锁的范围更小，锁的时间更短，在业务允许的情况下隔离级别设为RC可以提高并发度。当设为RC后，Innodb 会强制要求将 binlog_format 设为 ROW，这是为了保证数据和binlog日志的一致。以下文综合案例中的表和数据举例：            sessionA      sessionB                  begin;select * from t where d=5 for update;update t set d=100 where d=5;                            update t set d=5 where id=0;update t set c=5 where id=0;              commit;             在RC级别下，语句执行完就会释放所有 d!=5 的行锁，而且没有间隙锁，因此 sessionB 不会阻塞。执行完毕后数据变为：(id=0, c=5, d=5)、(id=5, c=5, d=100)。如果 binlog_format 为 statement，那么在 binlog 中会产生类似如下日志：  update t set d=5 where id=0;  update t set c=5 where id=0;  update t set d=100 where d=5;  #事务提交时才会写入日志，所以它出现在了最后使用该日志恢复出来的数据变成了：(id=0, c=5, d=100)、(id=5, c=5, d=100)。可以发现，恢复出来的数据和数据库里的数据不一致，这肯定是不行的。改为 row 以后，binlog中的日志类似如下，可使用 mysqlbinlog --base64-output=decode-rows -vv 查看：123456789### UPDATE `test`.`t`### WHERE###   1=0###   2=0###   3=0### SET###   1=0###   2=0###   3=5123456789### UPDATE `test`.`t`### WHERE###   1=0###   2=0###   3=5### SET###   1=0###   2=5###   3=5123456789### UPDATE `test`.`t`### WHERE###   1=5###   2=5###   3=5### SET###   1=5###   2=5###   3=100可看到日志恢复出来的数据和数据库里的数据是一致的。事实上，statement 的 binlog 由于是基于 sql 语句的日志，在保持数据和日志一致性上会有很多问题，比如 now() 函数。因此推荐使用 ROW 的binlog，这也是 MySQL8 的默认配置。综合案例：加锁规则以下规则适用于 5.x 系列 ≤ 5.7.24， 8.x系列 ≤ 8.0.13。其他版本需实践验证，可能会有微调。且都是在可重复读默认级别下，这个前提非常重要，其他级别如读提交有不同的处理。2个原则，2个优化，1个bug：  原则一：加锁的基本单位是 next-key lock，前开后闭区间      原则二：查找过程中访问到的才会加锁        优化一：唯一索引等值查询， next-key lock 会退化成行锁        优化二：索引等值查询，向右遍历且最后一个值不满足等值条件时， next-key lock 退化成间隙锁          在 8.0.26 版本中，唯一索引的范围查询，向右遍历且最后一个值不满足条件时， next-key lock 也会退化成间隙锁。      具体从哪个版本开始修复的，尚未确定。见示例三。            一个bug：唯一索引上的范围查询会访问到第一个不满足条件的值为止          在 8.0.26 版本中已修复，具体从哪个版本开始修复的，尚未确定。见示例五。      以下示例基于：123456789CREATE TABLE `t` (  `id` int(11) NOT NULL,  `c` int(11) DEFAULT NULL,  `d` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `c` (`c`)) ENGINE=InnoDB;insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25);示例一：等值查询间隙锁            sessionA      sessionB      sessionC                  begin;update t set d=d+1 where id=7;                                   insert into t values(8,8,8)                                   update t set d=d+1 where id=10;      分析：  根据原则二：访问到的都加锁。id 是主键索引，能快速定位到 id=5 这条记录，向右遍历下一条记录发现 id=10≠7，遍历结束；  根据原则一：加锁的基本单位是 next-key lock。所以 sessionA 的加锁范围是 (5,10]；  id=8 位于加锁范围内，所以 sessionB 会阻塞；  根据优化二：索引等值查询，向右遍历且最后一个值不满足等值条件时(这里就是10)， next-key lock 退化成间隙锁。所以 (5,10] 会退化成 (5,10)，因此 sessionC 不会被阻塞。这里优化一不适用，因为不存在 id=7 这行记录，因此没有办法退化成行锁。示例二：非唯一索引等值锁            sessionA      sessionB      sessionC                  begin;select id from t where c=5 lock in share mode;                                   update t set d=d+1 where id=5;                                   insert into t values(7,7,7)      分析：  根据原则一得出 sessionA 的加锁范围是 (0,5] (行锁 + 行之前的间隙锁);  根据优化二向右遍历直到 c=10停止，先是 next-key lock (5,10]，然后退化成间隙锁 (5,10)；  所以 sessionA 的加锁范围是 (0,5]  加上  (5,10)；  因此 sessionC 会阻塞；  那为什么 sessionB 不会阻塞？按理说 sessionB 需要 id=5 这一行的行锁，同样应该被阻塞才对。 原因在于 sessionA 的查询用的是覆盖索引，不需要回表。而根据原则二：访问到的才加锁。所以锁只会加在 c 索引树上，而 sessionB 的条件是 id=5，走的是主键索引，因此不会被阻塞。但如果 sessionA 换成 for update  就不一样了， for update  会同时给主键索引上满足条件的行上锁，而 lock in share mode 如果有覆盖索引的情况下只会给覆盖索引上锁。所以，如果使用的是  lock in share mode 加锁的话，为了避免数据被更新，需要绕过覆盖索引的优化：在查询字段中加入索引中不存在的字段，让查询回表，如改成 select d from t where c=5 lock in share mode; 。示例三：主键索引范围锁            sessionA      sessionB      sessionC                  begin;select * from t where idgt;=10 and idlt;11 for update;                                   insert into t values(8,8,8);insert into t values(13,13,13);                                   update t set d=d+1 where id=15;      sessionA 的查询在逻辑上和 select * from t where id=10 是一致的，但是加锁范围却有很大区别。分析：      初步分析，sessionA 的加锁范围是 (5,10] +  (10,15];          idgt;=10 是等值查询，会先定位到 id=10 这行记录，会先加一个 next-key lock  (5,10] ；      idlt;11 是范围查询，会向右遍历到 id=15 停止，发现不满足条件，于是再加一个 next-key lock (10,15]。            根据优化一：唯一索引等值查询会退化成行锁。所以 (5,10] 会退化成 id=10 的行锁；        综上，sessionA 的加锁范围是  id=10 的行锁 +   (10,15]；        因此，sessionB 的第一条语句可正常执行，第二条语句会被阻塞；        sessionC 的语句也会被阻塞。          以上示例是在8.x系列 ≤ 8.0.13中验证的。      在 8.0.26 中，(10,15] 退化成了间隙锁 (10,15)，这条语句不会被阻塞。      估计是调整了优化二：不光是普通索引的等值查询，唯一索引的范围查询也是一样，向右遍历到不满足条件的第一个值时， next-key lock 也会退化成间隙锁。具体是哪个版本调整的，尚未确定。见示例四。      示例四：非唯一索引范围锁            sessionA      sessionB      sessionC                  begin;select * from t where cgt;=10 and clt;11 for update;                                   insert into t values(8,8,8);                                   update t set d=d+1 where c=15;      将示例三中 where 字段由 id 改为 c，分析如下：  同示例三，初步分析，sessionA 加锁范围是 (5,10] + (10,15]；  区别在于优化一不适用了，因为不是唯一索引，所以(5,10] 不能退化成行锁；  所以 sessionB 会被阻塞；  由于 c 是普通索引，所以 c 上的范围查询不满足优化二，不会退化成间隙锁。sessionC 会被锁住。示例五：唯一索引范围锁bug            sessionA      sessionB      sessionC                  begin;select * from t where idgt;10 and idlt;=15 for update;                                   update t set d=d+1 where id=20;                                   insert into t values(16,16,16);        基于8.x系列 ≤ 8.0.13。在 8.0.26 版本中已修复，具体从哪个版本开始修复的，尚未确定。  初步分析，sessionA 加锁范围是 (10,15](直接就定位到了 id=15 这一行)；  id 是主键索引，按理说扫描到 id=15 时就可以结束了，因为 id 是唯一的且在主键索引中是递增的，再往后遍历都是 gt;15 的，不可能满足 idlt;=15。但事实上还会继续往后扫描到第一个不满足条件的值为止，即 id=20，所以加锁范围还会加上 (15,20]。结果就是导致 sessionB 和 sessionC 都会被锁住。示例六：非唯一索引上存在“等值”先插入一条记录：1insert into t values(30,10,30);然后执行以下序列：            sessionA      sessionB      sessionC                  begin;delete from t where c=10;                                   insert into t values(12,12,12);                                   update t set d=d+1 where c=15;            delete 和 for update 加锁的逻辑是类似的。sessionA 的加锁范围是 (5,10] + (10,15)；          注意，现在 c=10 的记录在 c 索引树上有两条：(c=10,id=10) 和 (c=10,id=30) ，在这两条记录中间还有一个间隙锁。这个间隙锁只存在于 c 索引树上，主键索引上只有行锁，见第3点。      由于这个间隙锁只在 c 索引树上，所以它实际上没有任何作用，因为在两个10之间不存在任何int值，知道这个间隙锁的存在就行。            sessionB 会阻塞，sessionC 正常执行。        上面讲过 for update 会同时给主键索引上满足条件的行上锁，这里也是一样，因此在主键索引树还有 id=10 和 id=30 两个行锁。  示例七：limit加锁            sessionA      sessionB                  begin;delete from t where c=10 limit 2;                            insert into t values(12,12,12);      示例六的对照示例，按照示例六的分析加锁范围为  (5,10] + (10,15) ，但是由于加了 limit 2，因此在扫描到两行记录，(c=10,id=10) 和 (c=10,id=30) 后便结束了，因此加锁范围变为了 (5,10]，所以 sessionB 不会被阻塞。这个示例也说明了删除数据时尽量使用limit，不仅可以控制删除的条数更安全，而且还可以减小锁的范围。示例八：降序排序加锁            sessionA      sessionB                  begin;select * from t where cgt;=15 and clt;=20 order by c desc lock in share mode;                            insert into t values(6,6,6);                     update t set d=d+1 where id=10;                     update t set c=c+1 where id=10;        降序排序，整体从右往左扫描。首先定位到 c=20，加上 (15,20] 的 next-key lock；由于是普通索引，继续向右扫描到 c=25，不满足条件 clt;=20 停止，于是再加上 (20,25] 的 next-key lock，然后退化成间隙锁 (20,26)；  向左扫描，直到 c=10 不满足条件 cgt;=15 停止，于是加上 (5,10] 的 next-key lock;  综上，sessionA 在 c 索引树上的加锁范围是 (5,25)；因此insert into t values(6,6,6) 阻塞；  由于是 select *，需要回表。虽然在索引c上的扫描范围是 (5,25)，但是满足条件的行是 c=15、c=20，回表的也是这两行。因此，会在主键索引上加上 id=15、id=20 两个行锁；  因此 update t set d=d+1 where id=10 不会阻塞；但如果把条件换成 c=10 就会阻塞，由此可见，锁是加在索引上的，c索引上有 c=10 的行锁， 主键索引上没有 id=10 的行锁；  update t set c=c+1 where id=10 与上一句不同的是：上一句只需更新主键索引树，这一句需要同时更新主键索引和索引c两棵索引树。 更新主键索引不会被锁住，锁住是因为需要更新 c 索引树(需要更新 c=10 的行)。  如果去掉 desc，那么加锁范围就变成了 (10,25]，可以实验验证一下。示例九：死锁            sessionA      sessionB                  begin;select * from t where c=10 lock in share mode;                            update t set d=d+1 where c=10;              insert into t values(8,8,8);                            Error 40001 1213  Deadlock found when trying to get lock; try restarting transaction            sessionA 的第一条语句的加锁范围是 (5,10] + (10,15)；        毫无疑问sessionB 会被阻塞，但按理说 sessionB 此时应该还没有获取到任何锁，那么 sessionA 的第二条语句应该能执行才对，但是立马报了一个死锁错误？    实际上，next-key lock 的加锁分为两个阶段：先加间隙锁，再加行锁。    sessionB 需要申请 (5,10]  的next-key lock，会先加 (5,10) 的间隙锁，加锁成功；然后再加 c=10 的行锁，这时候才进入了锁等待。    然后 sessionA 第二条语句被 sessionB (5,10)  的间隙锁锁住()，而 sessionB 又在等待 sessionA 释放 c=10 的行锁，于是出现了死锁。  所以，在分析加锁规则的时候可以用 next-key lock 来分析，但是要知道具体执行的时候是先加间隙锁，再加行锁。"
} ,

{
"title"    : "MySQL学习笔记(七)：MySQL索引",
"category" : "",
"tags"     : "mysql, 索引",
"url"      : "/blog/2021/10/11/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%83)-MySQL%E7%B4%A2%E5%BC%95/",
"date"     : "2021-10-11 00:00:00 +0800",
"content"  : "索引的常见实现方式有哪些？      哈希表：O(1) 的时间复杂度，速度最快，但缺点是只适用于等值查询。因为key是无序的，所以区间查询时只能全部遍历一遍。            有序数组：O(logn)的时间复杂度，利用二分法。可用于等值查询和区间查询，但插入删除时间复杂度较高，因为需要移动插入点后面的所有元素。所以有序数组比较适合静态存储引擎，即基本不会变的数据。            搜索树：常用实现是B+树。  为什么采用B+树而不是常见的二叉树?二叉树即每个节点只有左右2个子节点，所以显而易见的问题就是当节点变多时树的高度会很高。比如需要存储100万条数据，就需要20层（n层二叉树的节点数为 2^n-1，20层二叉树的节点总数为1048576）。因为一个节点就是一个数据页，那么一次查询很可能就需要进行20次随机IO（大概率会触发随机IO），在传统机械硬盘时代，一次随机IO大约10ms，那么单一次查询可能就需要200ms，这个查询是很慢的。所以解决办法就是增加树的子节点数，由二叉变为N叉。B+树就是一颗N叉树。一个节点就是一页，页是Innodb磁盘IO的基本单位，一页在Innodb中默认是16k，假如索引字段为整数类型占4个字节，每个key还有一个指向下一层节点的指针固定占6字节，再加上一些辅助字节总共差不多占13字节左右（非叶子节点）。16k/13=1260，那么一个节点就可以差不多有1200个分叉，一颗树高为4的B+树，就可以存 1200^3≈17亿 个值。因为根节点总是在内存中，第二层大概率也在内存中，这时17亿数据量的单次查询理论上只需要进行2-3次磁盘IO，速度大大提高了。顺便说一下 Innodb 中B+树 (不是严格定义下的 B+树，做了一些优化)的特点：  N叉树，即每个节点可以有多个key，每个key对应一个分叉  内部节点不存储数据，只有指针，只在叶子节点存储数据  每一层页与页之间构成一个双向链表  页内 records 之间构成一个单向链表  叶子节点为第0层，从下往上递增，root为最大层数  一个节点就是一个pageInnodb中的页是什么？先从表空间说起。Innodb表空间  参考：The basics of InnoDB space file layoutInnodb的数据存储模型被称为 space，即“表空间”。表空间是一个逻辑概念，有一个32位的space ID，实际上可能由多个物理文件组成（如ibdata1、ibdata2）。表空间分为系统表空间（system space：ibdata1、ibdata2，space ID 为0）和表对应的表空间(per-table space：ibd文件)。ibd文件实际上是一个功能齐全的space，可以包含多张表，但在MySQL实现中一个ibd只对应一张表。每个space会被划分为多个page，一个page默认16k。page也有一个32位的page number（页号），表示在space内的偏移量（offset），比如page 0 对应 offset 为0，page 1 对应 offset 为16384。注意一个space可能包含多个文件，所以这个offset不一定是文件内的，而是整个space中的。Innodb单表空间最大为64TB，是因为 2^32 * 16k。表空间(space file)文件结构系统表空间(system space)文件结构系统表空间(system space)的 space ID 为 0 。它采用了一些固定页号的页来存储一些关键信息。结构如下：单表空间(per-table space file)文件结构Page3为聚簇索引(主键索引)的root page，Page4为第一个二级索引的root page，如果有多级索引的话以此类推。页的基本结构页包含一个38字节的头部（FIL为File的缩写）和一个8字节的尾部，中间的内容取决于不同的page type，可用大小为 16k-38-8=16338。FIL Header 和 Trailer 结构如下：可以看到，头部包含了  Offset(Page Number)  Page Type  Space ID  指向前一页和后一页的指针，构成一个双向链表（树的同一层中）  最后一次改动页的LSN(Log Sequence Number，日志逻辑序列号)  当前系统中（所有space）最大的LSNInnodb索引  详见 The physical structure of InnoDB index pages一切皆索引在Innodb中一切皆索引，意思是：      每张表都有一个主键。如果没有手动指定，会使用第一个 not null 的 unique key。如果仍然没有，会自动分配一个6字节的隐藏 Row ID作为主键。            主键索引树(聚簇索引)叶子节点key是主键值，value是是整行数据。            二级索引key是索引列的值，value是对应的主键值。            一张表有几个索引，就有几棵B+树。且至少有一棵主键B+树，数据存储在主键索引树上。查询不走索引其实是遍历主键索引树。            B+树中一个节点为一页。  索引结构因为一个索引就是一棵B+树，B+树中一个节点对应一页，所以索引页的基本结构和上面讲到的页的基本结构相同，都包含一个FIL Header和FIL Trailer，不同的是主体部分，如下图所示：重点关注其中的 User Records 和 Page Directory。User Records 是实际存储数据的地方：  非叶子节点：除key外，还存储指向下一层子节点的指针  叶子节点：假设为主键索引树，存储的就是整行数据一个page中的所有 User Records 组成了一个单链表，头是一个叫 infimum 的 system record(存储了当前页中最小的key)，尾是一个叫 supremum 的 system record(存储了当前页中最大的key)。Index Header 结构如下：可以看到有 Number of Records 、 Page Level(叶子节点所在层为第0层，从下往上递增，root节点所在层为最大层) 等等。联合主键假设存在以下表记录            a      b      c      d                  1      2      3      d              1      3      2      d              1      4      3      d              2      1      3      d              2      2      2      d              2      3      4             存在一个联合主键 (a,b)，三个索引 c(c)、ca(c,a)、cb(c,b)，分析这三个索引是否有冗余？索引 ca 即先对 c 排序，再对 a 排序，因为key已经包含了a，所以value只需要存储 b，ca相同时，b升序。记录如下：            c(key的部分)      a(key的部分)      b(value存的值)                  2      1      3              2      2      2              3      1      2              3      1      4              3      2      1              4      2      3      再看索引 c。key先对 c 排序，value 存的是联合主键 (a,b)，即先对 a 排序，再对 b 排序，结果和 ca 是一样的。所以索引ca 是多余的。索引 cb 先对 c 排序，再对 b 排序，再对 a 排序，可用于基于c、b 的查询，需要保留。实践一下可以通过 innodb_space 命令直接分析 ibd 文件，获取文件中存储的page、records等信息。(目前还不支持MySQL8.0)  详见：  innodb_ruby  B+Tree index structures in InnoDB基于主键索引和普通索引的查询有什么区别主键索引树叶子节点直接存储行数据，所以主键索引查询只需要扫描主键索引树即可。而普通索引树叶子节点存储的是主键值，所以需要先扫描普通索引树拿到主键值，再回到主键索引树获取行数据，相较于主键索引查询多扫描了一棵索引树，这个过程称为 回表。一些索引设计原则假设存在表 u(id,id_card,name,age,gender)，id 是主键，另有一个 id_card 索引覆盖索引即索引key中包含了要查找的字段。假如现需要根据身份证查询数据，可直接走 id_card 索引。现又有另一个高频需求，根据身份证查询姓名。目前走的仍然是 id_card 索引，需要先在 id_card 索引树上找到对应的 id_card，然后再回到主索引树上根据 id获取到姓名，也就是 回表。那有没有什么办法可以优化呢？方法就是建立一个 (id_card,name) 的联合索引。这棵索引树节点的key包含了两个字段：id_card 和 name，这样的话直接在 (id_card,name) 索引上搜索便可直接得到 name，而不需要回表查整行记录，减少了语句的执行时间。与此同时，根据最左匹配原则，原先根据身份证查询数据的请求也可以用到这个索引，所以现在可以删除id_card 这个索引了。这就是覆盖索引，即索引key中包含了要查找的字段。可使用 explain 查看是否使用了覆盖索引，如果在 explain 中的 extra 列中出现了 Using index，说明当前查询使用了覆盖索引，即不需要回表查询。当然，索引是有代价的。因为每新建一个索引就相当于新建一棵索引树，虽然可以提高查询速度，但增删改就需要多维护一棵索引树。所以需要权衡使用，任何索引都是这样，数据量小的话就没有什么必要，没有太大区别。最左匹配原则假设现在又有一个低频需求：根据身份证查询地址，那么有必要再建立一个 (id_card,address)的联合索引吗？答案是不需要。因为这是一个低频请求，意味着请求的次数不会太多，上一节的 (id_card,name)索引就够用了。可先通过(id_card,name) 这个索引定位到相应的 id_card，获取到主键后再回表查询。原理很简单，因为联合索引 (a,b)是先根据 a 排序再根据  b 排序，所以对于 a 的检索可以用到这个B+树。所以最左匹配原则的定义就是只要索引满足最左前缀，便可利用该索引来加速检索。这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左N个字符。如果既有a,b的联合查询，又有基于a、b各自的查询呢？这时考虑的原则就是空间了。如果b字段比a字段大，那么就应该建立一个 (b,a)的联合索引和一个 a 的单独索引。这两个索引可以同时满足 a,b的联合查询和基于a、b各自的查询。除此之外，最左匹配原则当遇到范围匹配时就会失效。比如有一个 (a,b)的联合索引， a=1 and b=2 或 b=1 and a=2 都可以用到索引，顺序无所谓，优化器会调整 where 条件 and 的顺序。但是当遇到类似 a gt;1 and b=2 时，就只有 a 能用到该索引，会先快速定位到 agt;1 的记录，此时 b 是无序的 (在 agt;1 的整个范围中 b 是无序的)，只能遍历判断 b 是否满足。  也有些特例，当满足最左匹配原则可以使用联合索引却需要回表时，优化器可能会认为扫描联合索引再加上回表的代价高于直接扫描主键索引树，这时可能(不一定)会选择不走联合索引，而是直接全表扫描。示例：      如果 sql 为    1SELECT * FROM table WHERE a = 1 and b = 2 and c = 3;         该怎么建立索引？    答：第一反应是直接建立 (a,b,c) 的联合索引，但是这里要注意区分度，区分度高的字段放在联合索引前面 (可从 show index from a 中观察  Cardinality 得知)。区分度越高，检索效率越高，因为能过滤掉的记录越多。像性别、状态这种区分度很低的字段，放到后面。            如果 sql 为    1SELECT * FROM table WHERE a gt; 1 and b = 2;         该怎么建立索引？    答：因为是范围查询，如果建立 (a,b) 的索引，就只有 a 能用上索引。    ​        所以应该建立 (b,a) 的索引，优化器会调整条件的顺序，然后b 就能用上索引，在此基础上，agt;1 也能用上。            如果 sql 为    1SELECT * FROM `table` WHERE a gt; 1 and b = 2 and c gt; 3;         该怎么建立索引？    答：首先肯定要以 b 开头，所以有两种选择：(b,a,c) 或 (b,c,a)，至于具体选择哪个，就看区分度和字段长度了。            如果 sql 为    1SELECT * FROM `table` WHERE a = 1 ORDER BY b;        该怎么建立索引？    答：建立 (a,b) 联合索引，当 a=1 时，b 相对有序，可以避免再次排序。    ​        那如果是    1SELECT * FROM `table` WHERE a gt; 1 ORDER BY b;        ​        因为此时是范围查询， agt;1 时 b 是无序的，可以建立一个 (a,b) 的联合索引，但只有 a 能用上这个索引。            如果 sql 为    SELECT * FROM `table` WHERE a IN (1,2,3) and b gt; 1;     该怎么建立索引？    答：还是建立  (a,b) 的联合索引，IN 查询 可视为等值查询，相当于 a=1 or a=2 or a=3，所以还是一样的思路。  索引下推严格说索引下推并不是一个索引设计原则，它是一个索引查找的内部优化。前提：因为范围查询不能使用联合索引，只能使用最左前缀。以下面的 sql 举例：1select * from tuser where name like &#39;张&#39; and age=10 and is_male=1;假设以 张 开头的记录有 (张三，10)，(张三，10)，(张三，20)，(张六，30)，有一个联合索引 (name,age)。在 MySQL5.6 之前，存储引擎提供的接口对于这种情况只允许传入最左前缀一个参数，即只能传入 name  这个字段，所以需要回表4次用于判断 age 和 is_male 是否满足条件。在 MySQL5.6之后，接口可以传入包含最左前缀的整个联合索引，即name,age字段。这样的话可直接在 (name,age) 索引树上就对 age 进行判断，提前过滤掉不满足条件的记录，最后只需要回表2次。当 explain 的 extra 字段中显示 Using index condition 时则表示本次查询使用到了索引下推。一些不能走索引的反面示例以下示例用到的表结构如下：1234567891011121314151617181920212223242526272829303132333435CREATE TABLE `tradelog` (  `id` int(11) NOT NULL,  `tradeid` varchar(32) DEFAULT NULL,  `operator` int(11) DEFAULT NULL,  `t_modified` datetime DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `tradeid` (`tradeid`),  KEY `t_modified` (`t_modified`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;    CREATE TABLE `trade_detail` (  `id` int(11) NOT NULL,  `tradeid` varchar(32) DEFAULT NULL,  `trade_step` int(11) DEFAULT NULL, /*操作步骤*/  `step_info` varchar(32) DEFAULT NULL, /*步骤信息*/  PRIMARY KEY (`id`),  KEY `tradeid` (`tradeid`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into tradelog values(1, &#39;aaaaaaaa&#39;, 1000, now());insert into tradelog values(2, &#39;aaaaaaab&#39;, 1000, now());insert into tradelog values(3, &#39;aaaaaaac&#39;, 1000, now());insert into trade_detail values(1, &#39;aaaaaaaa&#39;, 1, &#39;add&#39;);insert into trade_detail values(2, &#39;aaaaaaaa&#39;, 2, &#39;update&#39;);insert into trade_detail values(3, &#39;aaaaaaaa&#39;, 3, &#39;commit&#39;);insert into trade_detail values(4, &#39;aaaaaaab&#39;, 1, &#39;add&#39;);insert into trade_detail values(5, &#39;aaaaaaab&#39;, 2, &#39;update&#39;);insert into trade_detail values(6, &#39;aaaaaaab&#39;, 3, &#39;update again&#39;);insert into trade_detail values(7, &#39;aaaaaaab&#39;, 4, &#39;commit&#39;);insert into trade_detail values(8, &#39;aaaaaaac&#39;, 1, &#39;add&#39;);insert into trade_detail values(9, &#39;aaaaaaac&#39;, 2, &#39;update&#39;);insert into trade_detail values(10, &#39;aaaaaaac&#39;, 3, &#39;update again&#39;);insert into trade_detail values(11, &#39;aaaaaaac&#39;, 4, &#39;commit&#39;);      索引字段上用了函数    会破坏索引的有序性，因此优化器会决定放弃走树搜索功能    如：    1select count(*) from tradelog where month(t_modified)=7;        t_modified 上虽然有索引，但由于用了 month函数，破坏了索引的有序性，导致没办法快速定位。    注意，只是不使用树搜索功能，并不是放弃使用这个索引。比如这个例子，虽然放弃了树搜索快速定位，但是对比主键索引树和 t_modified索引树后，发现后者更小，优化器最终还是会选择遍历 t_modified索引树，也即是全索引扫描。    explain如下：        key 为 t_modified，说明用到了 t_modified 索引。rows 为 100335(测试数据有十万条)，说明是全索引扫描。    这个例子要使用快速定位的话，就得把索引上的函数去了:    1234select count(*) from tradelog where    -gt; (t_modified gt;= &#39;2016-7-1&#39; and t_modifiedlt;&#39;2016-8-1&#39;) or    -gt; (t_modified gt;= &#39;2017-7-1&#39; and t_modifiedlt;&#39;2017-8-1&#39;) or     -gt; (t_modified gt;= &#39;2018-7-1&#39; and t_modifiedlt;&#39;2018-8-1&#39;);        还有一些例子，我们可能会理所应当的以为优化器会优化，但是并没有，它还是一视同仁：    1select * from tradelog where id + 1 = 10000;        虽然 id+1 并不会改变索引的有序性，但优化器并不会重写这类语句，一视同仁，必须得改成 where id = 10000-1 才行。            隐式类型转换    比如这条sql：    1select * from tradelog where tradeid=110717;        explain 显示走的是全表扫描。原因是因为tradeid 是 varchar 类型，值是 int 类型，明显会需要一个类型转换。    在MySQL中，当字符串和数字做比较的时候，是由字符串转为数字。如果记不住这个规则，可用 select &#39;10&#39; gt; 9 验证一下：结果为1，表示字符串转为了数字。    所以，上面的sql其实相当于：    1select * from tradelog where  CAST(tradid AS signed int) = 110717;        那原因就很明显了，同样是因为索引列上用到了函数，导致不能快速定位。            隐式字符编码转换    比如这句sql：    1select d.* from tradelog l, trade_detail d where d.tradeid=l.tradeid and l.id=2;            表 l 只扫描了一行，表示用了主键索引快速定位，快速定位到了 id=2 这一行。    表 d 走了全表扫描，且没用上 tradeid 索引。key=NUll 是表示走的主键索引遍历。    简单拆解一下这句sql的执行步骤：          从表 l 中找到 id=2 这一行数据，从中取出 tradeid；      从表 d 中找到 tradeid=上一步查询到的值的数据。        所以第2步就是：    1select * from trade_detail where tradeid = 上一步查询到的tradeid;         那为什么用不上 tradeid 索引呢？    细心点可发现两个表的字符编码不同，表l 是 utf8mb4，表d 是 utf8。utf8mb4是utf8 的超集，类型转换的时候都是子集转超集，所以上面的sql相当于：    1select * from trade_detail  where CONVERT(traideid USING utf8mb4) = 上一步查询到的tradeid;         所以，原因还是一样，索引字段上加了函数操作导致不能快速定位。    作为对比验证，下面这句sql就能都用到索引快速定位：    1select l.operator from tradelog l , trade_detail d where d.tradeid=l.tradeid and d.id=4;        还是和上面一样的分析步骤，只不过这次的连接顺序倒了过来。先找到表 d 中 id=4 这一行，从中取出 tradeid，再到表 l 中去匹配，第2步的sql就相当于：    1select operator from tradelog  where traideid = 上一步查询到的tradeid;         由于需要做字符编码转换，记住转换是子集转超集，所以又相当于：    1select operator from tradelog  where traideid =CONVERT(上一步查询到的tradeid);         区别就在于函数操作是加在值上面，所以可以先计算出来，然后索引就能快速定位到。    这类问题解决办法有两个：                  最简单直接的就是把 utf8 编码的字段改为 utf8mb4，这样从根上避免了字符编码转换：        1alter table trade_detail modify tradeid varchar(32) CHARACTER SET utf8mb4 default null;                            如果不能更改字符编码，那只能手动改下sql，手动来做这个编码转换，如上面的sql可改为：        1select d.* from tradelog l , trade_detail d where d.tradeid=CONVERT(l.tradeid USING utf8) and l.id=2;                 这里手动把 l.tradeid 转为了 utf8，保证了编码一致，避免了编码转换。        注意：手动转换要确保不会丢失精度才行。                          字符串截断    比如有这么一张表：    123456CREATE TABLE `t` (  `id` int(11) NOT NULL,  `b` varchar(10) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `b` (`b`)) ENGINE=InnoDB;        假设现在表里面，有 100 万行数据，其中有 10 万行数据的 b 的值是 ‘1234567890’，然后查询是这样写的：    1select * from table_a where b=&#39;1234567890abcd&#39;;        细心一点会发现，传进去的值超过了字段 b 定义的长度，理想情况下根本就不用去查询，直接返回空就行了。事实上这句sql执行了很长时间才会返回空，它的步骤是这样的：          由于超过了字段 b 定义的长度，会首先做字符串截断，最终传到引擎层的值变成了 ‘1234567890’；      匹配到10万条数据，然后由于是 select *，还需要回表10万次；      每次回表后查出整行，到 Server 层一判断，不等于 ‘1234567890abcd’，最后返回空。        对于这种情况，最好就是先在应用层判断过滤一下，MySQL并不总是那么智能。  选择自增主键还是业务主键可从存储空间大小和性能两个方面来考虑：  存储空间：业务主键相较于自增主键都较长，由于二级索引树叶子节点存储的是主键值，所以采用业务主键的二级索引相较于自增主键会占用更多的空间。  性能：由于自增主键是有序的，所以在维护索引树时直接追加即可(叶子节点所在层即第0层的最后一个节点中的最后一个record后)，当一页写满会自动开辟一个新的页。而业务主键很难保证有序性，维护索引时很可能会在中间插入，就很有可能引起节点分裂(甚至是父索引节点的分裂)，自然性能会受到影响。所以，在大多数情况下，都应优先使用自增主键。当然事无绝对，如果只有一个索引，且该业务字段是唯一的，可以将该字段设为主键。因为不存在其他索引，就不用考虑其他索引的叶子节点大小问题。当然，性能上相较于自增主键还是会有一点影响。选择唯一索引还是普通索引？从读和写两方面来分析。      读：二者区别就在于是否唯一。唯一索引找到记录后即可返回，普通索引还需继续向后遍历检查是否满足条件。但此时数据页已在内存中，而且很大概率都是页内遍历(通过二分法)，这点差异对于现在的CPU来说可以忽略不计。所以可认为二者在查询方面差异不大。        写：需要考虑目标数据页是否在内存中，下面以 insert 举例说明          目标数据页在内存中：对于唯一索引来说，找到插入位置，判断到没有冲突，插入，语句执行结束；对于普通索引来说，找到插入位置，插入，语句执行结束。这种情况下直接更新内存即可，性能也没有多大差异。      目标数据页不在内存中：                  由于唯一索引需要判断唯一性，所以必须要将数据页从磁盘读到内存。          而普通索引没有这个要求，所以可直接在内存中记录下一条 “insert” 操作，语句就执行结束了。记录这个操作的区域叫 change buffer， 由于只需要写内存，避免了磁盘的随机读(磁盘的随机IO是数据库中成本最高的操作之一)，这种情况下普通索引性能就远远优于唯一索引，尤其如果是机械硬盘的话。                    综上所述，如果业务可以接受的话，从性能角度出发，应该选择普通索引。change bufferchange buffer 是 buffer pool 的一部分，默认值是 25()，最大值可设为 50()。1show global variables like &#39;innodb_change_buffer_max_size&#39;;在MySQL5.5之前的版本，只支持缓存insert操作，所以最初叫 insert buffer(很多地方见到的 ibuf 指的就是它，后来也一直延用了下来)。后来也加入了对 update、delete 的支持，便改名为了 change buffer。作用当目标数据页不在内存(buffer pool)中时，普通索引更新类操作的提速器。注意：只能作用于普通索引，不能作用于唯一索引。怎么保证数据被正确更新？上面说到，普通索引的更新写到 change buffer 中就结束了，那后续的查询是怎样的？还是对应到上面的2种情况：目标数据页在不在内存中。      如果目标数据页在内存中，意味着更新操作是直接更新的内存。那此时内存中的数据页一定是更新后的数据，虽然磁盘上还是老的数据，所以直接从内存返回即可；        如果目标数据页不在内存中，需要先把数据页从磁盘读入内存，然后应用 change buffer 中的操作日志，生成一个正确的版本后返回，这个过程称为 merge。          merge 的时机：              查询时。这时会把目标数据页从磁盘读到内存中；        作为后台任务定期运行。innodb_io_capacity 和 innodb_io_capacity_max 用于设置 Innodb 后台任务(刷脏页、merge)的 IOPS，可调整该数值来控制 merge 的频率；        在崩溃恢复期间，会从系统表空间(ibdata1)中读取 change buffer，然后当把数据页从磁盘读到内存中时，会进行 merge；        重启后；        slow shutdown 时。可通过 --innodb-fast-shutdown=0 开启 slow shutdown。            怎么保证更新不丢失？如果写完 change buffer 后断电了或意外宕机了，重启后 change buffer 和数据会丢失吗？不会。change buffer 也会被记到 redo log 中(redo log 中包含了数据页的变更和change buffer的变更)，回想之前讲过的两阶段提交协议，redo log 和 binglog  落盘才代表事务成功提交。所以，如果一个事务已提交，则代表 change buffer 已经写到 redo log 中，且 redo log 已落盘，崩溃恢复时会根据 redo log 来恢复 change buffer。适用场景change buffer 简单来说就是把对普通索引的更新缓存了下来，然后在适当的时候进行 merge。所以在 merge 之前， change buffer 中记录的变更越多，收益就越大。所以对于写多读少类业务，数据页在写完之后马上被访问到的概率很小，也就是说不会马上进行 merge，这种情况下 change buffer 的效果最好。比如账单类、日志类等。相反，如果是写后马上进行查询的业务，由于马上要访问数据页，会立即触发 merge。这种情况不仅不会减少随机IO的次数，反而会增加维护 change buffer 的代价，起到了副作用，这种情况下可以关闭 change buffer ：1show global variables like &#39;innodb_change_buffering&#39;;默认值为 all ，设为 none 即可关闭 change buffer。官方文档InnoDB Change Buffer给长字符串字段创建索引的几种方法出发点是尽量减小索引长度。      直接创建完整索引，占空间最多        创建前缀索引    即可以只定义字符串的一部分作为索引    1alter table user add index index_email(email(6));        优点是：节省空间    缺点是：                  可能会增加额外的扫描次数        ​    比如执行这样一句查询：        1select id,name,email from user where email=&#39;zhangssxyzxxx.com&#39;;                ​    对于完整索引，在 email 索引树上定位到 zhangssxyzxxx.com，然后回表取出对于记录即可，只需扫描一行；        ​    对于前缀索引，在 email 前缀索引树上定位到 zhangs，回表判断 email 是否等于 zhangssxyzxxx.com，是的话将记录加入结果集，继续在 email 前缀索引树上遍历下一条记录，再回表判断，重复此过程，直到遍历的下一条记录不等于 zhangs。所以前缀索引可能会增加记录的扫描行数。                如何优化？        关键在于增加前缀的区分度。区分度越高，过滤掉的记录就越多，需要回表的次数就越少。        先统计索引上有多少个不同的值：        1select count(distinct email) as L from user;                再依次选取不同长度的前缀来对比区分度：        123456select   count(distinct left(email,4)）as L4,  count(distinct left(email,5)）as L5,  count(distinct left(email,6)）as L6,  count(distinct left(email,7)）as L7,from user;                数值越大表示对应长度的前缀区分度越高，效果越好。                            不能使用覆盖索引        比如执行这样一句查询：        1select id,email from user where email=&#39;zhangssxyzxxx.com&#39;;                因为只需要查询 id,email，对于完整索引来说，使用覆盖索引即可，不需要再回表；        而对于前缀索引，则必须要回表判断 email 的值，即便前缀索引包含了全部字段(email(18)，假设email 有18个字符)也是如此。                      倒序存储    对于前缀区分度不够好的情况，可以考虑使用倒序存储。    比如身份证，同一个区域内的身份证前面几位都是相同的，如果按照上面的方法建立前缀索引，这个前缀的长度可能会比较长。    这时可把身份证倒过来存，因为身份证的尾部都是不同的，区分度足够高，查的时候转换一下：    1select field_list from t where id_card = reverse(&#39;input_id_card_string&#39;);        这时为身份证建立前缀索引需要的长度就会短很多，具体多长可通过上面的方法来确定。    缺点：不支持范围查询，因为不是有序的，没办法按顺序遍历。            新增一个 hash 字段    专门新增一个 hash 字段用来做索引。          相较于倒序存储查询性能相对稳定一些，因为倒序存储毕竟还是前缀索引，或多或少还是会增加扫描行数。而crc32(或其他哈希算法)冲突的概率总体还是非常小的，可认为每次查询的平均扫描行数接近1。        1alter table t add id_card_crc int unsigned, add index(id_card_crc);        每次插入新纪录的时候，都用 crc32() 计算出一个哈希值填到这个新字段中。    查询的时候计算一下，同时因为 crc32 会有冲突(虽然概率也非常小)，所以还需要在 where 中校验一下原值：    1select field_list from t where id_card_crc=crc32(&#39;input_id_card_string&#39;) and id_card=&#39;input_id_card_string&#39;        这时索引的长度就只有4个字节(crc32的长度)，相较于身份证长度大大减少了。    缺点：和倒序存储一样，不支持范围查询，因为哈希字段对应的原值完全是无序的，没办法在哈希索引上按顺序遍历。  "
} ,

{
"title"    : "MySQL学习笔记(六)：MySQL事务",
"category" : "",
"tags"     : "mysql, 事务",
"url"      : "/blog/2021/09/19/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%85%AD)-MySQL%E4%BA%8B%E5%8A%A1/",
"date"     : "2021-09-19 00:00:00 +0800",
"content"  : "解决的问题      脏读：读到还没有提交事务的数据        不可重复读：前后读取的记录内容不一致        幻读：同一事务前后两次查询同一范围数据时，后一次查询看到了前一次查询没有看到的行          主要针对 insert 操作， 使用间隙锁解决，详见 MySQL学习笔记(八)-MySQL锁之 next-key lock      隔离级别  读未提交  读提交：可解决脏读问题  可重复读：可解决不可重复读问题  串行化：事务都是串行执行，安全度最高，并发度最低如何查询隔离级别？1show global variables like &#39;isolation&#39;;默认级别为可重复读。如何更改隔离级别？1set global(session) transaction isolation level READ UNCOMMITTED ;如何实现？MVCC（Multiple Version Concurrency Control）多版本并发控制简而言之，就是一行记录在数据库中存在多个版本，如下图所示：V1、V2、V3 并不是物理真实存在的，真实存在的是U1、U2、U3，也就是undo日志。当需要上一个版本的数据时，会通过当前记录和undo日志推算出来。其实每行记录都会有一个我们看不到的隐藏字段trx_id。一致性视图（一致性读）有两种开启事务的方式：  begin/start transaction：并不是一个事务的真正起点，到执行第一个语句的时候才会创建一致性视图  start transaction with consistent snapshot：马上开启事务并创建一致性视图事务开启时，会创建一个一致性视图。不必纠结于视图两个字(其实就是个数组)，可简单理解为这样一个操作：记录下真正开启事务的那一刻(创建一致性视图的那一刻)，后面会以这个时刻为准来判断数据的哪个版本对于当前事务可见。具体实现上，会在开启事务的那一刻生成一个数组，保存了当前系统正在活跃（启动了还没提交）的事务ID，定义一个低水位：数组里的最小事务ID，高水位：当前系统已经创建过的最大的事务ID+1，这个数组和高低水位即构成了一致性视图。后续的操作应该可以猜到，就是拿记录的事务ID和这个高低水位比较来判断是否可见。这是具体的代码逻辑，不方便记忆，可简化为下面的可见性规则(RR级别下)：  本事务的更新总是可见  版本未提交，不可见  版本已提交，但是在一致性视图创建之后提交的，不可见  版本已提交，而且是在一致性视图创建之前提交的，可见以上就是事务具体是如何实现的不同隔离级别下的一致性视图生成时机  读未提交：直接读取记录的最新版本，没有视图的概念  读提交：每个语句会生成新的一致性视图  可重复读：在事务开启的时候生成一致性视图，在后续的整个事务期间都使用该视图  串行化：直接用加锁的方式来避免并行访问当前读事务使用的是一致性读，这里针对的是查询类操作，而更新类操作使用的是当前读，即读取记录的最新版本，这个区别这很重要。示例一千言万语，不如动手验证一下假设存在表 t ( id, k)  values (1,1) ( 2, 2)，事务级别为默认级别，即可重复读。            事务A      事务B      事务C                  start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                                   update t set k=k+1 where id=1;                     update t set k=k+1 where id=1;                            select k from t where id=1;                     select k from t where id=1;                            commit;                                   commit;             事务C没有显示开启事务，所以更新完成以后自动提交了，此时k的值是2。先分析事务A，事务A最先开启，根据可重复读的一致性视图生成时机，在开启的那一刻创建了一致性视图，在此后的整个事务期间都使用该视图。又根据事务可见性规则，事务C的版本虽已提交，但是在视图创建之后提交的，不可见。事务B先不管值是多少，版本尚未提交，仍不可见。所以事务A的查询结果仍是1。事务B首先执行了一次更新操作，这里的重点就是：此时读取的是哪个版本？结论是最新版本，即当前读。因为要保证更新不能丢失。所以读到的值是2，再+1=3。然后再进行一次查询操作，根据事务的可见性规则，本事务的更新总是可见。所以，事务B的查询结果是3。如果换成读提交隔离级别，结果是怎样的？根据读提交的一致性视图生成时机：在每一次执行语句的时候都生成新的视图，所以事务A的视图是在执行到 select 的时候生成。再根据事务的可见性规则，事务C的更新已提交，且是在视图生成之前提交，可见。事务B的更新尚未提交，不可见。所以事务A的查询结果是2。事务B的查询结果还是3，分析过程和可重复读隔离级别下一样。将上面的操作步骤稍微改一下，事务C稍后再提交，隔离级别仍是可重复读，看下结果会怎样            事务A      事务B      事务C                  start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                                   start transaction with consistent snapshot;                            update t set k=k+1 where id=1;                     update t set k=k+1 where id=1;                            select k from t where id=1;                     select k from t where id=1;             commit;              commit;                                   commit;             事务A查询结果：事务B和事务C的更新都尚未提交，不可见，结果是1。事务B查询结果：重点来了，由于事务C更新后没有马上提交，所以id=1的写锁一直未释放。而事务B是当前读，必须要读最新版本，若读到的记录最终被回滚了，就产生了脏读，这是不可接受的，所以必须确保当前读读到的数据是最终会被提交的数据。具体实现就是加锁，来确保当前读的数据是最新的且已提交的(从事务的角度来解释行锁)。所以事务B的更新，要等待事务C提交后才能继续。示例二用事务的可见性来验证一个行为：更新的值与原来的值相同的情况下，MySQL还会执行更新吗？还是看到值相同直接返回了？假设存在表 t(id,a)，插入一条数据 (1,2)。      先验证是否还会加写锁(因为行锁是Innodb特有的，如果加写锁，说明还是会调用Innodb引擎的update接口)                            sessionA          sessionB                                      begin;                                     update t set a=2 where id = 1;                                                update t set a=2 where id = 1; (blocked)                      sessionB的更新被阻塞，说明还是会加写锁。        再验证Innodb引擎是否会执行更新    注：此时binlog_format 为 statment，binlog_format 会影响结果，后面会详细分析                            sessionA          sessionB                                      begin;                                     select * from t where id=1; 返回(1,2)                                                update t set a=3 where id=1;                          update t set a=3 where id=1;                                                                           select * from t where id=1; 返回(1,3)                                 sessionA 的 update 是当前读，它肯定读到了 sessionB 更新后的最新版本，现在的问题是 sessionA 的这个 update 会不会执行更新？    根据事务的可见性规则，sessionA的第二个select 是看不到 sessionB 的更新的，但是它读取到的值却是(1,3)，所以这个版本只能是本事务自己更新的，也就证明了sessionA中的 update 是执行了更新操作的。    到这里可能会觉得这不是多此一举吗？判断一下值是否相等，相等的话就不用更新了。    事实上，是执行了判断的，只是这个判断没有办法判断出是否要执行更新。因为和具体的 sql 有关，在这句sql里：set a=3，这个赋值操作和 a 原本的值没有任何关系，所以虽然 update 是当前读，但因为和 a 没有关系，就不会读出 a 的值。既然没有 a 的值，就没有办法判断是否和原值相等，所以只能老老实实执行一遍更新操作。    可将sessionA的 update where 条件加上 a=3，此时因为读出了 a的值，Innodb判断到值是相同的，就不会执行更新操作。因此第二个 select 还是读取的一致性视图，返回 (1,2)：                            sessionA          sessionB                                      begin;                                     select * from t where id=1; 返回(1,2)                                                update t set a=3 where id=1;                          update t set a=3 where id=1 and a=3;                                                                           select * from t where id=1; 返回(1,2)                                 这个实验也可以得出一个结论：update  虽然都是当前读，但不一定都会执行更新操作。    上面说过，binlog_format 会影响结果，将 binlog_format 改为 row 后，会发现 sessionA 的第二个 select  读取到的都是 (1,2)。这是因为 row 格式的 binlog 记录的是行数据的变更，需要拿到所有字段，拿到所有字段后就可以进行判断，发现值相同便不执行更新操作，所以 sessionA 的第二个 select  读取到的还是一致性视图中的 (1,2)。  长事务尽量避免长事务，长事务有以下风险：      会产生大量undo日志，会占用大量存储空间。在MySQL 5.5 及以前版本中，回滚日志是和数据字典一起放在ibdata文件中的，即使长事务最终被提交，回滚日志被清理，文件也不会变小。            还会占用锁，可能拖垮整个库。在 RR 级别下，所有锁资源要在事务提交后才释放。            由于undo日志太多，导致需要快照读的查询变得很慢    如这个例子：    12select * from t where id=1;select * from t where id=1 lock in share mode;        按理说，第二句还需要加锁，应该是第一句更快点，而结果却是(慢查询日志)：            第一句查询需要800ms，第二句只需要0.2ms，为什么有这么大的差距？    其实它们的执行序列是这样的：                            sessionA          sessionB                                      start transaction with consistent snapshot;                                                #执行100万次update t set c=c+1 where id=1;                          select * from id=1;                                     select * from id=1 lock in share mode;                                 所以很明显，sessionB  产生了大量的回滚日志，而select * from id=1 lock in share mode 是当前读 ，读取的是最新版本，所以很快。而 select * from id=1 是快照读，需要从当前版本应用100万次回滚日志得到最初的版本，因此自然慢多了。            甚至可能会导致优化器选错索引。    案例重现：    表 t 定义如下：    12345678CREATE TABLE `t` (  `id` int(11) NOT NULL AUTO_INCREMENT,  `a` int(11) DEFAULT NULL,  `b` int(11) DEFAULT NULL,  PRIMARY KEY (`id`),  KEY `a` (`a`),  KEY `b` (`b`)) ENGINE=InnoDB;        存储过程，插入100000条数据：    123456789101112delimiter ;;create procedure idata()begin  declare i int;  set i=1;  while(ilt;=100000)do    insert into t(a,b) values(i, i);    set i=i+1;  end while;end;;delimiter ;call idata();        插入数据后执行 select * from t where a between 10000 and 20000，查询走的是 a 索引，没有问题    1explain select * from t where a between 10000 and 20000            sessionA 模拟一个长事务，sessionB 先清空表，再调用存储过程插入数据                            sessionA          sessionB                                      start transaction with consistent snapshot;                                                delete from t;                                     call idata();                                     explain select * from t where a between 10000 and 20000;                          commit                                 再查看执行计划，会发现没有选择使用索引 a，走的是全表扫描        为什么？    选择索引是优化器的工作，它会综合考虑扫描行数、索引基数、是否使用了临时文件、是否排序等因素来选择它认为速度最快的索引。    可通过 show index from t 来查看索引基数 cardinality (近似值，并不精确)，结果如下：        索引基数越大，说明区分度越高，选择它的概率就越大。图中可以看出主键和 a 的差距并不大，a 反而是最高的，不选择索引 a 肯定还有其他的原因。    再来看一下扫描行数。查询计划显示走的是全表扫描，rows为105033行。强制使用索引 a 看下扫描行数是多少：    1explain select * from t force index(a) where a between 10000 and 20000;            rows为39940行。这个差距挺大的，那为什么优化器放着 39940行的 a 索引不用而要选择 105033行的全表扫描？    因为使用索引 a 需要回表，而走全表扫描是直接扫描主键索引树，不需要回表，优化器”认为“全表扫描更快。从结果上看，优化器明显选错了，所以优化器也不是百分百可靠的。        到这里又有一个疑问，为什么开启了一个长事务后，删除再插入数据就会导致扫描行数发生变化？    很明显，肯定和这个长事务有关。    先简单说一下undo log，分为两种：insert undo logs 和 update undo logs，对应 insert操作和update、delete操作(delete本质上也是update操作，delete时并不是直接物理删除，而是先做一个 deleted 标记)。    insert undo logs 只用于回滚操作，并且在事务提交后便可清除(事务提交即意味着插入已永久持久化了，不会再有回滚操作，也就不再需要 insert undo logs 了)。而 update undo logs 除了用于回滚操作之外，还用于MVCC，且当没有在该事务开启之前开启的其他事务在运行时，方可删除。    上面讲到 delete 时并不是直接物理删除，而是先标记。只有在对应的 update undo logs 被清除时才会进行真正的物理删除，这个过程也称为 purge。        所以在这个案例中，因为先开启了一个长事务 sessionA，为了保证事务的可重复读，在 sessionB 中进行的 delete 和 insert 操作所产生的 upodate undo logs 会一直保留直到长事务提交。也就意味着原先的10万行数据并没有被真正删除，而是保留在了索引树上。因此每一行都会有两个版本，即总共会有20万数据，从 ibd 文件大小也能看出，大小变为了原来的两倍。而优化器统计扫描行数时会将标记为删除的版本也统计在内，导致扫描行数增加。        那为什么只有索引 a 的扫描行数增加了，主键的扫描行数还是10万行？因为主键的扫描行数是直接按照表的行数来估算的，而表的行数优化器取的是 show table status like &#39;t&#39; 中 rows 的值。索引的统计则是通过对数据页采样统计估算出来的，所以会算上老版本的数据。    原因找到了，如何解决？          既然是由长事务引起的，提交或kill掉长事务后便可重新让查询走索引 a      简单粗暴，force index(a)  强制使用索引 a`      analyze table t 重新统计索引信息，优化器会更正扫描行数，然后选择使用索引 a 。当发现 explain 预估的 rows 和实际情况差距较大时，都可以采用这个方法来处理      如何监控长事务？下述语句用于监测持续时间超过60s的长事务1select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))gt;60如何避免长事务？应用开发端      确认是否使用了 set autocommit = 0 ，代表手动提交事物。可先开启 general_log，通过 show global variables like &#39;general_log&#39; 查看 general_log 相关设置。然后随便跑一个业务逻辑，在日志中检查是否有 set autocommit = 0 ；          The general query log is a general record of what mysqld is doing. The server writes information to this log when clients connect or disconnect, and it logs each SQL statement received from clients. The general query log can be very useful when you suspect an error in a client and want to know exactly what the client sent to mysqld.      mysql8 doc general_log                确认是否有不必要的只读事务。有些框架会不管什么语句都用 begin/commit 包起来。或者有些业务并没什么必要，也要把一堆 select 放到事务中。上面提到过，对当前读事务会加锁。所以对于单纯的 select 没有必要放到事务中，事务中主要放 update/insert/delete，除非是明确需要事务特性的查询，比如明确需要可重复读的查询，才需要放到事务中；            根据对业务的预估，通过 set max_execution_time 来设置单个语句的最长执行时间，来避免单个语句意外执行过长时间。          毫秒，0 表示未启用。      mysql 8 doc max_execution_time      数据库端      监控 information_schema.innodb_trx表，超过阈值就报警或者kill掉。        推荐使用 percona 的 pt-kill 工具，作用描述如下：          pt-kill captures queries from SHOW PROCESSLIST, filters them, and then either kills or prints them. This is also known as a “slow query sniper” in some circles. The idea is to watch for queries that might be consuming too many resources, and kill them.            在测试阶段可要求输出所有的 general_log，分析日志提前发现问题。        在MySQL5.6或更新版本，可将 innodb_undo_tablespaces 设置为 2 或更大的值，如果真的出现大事务导致回滚段过大，清理起来更方便。          innodb_undo_tablespaces ：              0：使用系统表空间，即 ibdata1        不为0：使用独立数量的undo表空间，默认为2，即存在两个独立的回滚表空间： undo_001、undo_002            "
} ,

{
"title"    : "MySQL学习笔记(五)：MySQL计算QPS和TPS",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/09/11/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%94)-MySQL%E8%AE%A1%E7%AE%97QPS%E5%92%8CTPS/",
"date"     : "2021-09-11 00:00:00 +0800",
"content"  : "QPS基于 Com_select  基于questions的会统计show命令，mysql设置环境变量的时候也会增加，不太准。  Com_select 用于统计 select 语句的执行次数，类似的还有 Com_delete、Com_update 等。  参考：Com_xxx12345#!/usr/bin/env bashOLD_QPS=`echo show global status where Variable_name=&#39;Com_select&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`sleep 1NEW_QPS=`echo show global status where Variable_name=&#39;Com_select&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`echo (NEW_QPS-OLD_QPS) / 1 | bc1 代表添加到shell的第一个参数值，2 代表第二个，以此类推。0 为shell文件名。获取当前时刻的 qps：1./qps.sh 1TPS12345678910#/usr/bin/env bashOLD_COM_INSERT=`echo show global status where Variable_name=&#39;Com_insert&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`OLD_COM_UPDATE=`echo show global status where Variable_name=&#39;Com_update&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`OLD_COM_DELETE=`echo show global status where Variable_name=&#39;Com_delete&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`sleep 1NEW_COM_INSERT=`echo show global status where Variable_name=&#39;Com_insert&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`NEW_COM_UPDATE=`echo show global status where Variable_name=&#39;Com_update&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`NEW_COM_DELETE=`echo show global status where Variable_name=&#39;Com_delete&#39;;|mysql -uroot -pxxx -N|awk &#39;print 2&#39;`echo ((NEW_COM_INSERT - OLD_COM_INSERT) + (NEW_COM_UPDATE - OLD_COM_UPDATE) + (NEW_COM_DELETE - OLD_COM_DELETE)) / 1 | bc"
} ,

{
"title"    : "MySQL学习笔记(四)：MySQL定时备份和恢复",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/09/05/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E5%9B%9B)-MySQL%E5%AE%9A%E6%97%B6%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8D/",
"date"     : "2021-09-05 00:00:00 +0800",
"content"  : "定时备份：使用 mysqldump + crontabmysqldump 用法      备份全部数据库的数据和结构    1mysqldump -uroot -p123456 -A gt; /data/mysqlDump/mydb.sql            备份全部数据库的结构（加 -d 参数）    1mysqldump -uroot -p123456 -A -d gt; /data/mysqlDump/mydb.sql            备份全部数据库的数据(加 -t 参数)    1mysqldump -uroot -p123456 -A -t gt; /data/mysqlDump/mydb.sql            备份单个数据库的数据和结构(数据库名mydb)    1mysqldump -uroot-p123456 mydb gt; /data/mysqlDump/mydb.sql            备份单个数据库的结构    1mysqldump -uroot -p123456 mydb -d gt; /data/mysqlDump/mydb.sql            备份单个数据库的数据    1mysqldump -uroot -p123456 mydb -t gt; /data/mysqlDump/mydb.sql            备份多个表的数据和结构（数据，结构的单独备份方法与上同）    1mysqldump -uroot -p123456 mydb t1 t2 gt; /data/mysqlDump/mydb.sql            一次备份多个数据库    1mysqldump -uroot -p123456 --databases db1 db2 gt; /data/mysqlDump/mydb.sql      crontab 用法crontab不同操作系统实现不同，语法是通用的crontab 表达式含义：1* * * * *  表示分钟  表示小时  表示一个月中的第几天  表示月份  表示一个星期中的第几天  * 表示分钟都要执行（以此类推）  a-b 表示从 a 到 b 这段时间内要执行（以此类推）  */n 表示每 n 分钟执行一次（以此类推）  a,b,c 表示在第 a、b、c 分钟执行（以此类推）定时任务脚本12345678910111213141516171819202122232425#!/bin/bashnumber=31backup_dir=/var/lib/mysqldd=`date +Y-m-d_H:M:S`tool=mysqldumpusername=rootpassword=xxxxdatabase_name=testif [ ! -d backup_dir ];then    mkdir -p backup_dir;fitool -uusername -ppassword database_name gt; backup_dir/database_name-dd.sqldelfiles=`ls -l -crt backup_dir/*.sql | awk &#39;print 9 &#39; | head -1`count=`ls -l -crt backup_dir/*.sql | awk &#39;print 9 &#39; | wc -l`if [ count -gt number ];then    rm delfiles;fi利用全量备份和binlog恢复数据      使用全量备份恢复临时库    1mysql -uroot -p database lt; dump.sql                flush logs 重开一个binlog，一是避免操作当前binlog文件防止发生意外情况，二是缩小范围排除干扰，在之前的binlog中定位操作范围    1flush logs                恢复    1mysqlbinlog --start-position 6276 --stop-position 6481  binlog.000011 | mysql -uroot -p      参考MySQL 定时备份数据库（全库备份）"
} ,

{
"title"    : "MySQL学习笔记(三)：redo log 和 binlog",
"category" : "",
"tags"     : "mysql, redo, binlog",
"url"      : "/blog/2021/08/28/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%89)-redo-log-%E5%92%8C-binlog/",
"date"     : "2021-08-28 00:00:00 +0800",
"content"  : "binlogMySQL Server层的日志，与存储引擎无关，存储的是逻辑日志，可理解为sql语句。主要作用：  如果是 Innodb 引擎，和 redo log 一起提供崩溃恢复的能力（crash-safe），保证了事务ACID中的持久性。  常被用于主备同步，或接入其他下游系统如 es 用于数据分析，即作为其他需要直接从数据库获取数据且有很高实时性要求的应用系统数据来源通道。如何查看？      ​    查看binlog 格式    1show global variables like &#39;binlog_format&#39;;            ​    查看当前使用的binlog文件    1show master status ;            ​    查看所有binlog文件    1show binary logs;            查看binlong文件内容    方式一：show 命令    1show binlog events in &#39;binlog.000008&#39;;        方式二： mysqlbinlog    1mysqlbinlog --base64-output=decode-rows --verbose binlog.000001            binlog 3种格式的区别    STATEMENT：记录具体执行语句，如 update t set name = &#39;newName&#39; where name = &#39;oldName&#39;    ROW：记录每一行记录的变更，一个 update 可能产生多条日志（每条记录都会产生一行日志）    MIXED：当满足某些条件时会自动从 STATEMENT切换到 ROW        选择哪种格式？                  STATEMENT ：直接记录sql，可能会在某些情况下导致通过日志恢复出来的数据不一致，比如 RC 和 RR 在锁问题上的区别、更新的值与原来的值相同的情况下，MySQL还会执行更新吗？、 delete 时走的索引不同会导致删除的记录不同、一些函数结果不同等等                    ROW： 直接记录每一行记录的变更，最安全，但是日志的量会很大，存储空间不紧张的话最好选择该格式。MySQL 8 默认格式就是 ROW                    MIXED： 需要非常熟悉切换的条件，以确保用日志恢复数据不会有问题                  切换当前连接 binlog 格式    1SET SESSION binlog_format = &#39;STATEMENT&#39;;            刷新binlog    1flush logs;      binlog的写入时机binlog会首先写到cache里，cache满了就会暂存到磁盘上，这个参数大小由 binlog_cache_size 控制，默认是32K。注意这个cache是单个线程拥有的，也就是说每个线程都有自己的binlog_cache，但是共用同一份binlog文件。一个事务的binlog是不能被拆开的，无论一个事务多大，也要确保一次性写入binlog文件。binlog会被首先write到操作系统的page cache中，然后才会被 fsync到磁盘中。一般可认为只有fsync才占磁盘的IOPS。write  和 fsync 的时机由参数 sync_binlog控制，默认值是1，总共有3种设置：      0：每次事务提交时都只write，不fsync。由于只保存到操作系统缓存，依赖操作系统来时不时的fsync，如果主机掉电或崩溃会丢失binlog        1：每次事务都 fsync        N(Ngt;1)：每次事务提交都write，但只有累计了 N 个事务以后才会 fsync，如果主机掉电或崩溃会丢失最近N个事务的binlog  参考：mysql8 doc sync_binlogredo logInnodb 特有的日志，存储的是物理日志，即数据页上的改动。数据页：Innodb 从磁盘读取数据的基本单位，即使只取一行记录，也会将改行所在的整个数据页读入内存。数据页的大小可通过以下参数查看，默认为16k1show global variables like &#39;innodb_page_size&#39;;主要作用：提供事务能力，保证 crash-saferedo log 格式采用循环写的方式，有2个游标：checkpoint、writepos。writepos 表示当前写的位置，checkpoint 表示要擦除的位置，都是边写边往后推移。这个图画的不太好，一开始 checkpoint 和 writepos 都处于同一位置，边写边往后推移。上图可理解为：两者一开始都处于 checkpoint 现在的位置，writepos 写了一圈又转到 checkpoint 后面了。 所以绿色的部分表示空闲的空间，黄色的表示已写的空间。查看 redo log 相关设置1show global variables like &#39;innodb_log&#39;;innodb_log_buffer_size：redo log 缓冲区大小，默认16Minnodb_log_file_size：redo log文件大小，默认48Minnodb_log_files_in_group：每组有多少个redo log 文件，默认2个：ib_logfile0、ib_logfile1redo log 一般设多大？redo log 太小的话，会导致很快被写满，然后不得不强行刷 redo log，发挥不出 WAL 的作用。所以，磁盘空间不是很紧张的话，设大点吧，比如设置为4个文件，每个文件1G。redo log 的写入时机？同样，redo log 也分为 write 和 fsync，由参数 innodb_flush_log_at_trx_commit控制，默认值为1：      0：只把 redo log 写到 redo log buffer 中，后台线程每隔1s会将 buffer 中的内容 write、fsync 到磁盘          如果 MySQL 崩溃，会丢失1s的事务数据            1：每次事务提交都 fsync 到磁盘          最安全，严格保证了事务的持久性，只要事务提交成功，就说明对应的日志已落盘            2：每次事务提交都只将redo log 写到 page cache 中，后台线程每隔1s会将 page cache 中的内容 fsync 到磁盘          如果主机掉电或异常重启，会丢失1s的事务数据      0 和 2 的区别在于，0是写到了 redo log buffer 中，属于 MySQL 管理的内存。2是写到了 page cache 中，属于操作系统的内存。可靠性 1 最高，性能 0和2 最好（2 可靠性要比 0 高，因为主机重启的概率一般比MySQL崩溃的概率低，二者选一的话，建议设置为2）。没提交的事务也会被 被动的 写入磁盘：  redo log buffer 占用空间达到 innodb_log_buffer_size 一半时，后台线程会主动写盘，注意由于没有提交事务，此时只是 write，没有 fsync  如果有并行事务，假如A执行了一半，已经写了一些redo log 到 buffer中，此时 B 提交，如果 innodb_flush_log_at_trx_commit=1，B要把buffer里日志全部fsync到磁盘，此时会带上A的内容一起fsync参考：mysql8 doc innodb_flush_log_at_trx_commit最终数据落盘的过程是？1、正常运行过程中，MySQL更新完内存后便可返回响应。内存中的数据页被称为脏页，最终数据落盘就是把脏页写盘，这个过程和redo log 毫无关系。当然，此时 redo log 中已经记录了相关数据页的改动。写盘的时候还会向前推进redo log 中 checkpoint 的位置。2、在崩溃恢复时或启动时（MySQL启动时会自动执行该过程），会先将 redo log 中记录的变更应用到内存数据页中，此时该数据页就变成了脏页，接下来的步骤就和第1种情况一样了。两阶段提交一个事务当写完内存和redo log、binlog以后就算提交成功了，并不需要写真正的数据文件。为了保证2个日志文件都写入成功，采用了两阶段提交的方法，如下图所示两阶段提交并不是MySQL特有的，它是一种通用的分布式处理策略。其本质说来也很直白，就是一个人的时候好办，人多的时候，为了保持大家步调一致，每个人准备好以后吼一声，都准备好了再继续下一步。这里有个细节，当  innodb_flush_log_at_trx_commit=1 时，在redo log的 prepare 阶段就会fsync一次。然后再写binlog，再将redo log 的标识设为 commit（但是这个阶段只会write 到page cache 中，不再需要fsync了，因为有后台定时线程：每1秒会fsync一次；和崩溃恢复逻辑：prepare的redo log 加上完整的binlog即可保证事务的持久性和一致性）两阶段提交如何保证 crash-safe？crash-safe具体提供了什么能力？  只要客户端收到事务成功的消息，事务就一定持久化了  只要客户端收到事务失败的消息，事务就一定失败了  如果客户端收到“执行异常”之类的消息，需要应用重连后查询当前状态来执行后续逻辑。数据库内部只要保证数据和日志、主库和备库之间一致就行了具体如何实现？在崩溃恢复时，检查 redo log 和 binlog 的状态，如果：1、redo log 状态为commit，直接提交事务2、redo log 状态为prepare，查看对于的binlog是否完整：​      a.  不完整，回滚​      b.  完整，提交事务如何知道binlog是否完整？完整的binlog最后会有个 XID EVENT 和  COMMIT此外，还提供了一个 binlog-checksum 参数用于校验binlog的完整性。redo log 是怎么和 binlog 对应起来的？它们都有一个共同的字段 XID为什么要引入两个日志，只用binlog 不就行了？很重要的一点：binlog不具备 crash-safe 的能力。1、binlog 中没有checkpoint，不能区分哪些是已经落盘到磁盘数据文件（即最终存储数据的文件），哪些是需要应用到内存中用于崩溃恢复的2、binlog中存储的是逻辑日志，即sql级别的语句。redo-log中存储的是物理日志，即数据页级别的改动。一个sql语句可能会更改到好几个数据页，如果单个数据页损坏，binlog是没有能力恢复单个数据页的，它只能应用整个sql语句。比如一个sql语句同时更改了ABC三个数据页，更新的时候发生了crash，B数据页没有正常更新，AC正常更新。如果使用binlog来恢复，它只能同时恢复ABC三个数据页，结果就是B恢复了，AC又不对了。简而言之，binlog粒度太大。为什么要写2个日志，直接更新数据文件不是更快吗？这种思路叫做 write ahead log，简称 WAL，是数据库的通用技术，主要基于以下两点：      顺序写快于随机写。写日志文件都是顺序写，而直接更新数据文件大多数都是随机写，在机械硬盘时代这个速度差异非常大。        虽然是写2个日志，看似意味着一次事务提交要经历两次刷盘，实际上利用了 组提交 的策略，fsync 的次数会大大减少，而write 到 操作系统page cache 基本上可以认为和写到内存差不多，可认为只有 fsync 会占用磁盘的 IOPS。  组提交写binlog在是实现上其实也分为两步：write 和 fsync。这里 MySQL 做了一个优化：拖时间。即将redo log 的 fsync 放到了 binlog的 write 之后、fsync之前。这样的话，由于redo log write 完之后没有立即 fsync，而是等了一步：等binlog write完，所以有机会可以积累更多的redo log 到 page cache中，然后再一并进行 fsync。如下图所示，在两阶段提交的图上进一步细化：与此同时，也给了binlog  组提交的机会，因为binlog fsync 之前要等待redo log fsync。但是由于通常情况下 redo log fsync会很快，所以binlog 组提交的效果不如 redo log 的明显。但是可以通过调整以下参数优化：binlog_group_commit_sync_delay：表示延迟多少微秒以后才调用 fsyncbinlog_group_commit_no_delay_count：表示积累多少次以后才调用 fsync两者是或的关系，当第一个参数为0时，表示不延迟，直接fsync，状态为true，则不再考虑第二个参数的设置了。组提交与 sync_binlog 不冲突，可一起使用。参考：mysql8 doc binlog_group_commit_sync_delay如何提高MySQL的IO性能？从提高写redo log 和 binlog 的性能入手。redo log：      增大redo log buffer 大小          尤其对于需要更新很多行的大事务，大的redo log buffer 意味着事务提交前不会由于buffer满了而需要先写到磁盘上            增大redo log file 大小和个数        将 innodb_flush_log_at_trx_commt 设为2，表示每次事务提交都只 write 到 page cache，由后台定时线程来fsync。这样做的风险是主机掉电重启后会丢失数据（正常关闭不影响，MySQL在正常关闭前会完成一系列收尾工作）  ​        参考：Optimizing InnoDB Redo Loggingbinlog：      将 sync_binlog 设为大于1的值（通常是100-1000），这样做的风险是主机掉电会丢失binlog日志        调整 binlog_group_commit_sync_delay 和 binlog_group_commit_no_delay_count，提高binlog组提交效率，减少写盘次数。这样做会增加语句的响应时间，但是不会有丢失数据的风险  增大 buffer pool 大小：buffer pool 是用来缓存表数据的内存，默认值是128M，增大这个值可以显著减少磁盘IO。如果是数据库专用服务器，可以把 buffer pool 设为物理内存的 80。当 buffer pool 大小超过1G，建议同时把 innodb_buffer_pool_instances 设为大于1的值。相当于把 buffer pool 划分为了多个区域，可提高并发度。参考：mysql8 doc innodb_buffer_pool_sizesql语句突然变慢了？有时会遇到这种情况，一条sql平时执行都很快，但有时候不知道为什么突然就变得很慢，而且这个情况还是随机的、持续时间也很短，很难复现。突然变慢的这一瞬间很有可能是MySQL在刷脏页。以下4种情况会引发刷脏页：  当需要读入新的内存页时，如果系统内存不足，就需要淘汰旧的数据页。如果淘汰的是脏页，就需要把脏页刷到磁盘。当一次淘汰的脏页太多时，就会明显影响性能。  redo log 写满了。此时需要把 checkpoint 到 write pos 之间的redo log对应的脏页刷到磁盘，并把 checkpoint 往前推进。这种情况是很严重的，此时MySQL将不再接受更新，所有更新都会堵住。  后台任务 purge 会在系统空闲时刷脏页。  MySQL正常关闭时。解决思路：主要解决前两种情况，后两种情况不会影响到系统性能。对于情况二，调整 redo log 的大小即可，默认值明显太小，上文有讲过。对于情况一，除了上面提到过的 buffer pool，还需要先了解下Innodb刷脏页的控制策略和相关参数。这里主要的参数是 innodb_io_capacity，从名字就可以看出是控制 IO 能力的，官方描述如下：  The innodb_io_capacity variable defines the number of I/O operations per second (IOPS) available to InnoDB background tasks, such as flushing pages from the buffer pool and merging data from the change buffer.简单说就是设置 IOPS，它用来告诉 Innodb 所在主机的 IO 能力，用于后台任务刷脏页和 change buffer 的 merge。默认值是200，即 Innodb 全力刷脏页可以达到 200 IOPS。这个值明显太低了，即便是对于机械硬盘。当然，Innodb 不会完全按照这个值去刷脏页，因为系统还需要处理服务请求。Innodb的做法是会算出一个百分比，然后按 innodb_io_capacity 定义的能力乘以这个百分比来控制刷脏页的速度。百分比的大致算法：      根据当前脏页比例再结合 innodb_max_dirty_pages_pct(脏页比例上限，默认值是90) 算出一个 0到100 之间的数字          脏页比例可通过下面命令得到：      123select VARIABLE_VALUE into a from global_status where VARIABLE_NAME = &#39;Innodb_buffer_pool_pages_dirty&#39;;select VARIABLE_VALUE into b from global_status where VARIABLE_NAME = &#39;Innodb_buffer_pool_pages_total&#39;;select a/b;            计算方法伪代码如下，M 为当前脏页比例：      123456F1(M)if Mgt;=innodb_max_dirty_pages_pct then   return 100;return 100*M/innodb_max_dirty_pages_pct;                  InnoDB 每次写入的日志都有一个序号(LSN)，当前写入的序号跟 checkpoint 对应的序号之间的差值，假设为 N。InnoDB 会根据这个 N 算出一个范围在 0 到 100 之间的数字，N 越大，算出来的值越大。        取2值较大者作为百分比  从这个过程可以看出，可以介入的部分就是两个参数：innodb_io_capacity 和 innodb_max_dirty_pages_pct。      innodb_io_capacity    建议设置为磁盘的IOPS，磁盘的IOPS可以通过下面的命令来测试，一般参考测试结果中 write 的能力来设置：    1fio -filename=filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest         注意：一定要熟悉 fio 的使用方法，否则可能会把盘都刷了！！！注意，注意，注意，危险的事情说三遍！！！    参考：fio 命令入门到跑路        innodb_max_dirty_pages_pct    从上面的伪代码可以看出，当脏页比例≥该参数值时，第一个参数为100；否则取脏页比例/该参数值的百分比。MySQL8.0以后该值默认为90，也就是说当脏页比例≥90时，会百分百按 innodb_io_capacity  的能力全力刷脏页。为了尽量避免因刷脏页引起的抖动，应经常关注脏页比例，不要让它经常接近90。或者可以调高一点(其实该值已经调整过了，之前是75，8.0后调为了90。90已经接近100了，可认为该值是一个很合理的值，如果脏页累计过多，刷脏页就会很频繁)。  除此之外，还有一个参数 innodb_flush_neighors，从参数名字可以看出大概意思：刷脏页的时候是只刷自己还是连着附近的脏页也一起刷了，1表示启用，0表示只刷自己。这个机制对于传统机械硬盘很有用，机械硬盘的IOPS一般只有几百，每次多刷一些可以减少很多随机IO。而SSD的随机IO已不是瓶颈，只刷自己反而会更快些。在MySQL8.0中，该参数的默认值已为0，表示只刷自己。"
} ,

{
"title"    : "MySQL学习笔记(二)：MySQL权限控制",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/08/23/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%BA%8C)-MySQL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6/",
"date"     : "2021-08-23 00:00:00 +0800",
"content"  : "MySQL权限可分为全局级、数据库级、表级、列级、子程序级（函数、存储过程）全局级1select * from mysql.user;数据库级1select * from mysql.db;表级1select * from mysql.tables_priv;列级1select * from mysql.columns_priv;子程序级1select * from mysql.procs_priv;查询用户拥有权限1show grants for rootlocalhost;示例1234567891011121314151617181920#创建用户create user &#39;test&#39;&#39;&#39; identified by  &#39;test&#39;;#授予全局级查询权限grant select on *.* to &#39;test&#39;&#39;&#39;;#授予数据库级查询权限grant select on test.* to &#39;test&#39;&#39;&#39;;#授予表级查询权限grant select on mysql.user to &#39;test&#39;&#39;&#39;;#授予列级权限（只能查询person表的name字段）grant select(name) on test.person to &#39;test&#39;&#39;&#39;;#撤销权限revoke select on *.* from &#39;test&#39;&#39;&#39;;#刷新权限flush privileges;"
} ,

{
"title"    : "MySQL学习笔记(一)：一条sql查询语句是如何执行的？",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2021/08/18/MySQL%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(%E4%B8%80)-%E4%B8%80%E6%9D%A1sql%E6%9F%A5%E8%AF%A2%E8%AF%AD%E5%8F%A5%E6%98%AF%E5%A6%82%E4%BD%95%E6%89%A7%E8%A1%8C%E7%9A%84/",
"date"     : "2021-08-18 00:00:00 +0800",
"content"  : "先放一张MySQL架构图，有助理解，一条sql查询语句经历了从上到下的以下过程。整体可分为两大块：Server层和存储引擎层。所有跨存储引擎层的功能都在Server层实现，如内置函数（日期、时间、数学、加密函数等）、存储过程、触发器、视图等。连接器作用：建立连接、检查权限、维持和管理连接建立连接时会去查询是否有相应权限，之后的所有权限判断逻辑都依赖于此时读到的权限。这也就意味着如果在建立连接后再对用户的权限做更改，只会在重新连接后才生效。连接建立后如果没有后续动作，该连接就处于空闲状态，可以在 show processlist 中看到它。Sleep表示当前系统中有一个空闲连接。如果长时间没有动作，连接就会断开。由参数wait_timeout控制，默认是8小时。长连接 vs 短连接长连接和短连接是一种行为，并不是由某个参数控制。如果一个连接建立后，一直有后续行为，它就会一直维持连接状态，这就是长连接。如果建立连接做出某些动作以后就断开，它就是短连接。长连接后续的所有操作使用的内存都是管理在该长连接的对象里，直到连接断开才会释放。所以有时候执行一些比较大的占用内存的操作后，长连接一直累积，可能会导致OOM，MySQL重启。短连接风暴数据库连接数突然暴涨，超过了最大连接数 max_connections(默认值151)，数据库会拒绝接下来的连接请求，并提示 Too many connections。这个值并不是设的越大越好，因为建立连接的成本是很高的，需要经过三次握手、各种权限的判断等。如果设的过大，会适得其反，将大量资源消耗在建立连接上，已经拿到连接的线程反而得不到 CPU 资源。和线程池是一个道理。可通过查看 Max_used_connections status变量，历史最高同时连接数，作为参考。[MySQL服务器最大连接数怎么设置才合理]除此之外，可以杀掉一些占着连接不工作的线程。可通过 show processlist 查看，杀掉 Command=Sleep 的线程；或者更详细点可以查看 information_schema.innodb_trx，来选择要 kill 掉的连接。  优先断开事务外空闲太久的连接，不行的话再考虑断开事务内空闲太久的连接。  Innodb_trx 有个字段 trx_rows_modified，表示此次事务更新的行数，为0的话可放心kill，否则会造成事务回滚。查询缓存MySQL 8 以后将整个缓存模块删除了，不再提供查询缓存功能。缓存顾名思义是用来提高查询速度的，但是由于缓存失效太频繁，只要对表有更新，会导致整个表的缓存失效，然后又要重新建立缓存，所以最后权衡下来把缓存模块拿掉了。但是对于静态表，读远远多于写的表，比如配置表，可以使用缓存。对于还提供查询缓存的版本，可以将 query_cache_type 设置为 DEMAND，即按需使用，在需要的时候加上 SQL_CAHCE 即可。1select SQL_CACHE * from test.person;分析器主要作用： 词法分析和语法分析。词法分析：分析每个单词代表什么含义，比如 select 代表查询，t 表示表名，id 表示列名。语法分析：分析语句是否符合sql语法规范，经常看见的 You have an error in your SQL syntax 就是在这个阶段。优化器主要作用：决定选择哪个索引或者join的连接顺序。不同的索引、不同的表连接顺序会对执行效率有很大影响。执行器主要作用：调用存储引擎提供的接口完成操作，如 取 id=1 的数据。存储引擎是以插件的形式接入MySQL的，可以把存储引擎看作一个黑盒，它对外提供了很多接口，只需要调用即可。mysql.slow_log  慢查询日志中有一个值： row_examined，表示 Server 层扫描的行数。注意：Server 层扫描行数和引擎内部扫描行数不一定相等。"
} ,

{
"title"    : "自己写一个starter",
"category" : "",
"tags"     : "springboot, starter",
"url"      : "/blog/2021/08/15/%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AAstarter/",
"date"     : "2021-08-15 00:00:00 +0800",
"content"  : "一个 starter 就是一个提供特定功能的库，它主要做了几件事：  添加了一些依赖  定义了一些属性（在application.properties 或 application.yml 中使用），可以理解为一些功能的开关，或者是初始值  根据某些条件动态的定义一些bean，这些 bean 就是提供功能的对象，给客户端使用目标：实现一个starter，提供 json 序列化功能，有两种序列化方式可供选择：fastjson 和 gson使用场景1： 由用户在属性文件中选择使用哪一种使用场景2： 由用户添加某个具体实现的依赖来选择使用哪一种使用场景1： 由用户在属性文件中选择使用哪一种      新建一个maven项目，命名为 json-spring-boot-starter（所有第三方库都命名为xxx-spring-boot-starter，spring 自己的 starter 命名为 spring-boot-starter-xxx）    pom 文件如下，重点是引入的依赖：spring-boot-autoconfigure、fastjson、gson    123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263lt;?xml version=1.0 encoding=UTF-8?gt;lt;project xmlns=http://maven.apache.org/POM/4.0.0 xmlns:xsi=http://www.w3.org/2001/XMLSchema-instance         xsi:schemaLocation=http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsdgt;    lt;modelVersiongt;4.0.0lt;/modelVersiongt;    lt;groupIdgt;com.examplelt;/groupIdgt;    lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;    lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;    lt;namegt;json-spring-boot-starterlt;/namegt;    lt;descriptiongt;Json Serializer Starterlt;/descriptiongt;       lt;propertiesgt;        lt;java.versiongt;11lt;/java.versiongt;    lt;/propertiesgt;       lt;dependencyManagementgt;        lt;dependenciesgt;            lt;dependencygt;                lt;groupIdgt;org.springframework.bootlt;/groupIdgt;                lt;artifactIdgt;spring-boot-dependencieslt;/artifactIdgt;                lt;versiongt;2.5.2lt;/versiongt;                lt;typegt;pomlt;/typegt;                lt;scopegt;importlt;/scopegt;            lt;/dependencygt;        lt;/dependenciesgt;    lt;/dependencyManagementgt;       lt;dependenciesgt;        lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-autoconfigurelt;/artifactIdgt;        lt;/dependencygt;           lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;        lt;/dependencygt;           lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-starter-testlt;/artifactIdgt;            lt;scopegt;testlt;/scopegt;        lt;/dependencygt;    lt;/dependenciesgt;       lt;buildgt;        lt;pluginsgt;            lt;plugingt;                lt;groupIdgt;org.apache.maven.pluginslt;/groupIdgt;                lt;artifactIdgt;maven-compiler-pluginlt;/artifactIdgt;                lt;versiongt;3.8.1lt;/versiongt;                lt;configurationgt;                    lt;sourcegt;11lt;/sourcegt;                    lt;targetgt;11lt;/targetgt;                lt;/configurationgt;            lt;/plugingt;        lt;/pluginsgt;    lt;/buildgt;lt;/projectgt;            定义一个 JsonSerializer 接口，提供两种实现    123public interface JsonSerializer     String serialize(Object obj);        123456public class FastjsonSerializer implements JsonSerializer     Override    public String serialize(Object obj)         return Fastjson:  + JSON.toJSONString(obj);            1234567public class GsonSerializer implements JsonSerializer     Override    public String serialize(Object obj)         Gson gson = new Gson();        return Gson:  + gson.toJson(obj);            ​        定义一个 JsonTemplate，这是最终用户要使用的工具类  1234567891011   public class JsonTemplate        private final JsonSerializer serializer;          public JsonTemplate(JsonSerializer serializer)            this.serializer = serializer;                 public String serialize(Object obj)            return this.serializer.serialize(obj);          ​  实现自动配置，这里根据属性来选择，属性名定义为 json.serializer.type，用到ConditionalOnProperty 注解。没有指定属性时使用 fastjson（matchIfMissing = true）1234567891011121314151617181920212223   Configuration   public class JsonSerializerAutoConfiguration        Bean       public JsonTemplate jsonTemplate(JsonSerializer serializer)            return new JsonTemplate(serializer);                 Bean       ConditionalOnProperty(               prefix = json.serializer,               name = type,               havingValue = fastjson,               matchIfMissing = true)       public JsonSerializer fastjsonSerializer()            return new FastjsonSerializer();                 Bean       ConditionalOnProperty(prefix = json.serializer, name = type, havingValue = gson)       public JsonSerializer gsonSerializer()            return new GsonSerializer();          ​  在 resources 下创建一个文件 META-INF/spring.factories12   org.springframework.boot.autoconfigure.EnableAutoConfiguration=#92;   com.example.json.JsonSerializerAutoConfigurationmvn install 到本地，接下来写客户端验证一下12345        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;12345678910111213141516171819SpringBootApplicationSlf4jpublic class StarterClientApplication implements CommandLineRunner     Autowired private JsonTemplate jsonTemplate;    public static void main(String[] args)         SpringApplication.run(StarterClientApplication.class, args);        Override    public void run(String... args) throws Exception         Person person = new Person();        person.setName(张三);        person.setAge(30);        log.info(jsonTemplate.serialize(person));    没有指定属性时，输出为：1Fastjson: age:30,name:张三指定属性为json时：123json:  serializer:    type: gson1Gson: name:张三,age:30为属性文件加上智能提示      为属性创建一个类，打上 ConfigurationProperties 注解    12345678910111213141516171819ConfigurationProperties(prefix = json.serializer)public class JsonSerializerProperties        /** 序列化类型：fastjson 和 gson */    private Type type;       public Type getType()         return type;           public void setType(Type type)         this.type = type;           public enum Type         FASTJSON,        GSON                改造自动配置类    12345678910111213ConfigurationEnableConfigurationProperties(JsonSerializerProperties.class)public class JsonSerializerAutoConfiguration     Bean    public JsonTemplate jsonTemplate(JsonSerializerProperties serializerProperties)         JsonSerializerProperties.Type type = serializerProperties.getType();        JsonSerializer serializer =                (type == null || JsonSerializerProperties.Type.FASTJSON.equals(type))                        ? new FastjsonSerializer()                        : new GsonSerializer();        return new JsonTemplate(serializer);                加入spring-boot-configuration-processor依赖，注意一定不要搞错了，之前我没注意写成 spring-boot-autoconfigure-processor，害我调半天    12345        lt;dependencygt;            lt;groupIdgt;org.springframework.bootlt;/groupIdgt;            lt;artifactIdgt;spring-boot-configuration-processorlt;/artifactIdgt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;        这个注解会在 META-INF 下生成一个元数据文件 spring-configuration-metadata.json    123456789101112131415161718  groups: [          name: json.serializer,      type: com.example.json.JsonSerializerProperties,      sourceType: com.example.json.JsonSerializerProperties      ],  properties: [          name: json.serializer.type,      type: com.example.json.JsonSerializerPropertiesType,      description: 序列化类型：fastjson 和 gson,      sourceType: com.example.json.JsonSerializerProperties      ],  hints: []        description 来自字段上的注释    12/** 序列化类型：fastjson 和 gson */    private Type type;        可以手动编辑该json文件，加上 defaultValue。  使用场景2：由用户添加某个具体实现的依赖来选择使用哪一种      改造自动配置类，用到 ConditionalOnClass注解。注意这里bean定义的顺序，JsonTemplate一定要放在最后，因为它依赖于JsonSerializer    12345678910111213141516171819202122ConfigurationEnableConfigurationProperties(JsonSerializerProperties.class)public class JsonSerializerAutoConfiguration        Bean    ConditionalOnClass(JSON.class)    Primary    public JsonSerializer fastjsonSerializer()         return new FastjsonSerializer();           Bean    ConditionalOnClass(Gson.class)    public JsonSerializer gsonSerializer()         return new GsonSerializer();           Bean    public JsonTemplate jsonTemplate(JsonSerializer serializer)         return new JsonTemplate(serializer);                将 fastjson 和 gson 依赖设置成可选项    1234567891011        lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;            lt;optionalgt;truelt;/optionalgt;        lt;/dependencygt;      测试一把，客户端先添加fastjson的依赖12345678910        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.alibabalt;/groupIdgt;            lt;artifactIdgt;fastjsonlt;/artifactIdgt;            lt;versiongt;1.2.76lt;/versiongt;        lt;/dependencygt;输出为：1Fastjson: age:30,name:张三换成gson123456789        lt;dependencygt;            lt;groupIdgt;com.examplelt;/groupIdgt;            lt;artifactIdgt;json-spring-boot-starterlt;/artifactIdgt;            lt;versiongt;0.0.1-SNAPSHOTlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;com.google.code.gsonlt;/groupIdgt;            lt;artifactIdgt;gsonlt;/artifactIdgt;        lt;/dependencygt;输出为：1Gson: name:张三,age:30如果两个依赖都没有添加，会报错提示：123Action:Consider defining a bean of type &#39;com.example.json.JsonSerializer&#39; in your configuration.这个信息也可以自定义成更友好的提示信息，这一part暂时不研究了，结束！"
} ,

{
"title"    : "正确理解浮点数",
"category" : "",
"tags"     : "浮点数, 数据类型",
"url"      : "/blog/2021/04/25/float/",
"date"     : "2021-04-25 00:00:00 +0800",
"content"  : "首先，浮点数是一种数字的表示方式，而不是指小数很多人一提到浮点数，就说是小数，这是不对的。浮点数只是一种数字的表示方式，数字是多少就是多少，它就在那里不会改变，改变的只是人类表示它们的方式。比如给定一个数 10，能确定它是浮点数还是定点数吗？不能！我们必须知道这个数是如何存储的，即底层是如何来表示这个数的，才能说它是定点数还是浮点数。这其实类似于十进制中的科学计数法，比如 1234.5678这个数，这个数本身不会改变，是多少永远是多少，变的是表示它的方式，我们可以以多种不同的方式来表示这个数：  1.2345678 #92;times 10^3  123.45678 #92;times 10  0.12345678 #92;times 10^4  12345678 #92;times 10^-4可以看到，小数点的位置是浮动的，这才是浮点数名字的由来。再次强调，浮点数指的是表示数字的方式，而不是数字本身。至于大家常提到的IEEE754，它是一个标准，即规定了大家统一采用哪种方式来表示和存储数据，它规定尾数部分格式为 1.xxxx，具体存储时1省略，其他细节不再赘述。为什么要使用浮点数简单来说，同样的位数，浮点数表示的范围更大比如 int 和 float 同是 32bitint的最大值为 2^31-1 = 2.147483647E9 = 2147483647float的最大值为 (2-2^23)#92;cdot2^127 = 0#92;mboxx1.fffffeP+127f = 3.4028235E38f         这表示范围可比 int 大多了为什么float的精度是6~7位？因为float的尾数部分有23位，即可以用23位二进制来表示一个数，指数部分不影响精度，只影响范围，因为有效数字是从第一个非0位算起。事实上还要加上规格化尾数省略的1，所以问题就变成24位二进制位可以表示多少位十进制？#92;[#92;beginalign*2^24  amp;= 10^x#92;#92;x  amp;= log_102^24 = 24 #92;cdot log_102 = 7.22471989594#92;endalign*#92;]12float f = 0.123456789f;System.out.println(f); // 0.12345679  只能保证7位精度"
} ,

{
"title"    : "Git基本原理",
"category" : "",
"tags"     : "Git",
"url"      : "/blog/2021/04/13/git/",
"date"     : "2021-04-13 00:00:00 +0800",
"content"  : "把 Git 看作是一个文件系统，这很重要，事实上它就是一个小型的文件系统。先创建一个空目录my_repo，进入该目录，接下来的所有操作都在这个目录内，建议按顺序阅读。git init 做了什么git init 会在当前目录内创建一个 .git 目录，包含如下内容1234567891011121314151617181920212223242526272829❯ cd .git❯ tree ..├── branches├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── info│   └── exclude├── objects│   ├── info│   └── pack└── refs    ├── heads    └── tags看看 git status 的输出1234567❯ cd ..❯ git status位于分支 master尚无提交无文件要提交（创建/拷贝文件并使用 git add 建立跟踪）告诉我们当前位于分支 master，git 是如何知道的，其实是存储在 HEAD 文件中12❯ cat .git/HEADref: refs/heads/master再看 git branch，结果是空，因为 .git/refs/heads 下还没有任何东西，这里就是存储分支的地方git add 做了什么创建一个文件1234❯ echo 123 gt; a.txt❯ ll总用量 4-rw-r--r-- 1 head head 4  4月 13 23:51 a.txt此时 .git 中的内容没有发生任何变化执行 git add a.txt 后再查看 .git 中的内容123456789101112131415161718192021222324252627282930313233❯ git add a.txt❯ tree .git.git├── branches├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── info│   └── pack└── refs    ├── heads    └── tagsobjects 中有新增的东西，用 git cat-file 看一下1234❯ git cat-file -t 190a18037c64c43e6b11489df4bf0b9eb6d2c9bfblob❯ git cat-file -p 190a18037c64c43e6b11489df4bf0b9eb6d2c9bf123可以看到该文件类型是 blob，文件内容是 123，就是 a.txt 的内容。blob是 git 文件系统 中的一种文件类型，即二进制大对象，用于存储文件的内容 (注意，只保存文件的内容，不保存metadata如文件名、创建时间等信息)，文件内容使用 zlib的deflat 压缩算法，文件名采用 sha-1 哈希算法(20个字节，160个bit，40个十六进制)，取第1个字节作目录名，剩下19个字节作文件名。所以 objects 下最多会有 256 个目录，这样做是为了提高查找速度。除此之外，还多了一个 index 文件，也就是常说的 stage area (暂存区)，它们是一个意思。  The index is a binary file (generally kept in . git/index ) containing a sorted list of path names, each with permissions and the SHA1 of a blob object; git ls-files can show you the contents of the index. Please note that words index, stage, and cache are the same thing in Git: they are used interchangeably.可以用 git ls-files  查看12❯ git ls-filesa.txt在这里存储了相关的metadata，可以家 --debug 查看1234567❯ git ls-files --debuga.txt  ctime: 1618345384:304625740  mtime: 1618345384:304625740  dev: 65024 ino: 20849651  uid: 1000  gid: 1000  size: 8 flags: 0查看 git status12345678❯ git status位于分支 master尚无提交要提交的变更：  （使用 git rm --cached lt;文件gt;... 以取消暂存）        新文件：   a.txt试试取消暂存1234567891011121314❯ git rm --cached a.txtrm &#39;a.txt&#39;❯ ❯ ❯ git status位于分支 master尚无提交未跟踪的文件:  （使用 git add lt;文件gt;... 以包含要提交的内容）        a.txt提交为空，但是存在尚未跟踪的文件（使用 git add 建立跟踪）再次查看 index 文件12❯ git ls-files没有内容所以常说的 git add 把文件添加到暂存区就是这个意思其他部分没有变化git commit 做了什么1234❯ git commit -m add a.txt[master（根提交） bb97804] add a.txt 1 file changed, 1 insertion(+) create mode 100644 a.txt查看 .git123456789101112131415161718192021222324252627282930313233343536373839404142❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tagsobjects 中多了2个文件，分别看下是什么12345678❯ git cat-file -t bb978046c355c1fa2875c6a8473cea4a60d55814commit❯ git cat-file -p bb978046c355c1fa2875c6a8473cea4a60d55814tree c4903888e91347c58a530c1f1987dfc4d203960aauthor lxh lt;452228391qq.comgt; 1618338183 +0800committer lxh lt;452228391qq.comgt; 1618338183 +0800add a.txt可以看到该文件类型是 commit，commit 是 git 中另一种文件类型，包含了 author、commit message 等信息。还包含了另一种叫作 tree 的文件类型，也就是多出来的另一个文件1234❯ git cat-file -t c4903888e91347c58a530c1f1987dfc4d203960atree❯ git cat-file -p c4903888e91347c58a530c1f1987dfc4d203960a100644 blob 190a18037c64c43e6b11489df4bf0b9eb6d2c9bf    a.txttree 即目录树的意思，它可以包含 blob 和 tree ，即可以包含文件和子目录，就和文件系统是一样的。所以 commit 可以理解为当前时刻的文件系统快照 snapshot，这一点很重要，所以 git 可以回到任一时刻的状态。除此之外，refs/heads 下也多出来一个文件 master，看下是什么12❯ cat .git/refs/heads/masterbb978046c355c1fa2875c6a8473cea4a60d55814所以 master 只是一个存储了 commit 文件名的文件，表示指向当前最新的 commit。所以 master 只是一个别名而已，同理任何分支都是这样，比如 dev 分支。它的作用是 HEAD 会指向它，用来表示当前是在哪个分支上。12❯ cat .git/HEADref: refs/heads/master改动文件会发生什么往 a.txt 中新增内容12345678910❯ echo 456 gt;gt; a.txt❯ less a.txt❯ git status位于分支 master尚未暂存以备提交的变更：  （使用 git add lt;文件gt;... 更新要提交的内容）  （使用 git restore lt;文件gt;... 丢弃工作区的改动）        修改：     a.txt修改尚未加入提交（使用 git add 和/或 git commit -a）执行 add1234567891011121314151617181920212223242526272829303132333435363738394041424344454647❯ git add a.txt❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── ce│   │   └── 8c77db7f732ddc56661bc5f5cae2e1198978b1│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tags16 directories, 26 filesobjects 中多出了一个文件12345❯ git cat-file -t ce8c77db7f732ddc56661bc5f5cae2e1198978b1blob❯ git cat-file -p ce8c77db7f732ddc56661bc5f5cae2e1198978b1123456就是 a.txt 最新的内容，而原来的 a.txt 依然还在(190a18037c64c43e6b11489df4bf0b9eb6d2c9bf)，所以 git 保存的是整个文件，而不是只是变化的部分，这也是 commit 的基础，所以 git 能回到任一时刻的状态。执行提交123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051❯ git commit -m modify a.txt[master 294a462] modify a.txt 1 file changed, 1 insertion(+)❯ tree .git.git├── branches├── COMMIT_EDITMSG├── config├── description├── HEAD├── hooks│   ├── applypatch-msg.sample│   ├── commit-msg.sample│   ├── fsmonitor-watchman.sample│   ├── post-update.sample│   ├── pre-applypatch.sample│   ├── pre-commit.sample│   ├── pre-merge-commit.sample│   ├── prepare-commit-msg.sample│   ├── pre-push.sample│   ├── pre-rebase.sample│   ├── pre-receive.sample│   ├── push-to-checkout.sample│   └── update.sample├── index├── info│   └── exclude├── logs│   ├── HEAD│   └── refs│       └── heads│           └── master├── objects│   ├── 19│   │   └── 0a18037c64c43e6b11489df4bf0b9eb6d2c9bf│   ├── 29│   │   └── 4a462e98ac84b47522beb5a6c5b795bad599c2│   ├── bb│   │   └── 978046c355c1fa2875c6a8473cea4a60d55814│   ├── c4│   │   └── 903888e91347c58a530c1f1987dfc4d203960a│   ├── ce│   │   └── 8c77db7f732ddc56661bc5f5cae2e1198978b1│   ├── e9│   │   └── bbf146022722173fb1c459daf3a03f211ad3ad│   ├── info│   └── pack└── refs    ├── heads    │   └── master    └── tagsobjects 下又多了两个文件，分别看一下123456789❯ git cat-file -p 294a462e98ac84b47522beb5a6c5b795bad599c2tree e9bbf146022722173fb1c459daf3a03f211ad3adparent bb978046c355c1fa2875c6a8473cea4a60d55814author lxh lt;452228391qq.comgt; 1618340817 +0800committer lxh lt;452228391qq.comgt; 1618340817 +0800modify a.txt❯ git cat-file -t 294a462e98ac84b47522beb5a6c5b795bad599c2commit这是我们刚刚的提交，里面又多了一个 tree1234❯ git cat-file -t e9bbf146022722173fb1c459daf3a03f211ad3adtree❯ git cat-file -p e9bbf146022722173fb1c459daf3a03f211ad3ad100644 blob ce8c77db7f732ddc56661bc5f5cae2e1198978b1    a.txt感觉似曾眼熟，这不跟 c4903888e91347c58a530c1f1987dfc4d203960a 重复了吗？事实上，基于 hash 的算法，只要内容有任何改动，哈希结果都会发生变化。原tree  中包含了 a.txt，a.txt 发生了变化会导致原 tree 的哈希结果也发生变化，所以产生了一个新的 tree。可能你觉得这里有点多余，我们换个场景就明白了。( 另外可以自行查看一下，refs/heads/master 中的内容，现在应该是指向了这个新的 commit)删除 a.txt12rm a.txtgit add .此时 objects 没有发生任何变化，只有 index 发生了变化12❯ git ls-filesindex 里没有文件了，即暂存区里没有文件了，因为我们把它删除了，显而易见，工作区里也不会有这个文件了1234❯ pwd/home/head/code/my_repo❯ ll总用量 0提交123456❯ git commit -m delete a.txt[master 138a68a] delete a.txt 1 file changed, 2 deletions(-) delete mode 100644 a.txt❯ ll总用量 0objects 下又多了两个文件，这里就不展示目录树了123456789101112❯ git cat-file -t 138a68ac42b5fde646849df01c1339cfaec874c3commit❯ git cat-file -p 138a68ac42b5fde646849df01c1339cfaec874c3tree 4b825dc642cb6eb9a060e54bf8d69288fbee4904parent 294a462e98ac84b47522beb5a6c5b795bad599c2author lxh lt;452228391qq.comgt; 1618342629 +0800committer lxh lt;452228391qq.comgt; 1618342629 +0800delete a.txt❯ git cat-file -t 4b825dc642cb6eb9a060e54bf8d69288fbee4904tree❯ git cat-file -p 4b825dc642cb6eb9a060e54bf8d69288fbee4904一个是最新的提交，一个是最新的tree，这个tree里没有任何东西现在我们来找回这个文件12345678910111213141516171819202122232425262728293031323334353637383940gt; git logcommit 138a68ac42b5fde646849df01c1339cfaec874c3 (HEAD -gt; master)Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 03:37:09 2021 +0800    delete a.txtcommit 294a462e98ac84b47522beb5a6c5b795bad599c2Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 03:06:57 2021 +0800    modify a.txtcommit bb978046c355c1fa2875c6a8473cea4a60d55814Author: lxh lt;452228391qq.comgt;Date:   Wed Apr 14 02:23:03 2021 +0800    add a.txt    ❯ git reset 294a462e98ac84b47522beb5a6c5b795bad599c2重置后取消暂存的变更：D       a.txt❯ git status位于分支 master尚未暂存以备提交的变更：  （使用 git add/rm lt;文件gt;... 更新要提交的内容）  （使用 git restore lt;文件gt;... 丢弃工作区的改动）        删除：     a.txt修改尚未加入提交（使用 git add 和/或 git commit -a）❯ git restore .❯ ll总用量 4-rw-r--r-- 1 head head 8  4月 14 03:43 a.txt❯ cat a.txt123456所以，由于保存了任一时刻的快照，便能很容易的恢复到任一时刻的文件系统状态。所以，得把 tree理解为系统快照，是带状态的一种对象，不能以普通文件系统的视角，虽然是同一个目录，但是有文件和没文件是两种状态；即便是同一个文件，文件改变了，那整个目录也是一种新的状态。这就是时光穿梭机！git checkout 做了什么git checkout -b dev 会在 refs/heads 下新增一个 dev 文件，文件内容为当前最新 commit ，此时应该和 master 内容是一样的，两个分支此时的状态完全一致，objects 下没有任何变化。除此之外，HEAD 的内容会更改为 ref refs/heads/dev ，表示当前分支为 dev 分支。这就是 checkout 所做的全部内容了。所以分支只是一个别名而已，指向不同的 commit，以表示不同时刻的文件系统状态。从 git 可以学到什么      链表(指针)          branch -gt; commit -gt; tree -gt; blob (subtree)      branch 只是一个 commit 的别名，或者叫指针也行      commit 如果不是第一个提交的话，还有一个 parent 属性，表示之前一个提交      git 的整个操作过程简化来说就是操作 HEAD 指针在各个 commit 之间跳转，然后根据 commit 中的 tree “渲染” 出整个文件系统            哈希          基于哈希的消息摘要能快速的标识出整个文件系统，而且 SHA-1 的碰撞概率在版本控制这个用途上来说基本可以认为不会发生      See Also Git是否考虑到SHA1碰撞的问题了？            存储整体而不是存储变化，好像 React 基本思想也是这样(当状态变化时重新渲染组件，React也可以进行时光穿梭)          这里的整体指的是文件和相关联的tree这个整体，而不是整个文件系统。其他未发生变化的blob和tree直接引用就行。      常用命令本地创建分支并推送到远程分支1git push origin branch:branch --- 本地分支与远程分支要同名删除远程分支1git push origin --delete branchSee AlsoGit internals"
} ,

{
"title"    : "Manjaro安装配置踩坑",
"category" : "",
"tags"     : "Manjaro",
"url"      : "/blog/2021/04/12/manjaro-install/",
"date"     : "2021-04-12 00:00:00 +0800",
"content"  : "LVM方式的安装我安装的版本是Manjaro 21 KDE，常规安装按图形界面方式很快，没有任何问题。但 calamares当前版本对 lvm 的支持还不是很好，直接在图形界面采用 lvm 点下一步后会马上闪退，或者进入安装过程后报各种卷创建的错误，实在无招，只能曲线救国。最开始手动创建卷，然后进图形界面选择文件系统和挂载点，安装倒是没问题了，最后也提示安装成功。但重启后找不到系统，还是会继续进入U盘。推测是 calamares 在 lvm 方式下创建 efi 分区有bug，gitHub上也有很多相关issue。试了N遍之后，最后的办法是先按常规方式安装一遍，保证重启后能正常进入系统。再用U盘进入live environment，在已分区的基础上手动创建物理卷，卷组和逻辑卷。efi 和 swap 不要动(注意在后续图形安装界面中 efi 千万别选择格式化，要的就是保留 efi 之前的内容)，其他分区可自由分配卷组和逻辑卷。最后进入图形安装界面选择挂载点和文件系统安装即可。Intel核显 + NVIDIA独显 的问题  Linus Torvalds:  NVIDIA, fuck u!安装完之后一开始正常使用没什么问题，但是一竖屏后就有撕裂现象。解决办法：安装 optimus-manager 和 optimus-manager-qt ，切换为 NVIDIA 之后就没问题了。大黄蜂方案已经过时且不再维护了，不建议使用。optimus-manager ( A Linux program to handle GPU switching on Optimus laptops)文档中有关于 Gnome and GDM users 和 IMPORTANT : Manjaro KDE users 的注意事项，一定要看，否则可能会导致切换后黑屏、无法启动的问题，我在这浪费了好多时间…换源sudo pacman-mirros -i -c China -m rank       选中一个sudo pacman -Syy      更新软件源sudo pacman -Syu      更新全部软件安装搜狗拼音添加 archlinuxcn 软件源，/etc/pacman.conf 在最后加入SigLevel = Optional TrustAll Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/archsudo pacman -Syy      更新软件源sudo pacman -S archlinux-keyring     更新软件密钥sudo pacman -S fcitx-im     安装输入法框架sudo pacman -S fcitx-configtool     安装输入法配置工具在软件中心找到并安装 fcitx-sogoupinyin( 注意：一定要按顺序安装，不能直接安装最后一个，会导致依赖缺失，不能配置等问题)在 /etc/environment 中加入123GTK_IM_MODULE=fcitxQT_IM_MODULE=fcitxXMODIFIERS=im=fcitx重启完成安装微信文档写的很详细：deepin-wine-wechat-arch中文字体显示为框框：  下载宋体字符集文件：https://www.freefonts.io/downloads/simsun/  将解压后的文件复制到如下路径：cp    SIMSUN.ttf    ~/.deepinwine/Deepin-WeChat/drive_c/windows/Fonts/  重启微信使用 oh-my-zshhttps://gist.github.com/yovko/becf16eecd3a1f69a4e320a95689249e主题美化IDEA Ultimate 输入法候选框不跟随鼠标，一直在左下角这是一个 idea 古老又没人管的bug解决办法参考：https://ld246.com/article/1601280084643下载修改过的  JetBrainsRuntime ，在 idea.sh 启动脚本开头添加export IDEA_JDK=xport IDEA_JDK=目录/java-11.0.7-jetbrain但是有个新的问题，markdown文件 的预览按钮不见了，也懒得折腾了，写 markdown 用专门的编辑器吧，比如 TypoarIDEA 不会显示全局系统菜单Manjaro安装了 Application Title 插件后所有应用的菜单都会在顶部的全局系统菜单显示，但 IDEA 不会。官方解决办法： idea中安装 JavaFX Runtime for Plugins 插件。但是有点延迟，打开 idea 后要过一会菜单才会出来，也是很烦。美化Make Your KDE Plasma Desktop Look Better参考着来，不需要完全照搬pamac 安装软件超时查看安装超时的地址，手动下载后放到 /var/tmp/pamac-build- lt; usernamegt; / lt; AppNamegt; ` 目录下，再重新安装即可。VMware Workstation第一次启动虚拟机时报错Cannot open /dev/vmmon: No such file or directory. Please make sure that the kernel module vmmon&#39; is loaded。 原因是内核中没有vmmon模块。首先推荐使用LTS版本，其次较新的版本可能还没有 vmmon模块的补丁。执行以下命令安装 linux-headers：1  sudo pacman -S (pacman -Qsq ^linux | grep ^linux[0-9]*[-rt]* | awk &#39;print 1-headers&#39; ORS=&#39; &#39;)重启后执行：1  sudo vmware-modconfig --console --install-all参考链接1参考链接2 　　打开虚拟网络编辑器报错Fail Network configuration is missing. Ensure that /etc/vmware/-networking exists.执行以下命令启动 vmware-networks-configuration 服务：1  systemctl start vmware-networks-configuration.service虚拟机无网络报错 Could not connect &#39;Ethernet0&#39; to virtual network &#39;/dev/vmnet0&#39;。重装 linux-headers：123  sudo pacman -R linux510-headers    sudo pacman -S linux510-headers重置网卡：1  sudo touch /etc/vmware/x amp;amp; sudo vmware-networks --migrate-network-settings /etc/vmware/x amp;amp; sudo rm /etc/vmware/x amp;amp; sudo modprobe vmnet amp;amp; sudo vmware-networks --start查看网卡状态：1  vmware-networks --status虚拟机设置里选择连接方式和网卡即可。若后面再次 could not connect，执行12systemctl start vmware-networkssystemctl enable vmware-networks保证 vmware-networks 是 loaded 的就行，状态是 inactive 也没关系。重新在虚拟机设置里选择连接方式和网卡即可。"
} ,

{
"title"    : "跟着 Github 学习 Restful HTTP API 设计",
"category" : "",
"tags"     : "restful",
"url"      : "/blog/2020/11/21/%E8%B7%9F%E7%9D%80-Github-%E5%AD%A6%E4%B9%A0-Restful-HTTP-API-%E8%AE%BE%E8%AE%A1/",
"date"     : "2020-11-21 00:00:00 +0800",
"content"  : "  转自 https://cizixs.com/2016/12/12/restful-api-design-guide/近几年提供 HTTP API 服务的公司越来越多，许多公司都把 API 作为产品重要的一部分，作为服务提供出去。而微服务的兴起，也让企业内部开始重视和频繁使用 HTTP API 。好的 HTTP API 设计容易理解、符合 RFC 标准、提供使用者便利的功能，其中经常被拿来作为教科书典范的当属 Github API。这篇文章就通过 Github API 总结了一些非常好的设计原则，可以作为以后要编写 HTTP API 的参考。注意：这篇文章只讨论设计原则，不是强制要求（API 设计者可以根据实际情况实现部分内容，甚至实现出和某些原则相反的内容），也不会给出实现的思路和细节。1. 使用 HTTPS这个和 Restful API 本身没有很大的关系，但是对于增加网站的安全是非常重要的。特别如果你提供的是公开 API，用户的信息泄露或者被攻击会严重影响网站的信誉。NOTE：不要让非SSL的url访问重定向到SSL的url。2. API 地址和版本在 url 中指定 API 的版本是个很好地做法。如果 API 变化比较大，可以把 API 设计为子域名，比如 https://api.github.com/v3；也可以简单地把版本放在路径中，比如 https://example.com/api/v1。3. schema对于响应返回的格式，JSON 因为它的可读性、紧凑性以及多种语言支持等优点，成为了 HTTP API 最常用的返回格式。因此，最好采用 JSON 作为返回内容的格式。如果用户需要其他格式，比如 xml，应该在请求头部 Accept 中指定。对于不支持的格式，服务端需要返回正确的 status code，并给出详细的说明。4. 以资源为中心的 URL 设计资源是 Restful API 的核心元素，所有的操作都是针对特定资源进行的。而资源就是 URL（Uniform Resoure Locator）表示的，所以简洁、清晰、结构化的 URL 设计是至关重要的。Github 可以说是这方面的典范，下面我们就拿 repository 来说明。12345/users/:username/repos/users/:org/repos/repos/:owner/:repo/repos/:owner/:repo/tags/repos/:owner/:repo/branches/:branch我们可以看到几个特性：  资源分为单个文档和集合，尽量使用复数来表示资源，单个资源通过添加 id 或者 name 等来表示  一个资源可以有多个不同的 URL  资源可以嵌套，通过类似目录路径的方式来表示，以体现它们之间的关系NOTE: 根据RFC3986定义，URL是大小写敏感的。所以为了避免歧义，尽量使用小写字母。5. 使用正确的 Method有了资源的 URL 设计，所有针对资源的操作都是使用 HTTP 方法指定的。比较常用的方法有：            VERB      描述                  HEAD      只获取某个资源的头部信息。比如只想了解某个文件的大小，某个资源的修改日期等              GET      获取资源              POST      创建资源              PATCH      更新资源的部分属性。因为 PATCH 比较新，而且规范比较复杂，所以真正实现的比较少，一般都是用 POST 替代              PUT      替换资源，客户端需要提供新建资源的所有属性。如果新内容为空，要设置 Content-Length 为 0，以区别错误信息              DELETE      删除资源      比如：12345GET /repos/:owner/:repo/issuesGET /repos/:owner/:repo/issues/:numberPOST /repos/:owner/:repo/issuesPATCH /repos/:owner/:repo/issues/:numberDELETE /repos/:owner/:repoNOTE：更新和创建操作应该返回最新的资源，来通知用户资源的情况；删除资源一般不会返回内容。不符合 CRUD 的情况在实际资源操作中，总会有一些不符合 CRUD（Create-Read-Update-Delete） 的情况，一般有几种处理方法。使用 POST为需要的动作增加一个 endpoint，使用 POST 来执行动作，比如 POST /resend 重新发送邮件。增加控制参数添加动作相关的参数，通过修改参数来控制动作。比如一个博客网站，会有把写好的文章“发布”的功能，可以用上面的 POST /articles/:id/publish 方法，也可以在文章中增加 published:boolean 字段，发布的时候就是更新该字段 PUT /articles/:id?published=true把动作转换成资源把动作转换成可以执行 CRUD 操作的资源， github 就是用了这种方法。比如“喜欢”一个 gist，就增加一个 /gists/:id/star 子资源，然后对其进行操作：“喜欢”使用 PUT /gists/:id/star，“取消喜欢”使用 DELETE /gists/:id/star。另外一个例子是 Fork，这也是一个动作，但是在 gist 下面增加 forks资源，就能把动作变成 CRUD 兼容的：POST /gists/:id/forks 可以执行用户 fork 的动作。6. Query 让查询更自由比如查询某个 repo 下面 issues 的时候，可以通过以下参数来控制返回哪些结果：  state：issue 的状态，可以是 open，closed，all  since：在指定时间点之后更新过的才会返回  assignee：被 assign 给某个 user 的 issues  sort：选择排序的值，可以是 created、updated、comments  direction：排序的方向，升序（asc）还是降序（desc）  ……7. 分页 Pagination当返回某个资源的列表时，如果要返回的数目特别多，比如 github 的 /users，就需要使用分页分批次按照需要来返回特定数量的结果。分页的实现会用到上面提到的 url query，通过两个参数来控制要返回的资源结果：  per_page：每页返回多少资源，如果没提供会使用预设的默认值；这个数量也是有一个最大值，不然用户把它设置成一个非常大的值（比如 99999999）也失去了设计的初衷  page：要获取哪一页的资源，默认是第一页返回的资源列表为 [(page-1)*per_page, page*per_page)。github API 文档中还提到一个很好的点，相关的分页信息还可以存放到 Link 头部，这样客户端可以直接得到诸如下一页、最后一页、上一页等内容的 url 地址，而不是自己手动去计算和拼接。8. 选择合适的状态码HTTP 应答中，需要带一个很重要的字段：status code。它说明了请求的大致情况，是否正常完成、需要进一步处理、出现了什么错误，对于客户端非常重要。状态码都是三位的整数，大概分成了几个区间：  2XX：请求正常处理并返回  3XX：重定向，请求的资源位置发生变化  4XX：客户端发送的请求有错误  5XX：服务器端错误在 HTTP API 设计中，经常用到的状态码以及它们的意义如下表：            状态码      LABEL      解释                  200      OK      请求成功接收并处理，一般响应中都会有 body              201      Created      请求已完成，并导致了一个或者多个资源被创建，最常用在 POST 创建资源的时候              202      Accepted      请求已经接收并开始处理，但是处理还没有完成。一般用在异步处理的情况，响应 body 中应该告诉客户端去哪里查看任务的状态              204      No Content      请求已经处理完成，但是没有信息要返回，经常用在 PUT 更新资源的时候（客户端提供资源的所有属性，因此不需要服务端返回）。如果有重要的 metadata，可以放到头部返回              301      Moved Permanently      请求的资源已经永久性地移动到另外一个地方，后续所有的请求都应该直接访问新地址。服务端会把新地址写在 Location 头部字段，方便客户端使用。允许客户端把 POST 请求修改为 GET。              304      Not Modified      请求的资源和之前的版本一样，没有发生改变。用来缓存资源，和条件性请求（conditional request）一起出现              307      Temporary Redirect      目标资源暂时性地移动到新的地址，客户端需要去新地址进行操作，但是不能修改请求的方法。              308      Permanent Redirect      和 301 类似，除了客户端不能修改原请求的方法              400      Bad Request      客户端发送的请求有错误（请求语法错误，body 数据格式有误，body 缺少必须的字段等），导致服务端无法处理              401      Unauthorized      请求的资源需要认证，客户端没有提供认证信息或者认证信息不正确              403      Forbidden      服务器端接收到并理解客户端的请求，但是客户端的权限不足。比如，普通用户想操作只有管理员才有权限的资源。              404      Not Found      客户端要访问的资源不存在，链接失效或者客户端伪造 URL 的时候回遇到这个情况              405      Method Not Allowed      服务端接收到了请求，而且要访问的资源也存在，但是不支持对应的方法。服务端必须返回 Allow 头部，告诉客户端哪些方法是允许的              415      Unsupported Media Type      服务端不支持客户端请求的资源格式，一般是因为客户端在 Content-Type 或者 Content-Encoding 中申明了希望的返回格式，但是服务端没有实现。比如，客户端希望收到 xml返回，但是服务端支持 Json              429      Too Many Requests      客户端在规定的时间里发送了太多请求，在进行限流的时候会用到              500      Internal Server Error      服务器内部错误，导致无法完成请求的内容              503      Service Unavailable      服务器因为负载过高或者维护，暂时无法提供服务。服务器端应该返回 Retry-After 头部，告诉客户端过一段时间再来重试      上面这些状态码覆盖了 API 设计中大部分的情况，如果对某个状态码不清楚或者希望查看更完整的列表，可以参考 HTTP Status Code 这个网站，或者 RFC7231 Response Status Codes 的内容。9. 错误处理：给出详细的信息如果出错的话，在 response body 中通过 message 给出明确的信息。比如客户端发送的请求有错误，一般会返回 4XX Bad Request 结果。这个结果很模糊，给出错误 message 的话，能更好地让客户端知道具体哪里有问题，进行快速修改。  如果请求的 JSON 数据无法解析，会返回 Problems parsing JSON  如果缺少必要的 filed，会返回 422 Unprocessable Entity，除了 message 之外，还通过 errors 给出了哪些 field 缺少了，能够方便调用方快速排错基本的思路就是尽可能提供更准确的错误信息：比如数据不是正确的 json，缺少必要的字段，字段的值不符合规定…… 而不是直接说“请求错误”之类的信息。10. 验证和授权一般来说，让任何人随意访问公开的 API 是不好的做法。验证和授权是两件事情：  验证（Authentication）是为了确定用户是其申明的身份，比如提供账户的密码。不然的话，任何人伪造成其他身份（比如其他用户或者管理员）是非常危险的  授权（Authorization）是为了保证用户有对请求资源特定操作的权限。比如用户的私人信息只能自己能访问，其他人无法看到；有些特殊的操作只能管理员可以操作，其他用户有只读的权限等等如果没有通过验证（提供的用户名和密码不匹配，token 不正确等），需要返回 401 Unauthorized状态码，并在 body 中说明具体的错误信息；而没有被授权访问的资源操作，需要返回 403 Forbidden 状态码，还有详细的错误信息。NOTE：Github API 对某些用户未被授权访问的资源操作返回 404 Not Found，目的是为了防止私有资源的泄露（比如黑客可以自动化试探用户的私有资源，返回 403 的话，就等于告诉黑客用户有这些私有的资源）。11. 限流 rate limit如果对访问的次数不加控制，很可能会造成 API 被滥用，甚至被 DDos 攻击。根据使用者不同的身份对其进行限流，可以防止这些情况，减少服务器的压力。对用户的请求限流之后，要有方法告诉用户它的请求使用情况，Github API 使用的三个相关的头部：  X-RateLimit-Limit: 用户每个小时允许发送请求的最大值  X-RateLimit-Remaining：当前时间窗口剩下的可用请求数目  X-RateLimit-Rest: 时间窗口重置的时候，到这个时间点可用的请求数量就会变成 X-RateLimit-Limit 的值如果允许没有登录的用户使用 API（可以让用户试用），可以把 X-RateLimit-Limit 的值设置得很小，比如 Github 使用的 60。没有登录的用户是按照请求的 IP 来确定的，而登录的用户按照认证后的信息来确定身份。对于超过流量的请求，可以返回 429 Too many requests 状态码，并附带错误信息。而 Github API 返回的是 403 Forbidden，虽然没有 429 更准确，也是可以理解的。Github 更进一步，提供了不影响当然 RateLimit 的请求查看当前 RateLimit 的接口 GET /rate_limit。12. Hypermedia APIRestful API 的设计最好做到 Hypermedia：在返回结果中提供相关资源的链接。这种设计也被称为 HATEOAS。这样做的好处是，用户可以根据返回结果就能得到后续操作需要访问的地址。比如访问 api.github.com，就可以看到 Github API 支持的资源操作。13. 编写优秀的文档API 最终是给人使用的，不管是公司内部，还是公开的 API 都是一样。即使我们遵循了上面提到的所有规范，设计的 API 非常优雅，用户还是不知道怎么使用我们的 API。最后一步，但非常重要的一步是：为你的 API 编写优秀的文档。对每个请求以及返回的参数给出说明，最好给出一个详细而完整地示例，提醒用户需要注意的地方……反正目标就是用户可以根据你的文档就能直接使用 API，而不是要发邮件给你，或者跑到你的座位上问你一堆问题。参考资料  Github API v3  RESTful API 设计指南  REST接口设计规范  Restful API 首次被提出的论文：Architectural Styles andthe Design of Network-based Software Architectures"
} ,

{
"title"    : "JPA 单表双向自关联、转dto、jackson序列化循环引用问题",
"category" : "",
"tags"     : "JPA",
"url"      : "/blog/2020/11/19/JPA-%E5%8D%95%E8%A1%A8%E5%8F%8C%E5%90%91%E8%87%AA%E5%85%B3%E8%81%94-%E8%BD%ACdto-jackson%E5%BA%8F%E5%88%97%E5%8C%96%E5%BE%AA%E7%8E%AF%E5%BC%95%E7%94%A8%E9%97%AE%E9%A2%98/",
"date"     : "2020-11-19 00:00:00 +0800",
"content"  : "首先看实体，Organization1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.example.organization.domain.repo.po;import lombok.Getter;import lombok.Setter;import org.hibernate.annotations.GenericGenerator;import javax.persistence.*;import java.util.Objects;import java.util.Set;EntityTable(name = organization)GetterSetterpublic class OrganizationPo     Id    GenericGenerator(name = idGenerator, strategy = uuid)    GeneratedValue(generator = idGenerator)    Column(name = org_id)    private String orgId;Column(name = org_name)private String orgName; ManyToOneJoinColumn(name = parent_id, referencedColumnName = org_id)private OrganizationPo parent; OneToMany(mappedBy = parent)private Setlt;OrganizationPogt; children; Overridepublic boolean equals(Object o)     if (this == o)         return true;        if (o == null || getClass() != o.getClass())         return false;        OrganizationPo that = (OrganizationPo) o;    return orgId.equals(that.orgId); Overridepublic int hashCode()     return Objects.hash(orgId);    这是一个双向自关联的实体。表数据如下：对于 OneToMany  默认fetch策略是 Lazy，也就是在实际访问到 children 属性时才会发出数据库请求，比如你打了断点调试，查看 children 属性时，或者  ResponseBody  序列化到前端时才会发出新的查询。注意不要使用 lombok 的 Data，会有带上所有字段的 toString() 方法，此时debug的时候会因为调用 toString() 而导致循环引用从而抛出 stackoverflowError，使用默认的 toString() 就好，默认的 toString() 采用的是 hashCode 的16进制。接下来需要做 po(persistence object)到do(domain object)、do到dto(data transfer object) 的转换，此时依旧会出现循环引用、堆栈溢出的问题。do实体如下：1234567891011121314151617181920212223242526272829303132package com.example.organization.domain.aggregate;import lombok.Getter;import lombok.Setter;import java.util.Objects;import java.util.Set;GetterSetterpublic class Organization     private String orgId;    private String orgName;    private Organization parent;    private Setlt;Organizationgt; children;Overridepublic boolean equals(Object o)     if (this == o)         return true;        if (o == null || getClass() != o.getClass())         return false;        Organization that = (Organization) o;    return orgId.equals(that.orgId); Overridepublic int hashCode()     return Objects.hash(orgId);    我采用的 bean mapping 工具是 mapstruct ，各方面都较优的一个映射工具，它提供的解决方案是：CycleAvoidingMappingContext 如下：123456789101112public class CycleAvoidingMappingContext     private Maplt;Object, Objectgt; knownInstances = new IdentityHashMaplt;Object, Objectgt;();BeforeMappingpublic lt;Tgt; T getMappedInstance(Object source, TargetType Classlt;Tgt; targetType)     return (T) knownInstances.get(source); BeforeMappingpublic void storeMappedInstance(Object source, MappingTarget Object target)     knownInstances.put(source, target);    使用方式：123private static Organization toDo(OrganizationPo po)         return OrganizationMapper.INSTANCE.toDo(po, new CycleAvoidingMappingContext());    原理就是以已经映射过的 po 为 key，do 为 value，存到一个 map 里。在递归设值的过程中，再遇到同一个 po，就可以直接从 map 中取出对应的 do，从而避免了无限循环。最后到了序列化的阶段，此时仍然有循环引用的问题。springboot 默认采用的是 jackson 框架，jackson 提供了一个注解可以解决此问题：意思就是对于相同的对象，直接使用 orgId 属性表示。最后假如执行 dao.findById(1)，返回的结果如下：123456789101112131415161718192021222324252627282930313233343536  orgId: 1,  orgName: 研发部,  children: [          orgId: 2,      orgName: 河北项目组,      parent: 1,      children: [                  orgId: 4,          orgName: 北京小分队,          parent: 2,          children: [                          orgId: 5,              orgName: 销售组,              parent: 4,              children: [                                      ]              ]      ],  orgId: 3,  orgName: 福建项目组,  parent: 1,  children: [              ]      ]可以看到解决循环引用的思路都是想办法切断循环引用，一个是返回已有的对象，一个是用其他属性表示，都可以终止递归调用。"
} ,

{
"title"    : "对DDD的一点理解",
"category" : "",
"tags"     : "DDD",
"url"      : "/blog/2020/11/13/%E5%AF%B9DDD%E7%9A%84%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3/",
"date"     : "2020-11-13 00:00:00 +0800",
"content"  : "DDD分层架构是什么？4层架构：由下往上，基础层 -gt; 领域层 -gt; 应用层 -gt; 用户接口层领域层和应用层的区别是什么？从底往上逐层讲。单个实体自身的方法就是实体本身的业务行为。多个实体可组成更复杂的业务动作，这个是领域服务，实体的方法和领域服务共同构成领域模型的基础业务能力，这个能力是原子的基础的，不太考虑外界的用户行为和流程。而应用服务是对这些基础的能力进行组合和编排，它组合和编排的服务可以是跨聚合的领域服务，主要体现组合后的业务能力，更面向前端的用户操作，属于比较粗粒度的服务，通过编排可以更灵活应对外部需求变化。中台业务建模过程？  按照业务流程（通常适用于核心域）或者功能属性、集合（通常适用于通用域或支撑域），将业务域细分为多个中台，再根据功能属性或重要性归类到核心中台或通用中台。核心中台设计时要考虑核心竞争力，通用中台要站在企业高度考虑共享和复用能力。  选取中台，根据用例、业务场景或用户旅程完成事件风暴，找出实体、聚合和限界上下文。依次进行领域分解，建立领域模型。由于不同中台独立建模，某些领域对象或功能可能会重复出现在其它领域模型中，也有可能本该是同一个聚合的领域对象或功能，却分散在其它的中台里，这样会导致领域模型不完整或者业务不内聚。这里先不要着急，这一步我们只需要初步确定主领域模型就可以了，在第三步中我们还会提炼并重组这些领域对象。  以主领域模型为基础，扫描其它中台领域模型，检查并确定是否存在重复或者需要重组的领域对象、功能，提炼并重构主领域模型，完成最终的领域模型设计。  选择其它主领域模型重复第三步，直到所有主领域模型完成比对和重构。  基于领域模型完成微服务设计，完成系统落地。划分核心中台要看你的企业核心竞争力是否在这个领域。以保险举例：怎么抉择一个实体是不是聚合根？判断一个实体是否是聚合根，你可以结合以下内容进行分析：  是否有独立的生命周期？  是否有全局唯一 ID ？  是否可以创建或修改其他对象？  是否有专门的模块来管理这个实体等？聚合根管理了聚合内所有实体和值对象的生命周期，我们通过聚合根就可以获取到聚合内所有实体和值对象等领域对象。一般来说，如果聚合根被删除了，那么被它引用的实体和值对象也就不会存在了。操作数据库应该放到哪里？聚合根中应该包括哪些业务行为？在聚合内部操作数据库都是通过调用仓储接口来实现的。由于一个聚合内需要按照统一的业务规则来完成实体的数据变更，也就是说不可以随意修改某一个实体的数据表。为了统一这些数据更新的操作，会统一由聚合根来管理，所以操作数据库的方法一般都是放在聚合根的方法中。对于聚合内跨多个实体的业务逻辑，这部分功能可以由聚合根的方法来实现，也可以通过聚合内的领域服务来实现。但为了避免聚合根的业务逻辑过于复杂，避免聚合根类代码量过于庞大，我个人建议聚合根除了承担它的聚合管理职能外，只作为实体实现与聚合根自身行为相关的业务逻辑，而将跨多个实体的复杂领域逻辑统一放在领域服务中实现。因此在完成业务逻辑后，在操作数据库时可以通过聚合根方法或领域服务来完成，切勿在某个实体的方法中不受控制的去修改数据库中实体的数据。在设计过程中，对于一些复杂的流程细节没考虑到位，或者忽略了某个细节流程，而导致在程序落地过程中，发现原有的建模不够严谨，对于这种场景，有什么补救措施吗，或者如何避免这一问题的发生？这个可能需要分一下几类情况来处理：  如果是聚合内实体的业务逻辑没考虑到，只需要修改对应实体内的属性或者方法即可。  如何是聚合内实体之间的关系没考虑到，调整或新增领域服务，或者聚合根的方法即可。  如果是在同一个限界上下文内的聚合之间的关系没考虑到，在应用层的应用服务中调整或新增即可。  如果是聚合划分到了错误的限界上下文内，整体将聚合内所有对象和代码调整到合适的限界上下文即可，并重新建立新的限界上下文内聚合之间的关系。DDD从设计到落地的大概流程？先进行战略设计，再进行战术设计。战略设计采用的方法是事件风暴，包括产品愿景、场景分析、领域建模、微服务拆分。领域建模又分为3步：  根据场景分析，分析并找出发起或产生这些命令或领域事件的实体和值对象，将与实体或值对象有关的命令和事件聚集到实体；  找出聚合根，根据实体、值对象与聚合根的依赖关系，建立聚合；      根据业务及语义边界等因素，定义限界上下文。    微服务拆分：理论上一个限界上下文就可以设计为一个微服务，但还需要综合考虑多种外部因素，比如：职责单一性、敏态与稳态业务分离、非功能性需求（如弹性伸缩、版本发布频率和安全等要求）、软件包大小、团队沟通效率和技术异构等非业务要素。  战术设计包括：  根据命令设计应用服务，确定应用服务的功能，服务集合，组合和编排方式。服务集合中的服务包括领域服务或其它微服务的应用服务；  根据应用服务功能要求设计领域服务，定义领域服务。这里需要注意：应用服务可能是由多个聚合的领域服务组合而成的；  根据领域服务的功能，确定领域服务内的实体以及功能。设计实体基本属性和方法。"
} ,

{
"title"    : "对Java回调的一点理解",
"category" : "",
"tags"     : "设计模式",
"url"      : "/blog/2020/11/10/%E5%AF%B9Java%E5%9B%9E%E8%B0%83%E7%9A%84%E4%B8%80%E7%82%B9%E7%90%86%E8%A7%A3/",
"date"     : "2020-11-10 00:00:00 +0800",
"content"  : "首先解释一下何为回调？我的理解是原本是A调用B，在方法执行过程中B又会调用由A提供的一个方法，故称回调。好处是可以让调用方来决定部分算法逻辑，这让整个算法变得非常灵活，但是控制权还是在提供方这边。Spring的 JdbcTemplate 就是回调的典型实现，摘取 queryForObject() 进行分析1234public lt;Tgt; T queryForObject(String sql, RowMapperlt;Tgt; rowMapper, Nullable Object... args) throws DataAccessException         Listlt;Tgt; results = query(sql, args, new RowMapperResultSetExtractorlt;gt;(rowMapper, 1));        return DataAccessUtils.nullableSingleResult(results);    此时假如要查询出一个 domian对象，通常会这么写123456789Actor actor = jdbcTemplate.queryForObject(        select first_name, last_name from t_actor where id = ?,        (resultSet, rowNum) -gt;             Actor newActor = new Actor();            newActor.setFirstName(resultSet.getString(first_name));            newActor.setLastName(resultSet.getString(last_name));            return newActor;        ,        1212L);第二个参数  RowMapper 就是一个回调函数1234567891011121314151617FunctionalInterfacepublic interface RowMapperlt;Tgt;     /**     * Implementations must implement this method to map each row of data     * in the ResultSet. This method should not call code next() on     * the ResultSet; it is only supposed to map values of the current row.     * param rs the ResultSet to map (pre-initialized for the current row)     * param rowNum the number of the current row     * return the result object for the current row (may be code null)     * throws SQLException if an SQLException is encountered getting     * column values (that is, there&#39;s no need to catch SQLException)     */    Nullable    T mapRow(ResultSet rs, int rowNum) throws SQLException;所以本质上就是由我们来提供 mapRow 的具体实现，比如这里就是要生成一个 Actor 对象，然后 jdbcTemplate 回调 我们提供的这个实现，将这段逻辑插入到自己的方法中。当然你可以做任何事情，因为我们已经有了结果集 ResultSet，所以非常灵活。具体执行过程可跟踪源码，最终都是进入到了 execute() 方法中，大概执行步骤都是12345678910111213try    获取 connection    ...    执行我们提供的回调函数    ...catch()finally    关闭 connection    释放资源"
} ,

{
"title"    : "让Spring事务支持同一个类的内部调用",
"category" : "",
"tags"     : "Spring",
"url"      : "/blog/2020/11/09/%E8%AE%A9Spring%E4%BA%8B%E5%8A%A1%E6%94%AF%E6%8C%81%E5%90%8C%E4%B8%80%E4%B8%AA%E7%B1%BB%E7%9A%84%E5%86%85%E9%83%A8%E8%B0%83%E7%94%A8/",
"date"     : "2020-11-09 00:00:00 +0800",
"content"  : "问题：当同一个类中的方法A调用方法B时，即使两个方法都打上了Transactional注解，方法B的事务也不会生效。原因：默认情况下Spring事务是基于代理的，也就意味着获取到的service对象是代理后的对象（class com.sun.proxy.Proxy，基于接口的情况）。当外部调用该对象上的方法时，经过aop加上的事务逻辑后，最终会进入到目标对象（即原始的service对象）的方法逻辑，此时在方法内部再调用自己的另一个方法B，本质上就是在原始对象上进行调用，此时自然而然不会牵扯到任何aop的事务逻辑。解决办法：由上而知，采用代理的方式肯定行不通，故而必须采用字节码提升的方式，直接修改类的字节码，为每个打上Transactional注解的方法都加上事务逻辑。具体分两步走：      设置 EnableTransactionManagement(mode = AdviceMode.ASPECTJ) ，表示采用Aspectj 来对需要事务管理的方法进行字节码提升；        首先引入    12345lt;dependencygt;    lt;groupIdgt;org.springframeworklt;/groupIdgt;    lt;artifactIdgt;spring-aspectslt;/artifactIdgt;    lt;versiongt;5.3.0lt;/versiongt;lt;/dependencygt;      ​      加入spring对Aspectj的支持。​      而Aspectj weaving 的方式有两种：编译时和运行时。​      编译时即在编译期间就完成了对类的字节码的修改。可以通过加入一个maven插件实现：123456789101112131415161718192021222324252627282930  lt;buildgt;      lt;pluginsgt;          lt;plugingt;              lt;groupIdgt;org.codehaus.mojolt;/groupIdgt;              lt;artifactIdgt;aspectj-maven-pluginlt;/artifactIdgt;              lt;versiongt;1.6lt;/versiongt;              lt;executionsgt;                  lt;executiongt;                      lt;goalsgt;                          lt;goalgt;compilelt;/goalgt;                          lt;!-- lt;goalgt;test-compilelt;/goalgt; --gt;                      lt;/goalsgt;                  lt;/executiongt;              lt;/executionsgt;              lt;configurationgt;                  lt;complianceLevelgt;1.8lt;/complianceLevelgt;                  lt;forceAjcCompilegt;truelt;/forceAjcCompilegt;                  lt;weaveDirectoriesgt;                      lt;weaveDirectorygt;project.build.outputDirectorylt;/weaveDirectorygt;                  lt;/weaveDirectoriesgt;                  lt;aspectLibrariesgt;                      lt;aspectLibrarygt;                          lt;groupIdgt;org.springframeworklt;/groupIdgt;                          lt;artifactIdgt;spring-aspectslt;/artifactIdgt;                      lt;/aspectLibrarygt;                  lt;/aspectLibrariesgt;              lt;/configurationgt;          lt;/plugingt;      lt;/plugingt;  lt;/buildgt;运行时即在class被加入到ClassLoader之前动态的修改类的字节码。具体用到了LoadTimeWeaving，详情参考。  Foo类中方法A调用Bar类中的方法B是ok的，因为是在代理后的Bar的对象上进行操作，事务逻辑是存在的。但是要注意方法B的事务的传播级别，默认为REQUIRED，即如果存在了一个事务则直接使用，不存在才会创建一个新的事务。所以加入方法A的事务是READ-WRITE，方法B的事务是READ-ONLY，这时方法B会直接使用方法A的事务，会导致READ-ONLY失效。这时需要将方法B的传播级别改为REQUIRES_NEW。    建议： 强烈建议同一个类内部的事务方法之间不要进行相互调用，尽可能的重构此类代码，最好合并到一个事务方法中。这也是Spring官方推荐的。"
} ,

{
"title"    : "Java 如何正确地输出日志",
"category" : "",
"tags"     : "log",
"url"      : "/blog/2020/11/08/Java-%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E5%9C%B0%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97/",
"date"     : "2020-11-08 00:00:00 +0800",
"content"  : "  转自：  作者：蒋老湿链接：https://juejin.cn/post/6844903678290165773什么是日志简单的说，日志就是记录程序的运行轨迹，方便查找关键信息，也方便快速定位解决问题。我们 Java 程序员在开发项目时都是依赖 Eclipse/ Idea 等开发工具的 Debug 调试功能来跟踪解决 Bug，在开发环境可以这么做，但项目发布到了测试、生产环境呢？你有可能会说可以使用远程调试，但实际并不能允许让你这么做。所以，日志的作用就是在测试、生产环境没有 Debug 调试工具时开发、测试人员定位问题的手段。日志打得好，就能根据日志的轨迹快速定位并解决线上问题，反之，日志输出不好不能定位到问题不说反而会影响系统的性能。优秀的项目都是能根据日志定位问题的，而不是在线调试，或者半天找不到有用的日志而抓狂…常用日志框架log4j、Logging、commons-logging、slf4j、logback，开发的同学对这几个日志相关的技术不陌生吧，为什么有这么多日志技术，它们都是什么区别和联系呢？相信大多数人搞不清楚它们的关系，下面我将一一介绍一下，以后大家再也不用傻傻分不清楚了。Logging如图所示，这是 Java 自带的日志工具类，在 JDK 1.5 开始就已经有了，在 java.util.logging 包下。更多关于 Java Logging 的介绍可以看官方文档Log4jLog4j 是 Apache 的一个开源日志框架，也是市场占有率最多的一个框架。大多数没用过 Java Logging， 但没人敢说没用过 Log4j 吧，反正从我接触 Java 开始就是这种情况，做 Java 项目必有 Log4j 日志框架。注意：log4j 在 2015/08/05 这一天被 Apache 宣布停止维护了，用户需要切换到 Log4j2上面去。下面是官方宣布原文：  On August 5, 2015 the Logging Services Project Management Committee announced that Log4j 1.x had reached end of life. For complete text of the announcement please see the Apache Blog. Users of Log4j 1 are recommended to upgrade to Apache Log4j 2.Log4j2 的官方地址commons-logging上面介绍的 log4j 是一个具体的日志框架的实现，而 commons-logging 就是日志的门面接口，它也是 apache 最早提供的日志门面接口，用户可以根据喜好选择不同的日志实现框架，而不必改动日志定义，这就是日志门面的好处，符合面对接口抽象编程。更多的详细说明可以参考官方说明Slf4j全称：Simple Logging Facade for Java，即简单日志门面接口，和 Apache 的 commons-logging 是一样的概念，它们都不是具体的日志框架，你可以指定其他主流的日志实现框架。Slf4j 的官方地址Slf4j 也是现在主流的日志门面框架，使用 Slf4j 可以很灵活的使用占位符进行参数占位，简化代码，拥有更好的可读性，这个后面会讲到。LogbackLogback 是 Slf4j 的原生实现框架，同样也是出自 Log4j 一个人之手，但拥有比 log4j 更多的优点、特性和更做强的性能，现在基本都用来代替 log4j 成为主流。Logback 的官方地址为什么 Logback 会成为主流？无论从设计上还是实现上，Logback相对log4j而言有了相对多的改进。不过尽管难以一一细数，这里还是列举部分理由为什么选择logback而不是log4j。牢记logback与log4j在概念上面是很相似的，它们都是有同一群开发者建立。所以如果你已经对log4j很熟悉，你也可以很快上手logback。如果你喜欢使用log4j，你也许会迷上使用logback。更快的执行速度基于我们先前在log4j上的工作，logback 重写了内部的实现，在某些特定的场景上面，甚至可以比之前的速度快上10倍。在保证logback的组件更加快速的同时，同时所需的内存更加少。更多请参考《从Log4j迁移到LogBack的理由 》日志框架总结commons-loggin、slf4j 只是一种日志抽象门面，不是具体的日志框架。 log4j、logback 是具体的日志实现框架。 一般首选强烈推荐使用 slf4j + logback。当然也可以使用slf4j + log4j、commons-logging + log4j 这两种日志组合框架。从上图可以看出 slf4j 很强大吧，不但能和各种日志框架对接，还能和日志门面 commons-logging 进行融合。日志级别详解日志的输出都是分级别的，不同的设置不同的场合打印不同的日志。下面拿最普遍用的 Log4j 日志框架来做个日志级别的说明，这个也比较奇全，其他的日志框架也都大同小异。Log4j 的级别类 org.apache.log4j.Level 里面定义了日志级别，日志输出优先级由高到底分别为以下8种。            日志级别      描述                  OFF      关闭：最高级别，不输出日志。              FATAL      致命：输出非常严重的可能会导致应用程序终止的错误。              ERROR      错误：输出错误，但应用还能继续运行。              WARN      警告：输出可能潜在的危险状况。              INFO      信息：输出应用运行过程的详细信息。              DEBUG      调试：输出更细致的对调试应用有用的信息。              TRACE      跟踪：输出更细致的程序运行轨迹。              ALL      所有：输出所有级别信息。      所以，日志优先级别标准顺序为：ALL lt; TRACE lt; DEBUG lt; INFO lt; WARN lt; ERROR lt; FATAL lt; OFF如果日志设置为 L ，一个级别为 P 的输出日志只有当 P gt;= L 时日志才会输出。即如果日志级别 L 设置 INFO，只有 P 的输出级别为 INFO、WARN，后面的日志才会正常输出。具体的输出关系可以参考下图：知道了日志级别，这还只是基础，如何了解打日志的规范，以及如何正确地打日志姿势呢？！打日志的规范准则最开始也说过了，日志不能乱打，不然起不到日志本应该起到的作用不说，还会造成系统的负担。在 BAT、华为一些大公司都是对日志规范有要求的，什么时候该打什么日志都是有规范的。阿里去年发布的《Java 开发手册》，里面有一章节就是关于日志规范的，让我们再来回顾下都有什么内容。下面是阿里的《Java开发手册》终极版日志规约篇。规范有很多，这里就不再一一详述了，完整终极版可以在微信公众号 “Java 技术栈” 中回复 “手册” 获取。这里只想告诉大家，在大公司打日志都是有严格规范的，不是你随便打就行的。阿里是一线互联网公司，所制定的日志规范也都符合我们的要求，很有参考意义，能把阿里这套日志规约普及也真很不错了。项目中该如何正确的打日志？      正确的定义日志 private static final Logger LOG = LoggerFactory.getLogger(this.getClass()); 通常一个类只有一个 LOG 对象，如果有父类可以将 LOG 定义在父类中。 日志变量类型定义为门面接口（如 slf4j 的 Logger），实现类可以是 Log4j、Logback 等日志实现框架，不要把实现类定义为变量类型，否则日志切换不方便，也不符合抽象编程思想。        使用参数化形式占位，[] 进行参数隔离 LOG.debug(Save order with order no：[], and order amount：[]); 这种可读性好，这样一看就知道[]里面是输出的动态参数，用来占位类似绑定变量，而且只有真正准备打印的时候才会处理参数，方便定位问题。 如果日志框架不支持参数化形式，且日志输出时不支持该日志级别时会导致对象冗余创建，浪费内存，此时就需要使用 isXXEnabled 判断，如：    1234if(LOG.isDebugEnabled())    // 如果日志不支持参数化形式，debug又没开启，那字符串拼接就是无用的代码拼接，影响系统性能    logger.debug(Save order with order no： + orderNo + , and order amount： + orderAmount);        至少 debug 级别是需要开启判断的，线上日志级别至少应该是 info 以上的。 这里推荐大家用 SLF4J 的门面接口，可以用参数化形式输出日志，debug 级别也不必用 if 判断，简化代码。        输出不同级别的日志，项目中最常用有日志级别是ERROR、WARN、INFO、DEBUG四种了，这四个都有怎样的应用场景呢。    ERROR（错误） 一般用来记录程序中发生的任何异常错误信息（Throwable），或者是记录业务逻辑出错。 WARN（警告） 一般用来记录一些用户输入参数错误、 INFO（信息） 这个也是平时用的最低的，也是默认的日志级别，用来记录程序运行中的一些有用的信息。如程序运行开始、结束、耗时、重要参数等信息，需要注意有选择性的有意义的输出，到时候自己找问题看一堆日志却找不到关键日志就没意义了。 DEBUG（调试） 这个级别一般记录一些运行中的中间参数信息，只允许在开发环境开启，选择性在测试环境开启。      几个错误的打日志方式      不要使用 System.out.print.. 输出日志的时候只能通过日志框架来输出日志，而不能使用 System.out.print.. 来打印日志，这个只会打印到 tomcat 控制台，而不会记录到日志文件中，不方便管理日志，如果通过服务形式启动把日志丢弃了那更是找不到日志了。        不要使用 e.printStackTrace() 首先来看看它的源码：    123public void printStackTrace()     printStackTrace(System.err);        它其实也是利用 System.err 输出到了 tomcat 控制台。        不要抛出异常后又输出日志 如捕获异常后又抛出了自定义业务异常，此时无需记录错误日志，由最终捕获方进行异常处理。不能又抛出异常，又打印错误日志，不然会造成重复输出日志。    1234567try     // ... catch (Exception e)     // 错误    LOG.error(xxx, e);    throw new RuntimeException();            不要使用具体的日志实现类 InterfaceImpl interface = new InterfaceImpl(); 这段代码大家都看得懂吧？应该面向接口的对象编程，而不是面向实现，这也是软件设计模式的原则，正确的做法应该是。    Interface interface = new InterfaceImpl(); 日志框架里面也是如此，上面也说了，日志有门面接口，有具体实现的实现框架，所以大家不要面向实现编程。        没有输出全部错误信息 看以下代码，这样不会记录详细的堆栈异常信息，只会记录错误基本描述信息，不利于排查问题。    12345678try     // ... catch (Exception e)     // 错误    LOG.error(&#39;XX 发生异常&#39;, e.getMessage());    // 正确    LOG.error(&#39;XX 发生异常&#39;, e);               不要使用错误的日志级别 曾经在线上定位一个问题，同事自信地和我说：明明输出了日志啊，为什么找不到…，后来我去看了下他的代码，是这样的：    123456try     // ... catch (Exception e)     // 错误    LOG.info(XX 发生异常..., e);        大家看出了什么问题吗？用 info 记录 error 日志，日志输出到了 info 日志文件中了，同事拼命地在 error 错误日志文件里面找怎么能找到呢？        不要在千层循环中打印日志 这个是什么意思，如果你的框架使用了性能不高的 Log4j 框架，那就不要在上千个 for 循环中打印日志，这样可能会拖垮你的应用程序，如果你的程序响应时间变慢，那要考虑是不是日志打印的过多了。    123for(int i=0; ilt;2000; i++)    LOG.info(XX);        最好的办法是在循环中记录要点，在循环外面总结打印出来。        禁止在线上环境开启 debug 这是最后一点，也是最重要的一点。 一是因为项目本身 debug 日志太多，二是各种框架中也大量使用 debug 的日志，线上开启 debug 不久就会打满磁盘，影响业务系统的正常运行  "
} ,

{
"title"    : "Java泛型",
"category" : "",
"tags"     : "泛型",
"url"      : "/blog/2020/11/03/Java%E6%B3%9B%E5%9E%8B/",
"date"     : "2020-11-03 00:00:00 +0800",
"content"  : "  转自：  作者： frank909  https://blog.csdn.net/briblue/article/details/76736356泛型，一个孤独的守门者。大家可能会有疑问，我为什么叫做泛型是一个守门者。这其实是我个人的看法而已，我的意思是说泛型没有其看起来那么深不可测，它并不神秘与神奇。泛型是 Java 中一个很小巧的概念，但同时也是一个很容易让人迷惑的知识点，它让人迷惑的地方在于它的许多表现有点违反直觉。文章开始的地方，先给大家奉上一道经典的测试题。1234Listlt;Stringgt; l1 = new ArrayListlt;Stringgt;();Listlt;Integergt; l2 = new ArrayListlt;Integergt;();        System.out.println(l1.getClass() == l2.getClass());请问，上面代码最终结果输出的是什么？不了解泛型的和很熟悉泛型的同学应该能够答出来，而对泛型有所了解，但是了解不深入的同学可能会答错。正确答案是 true。上面的代码中涉及到了泛型，而输出的结果缘由是类型擦除。先好好说说泛型。泛型是什么？泛型的英文是 generics，generic 的意思是通用,而翻译成中文，泛应该意为广泛，型是类型。所以泛型就是能广泛适用的类型。但泛型还有一种较为准确的说法就是为了参数化类型，或者说可以将类型当作参数传递给一个类或者是方法。那么，如何解释类型参数化呢？12345678910public class Cache     Object value;public Object getValue()     return value;public void setValue(Object value)     this.value = value;    假设 Cache 能够存取任何类型的值，于是，我们可以这样使用它。12345Cache cache = new Cache();cache.setValue(134);int value = (int) cache.getValue();cache.setValue(hello);String value1 = (String) cache.getValue();使用的方法也很简单，只要我们做正确的强制转换就好了。但是，泛型却给我们带来了不一样的编程体验。12345678910public class Cachelt;Tgt;     T value;public Object getValue()     return value;public void setValue(T value)     this.value = value;    这就是泛型，它将 value 这个属性的类型也参数化了，这就是所谓的参数化类型。再看它的使用方法。1234567Cachelt;Stringgt; cache1 = new Cachelt;Stringgt;();cache1.setValue(123);String value2 = cache1.getValue();        Cachelt;Integergt; cache2 = new Cachelt;Integergt;();cache2.setValue(456);int value3 = cache2.getValue();最显而易见的好处就是它不再需要对取出来的结果进行强制转换了。但，还有另外一点不同。泛型除了可以将类型参数化外，而参数一旦确定好，如果类似不匹配，编译器就不通过。上面代码显示，无法将一个 String 对象设置到 cache2 中，因为泛型让它只接受 Integer 的类型。所以，综合上面信息，我们可以得到下面的结论。      与普通的 Object 代替一切类型这样简单粗暴而言，泛型使得数据的类别可以像参数一样由外部传递进来。它提供了一种扩展能力。它更符合面向抽象开发的软件编程宗旨。    当具体的类型确定后，泛型又提供了一种类型检测的机制，只有相匹配的数据才能正常的赋值，否则编译器就不通过。所以说，它是一种类型安全检测机制，一定程度上提高了软件的安全性防止出现低级的失误。  泛型提高了程序代码的可读性，不必要等到运行的时候才去强制转换，在定义或者实例化阶段，因为 Cache这个类型显化的效果，程序员能够一目了然猜测出代码要操作的数据类型。下面的文章，我们正常介绍泛型的相关知识。泛型的定义和使用泛型按照使用情况可以分为 3 种。      泛型类    泛型方法  泛型接口泛型类我们可以这样定义一个泛型类。123public class Testlt;Tgt;     T field1;尖括号 lt;gt;中的 T 被称作是类型参数，用于指代任何类型。事实上，T 只是一种习惯性写法，如果你愿意。你可以这样写。123public class Testlt;Hellogt;     Hello field1;但出于规范的目的，Java 还是建议我们用单个大写字母来代表类型参数。常见的如：  T 代表一般的任何类。  E 代表 Element 的意思，或者 Exception 异常的意思。  K 代表 Key 的意思。  V 代表 Value 的意思，通常与 K 一起配合使用。  S 代表 Subtype 的意思，文章后面部分会讲解示意。如果一个类被 的形式定义，那么它就被称为是泛型类。那么对于泛型类怎么样使用呢？12Testlt;Stringgt; test1 = new Testlt;gt;();Testlt;Integergt; test2 = new Testlt;gt;();只要在对泛型类创建实例的时候，在尖括号中赋值相应的类型便是。T 就会被替换成对应的类型，如 String 或者是 Integer。你可以相像一下，当一个泛型类被创建时，内部自动扩展成下面的代码。123public class Testlt;Stringgt;     String field1;当然，泛型类不至接受一个类型参数，它还可以这样接受多个类型参数。123456789101112public class MultiType lt;E,Tgt;    E value1;    T value2;    public E getValue1()    return value1;public T getValue2()    return value2;    泛型方法12345public class Test1     public lt;Tgt; void testMethod(T t)        泛型方法与泛型类稍有不同的地方是，类型参数也就是尖括号那一部分是写在返回值前面的。中的 T 被称为类型参数，而方法中的 T 被称为参数化类型，它不是运行时真正的参数。当然，声明的类型参数，其实也是可以当作返回值的类型的。123public  lt;Tgt; T testMethod1(T t)        return null;泛型类与泛型方法的共存现象12345678public class Test1lt;Tgt;    public  void testMethod(T t)        System.out.println(t.getClass().getName());        public  lt;Tgt; T testMethod1(T t)        return t;    上面代码中，Test1是泛型类，testMethod 是泛型类中的普通方法，而 testMethod1 是一个泛型方法。而泛型类中的类型参数与泛型方法中的类型参数是没有相应的联系的，泛型方法始终以自己定义的类型参数为准。所以，针对上面的代码，我们可以这样编写测试代码。123Test1lt;Stringgt; t = new Test1();t.testMethod(generic);Integer i = t.testMethod1(new Integer(1));泛型类的实际类型参数是 String，而传递给泛型方法的类型参数是 Integer，两者不想干。但是，为了避免混淆，如果在一个泛型类中存在泛型方法，那么两者的类型参数最好不要同名。比如，Test1代码可以更改为这样12345678public class Test1lt;Tgt;      public  void testMethod(T t)        System.out.println(t.getClass().getName());        public  lt;Egt; E testMethod1(E e)        return e;    泛型接口泛型接口和泛型类差不多，所以一笔带过。12public interface Iterablelt;Tgt; 通配符 ？除了用 表示泛型外，还有 lt;?gt;这种形式。？ 被称为通配符。可能有同学会想，已经有了 的形式了，为什么还要引进 lt;?gt;这样的概念呢？123456class Baseclass Sub extends BaseSub sub = new Sub();Base base = sub;        上面代码显示，Base 是 Sub 的父类，它们之间是继承关系，所以 Sub 的实例可以给一个 Base 引用赋值，那么12Listlt;Subgt; lsub = new ArrayListlt;gt;();Listlt;Basegt; lbase = lsub;最后一行代码成立吗？编译会通过吗？答案是否定的。编译器不会让它通过的。Sub 是 Base 的子类，不代表 List和 List有继承关系。但是，在现实编码中，确实有这样的需求，希望泛型能够处理某一范围内的数据类型，比如某个类和它的子类，对此 Java 引入了通配符这个概念。所以，通配符的出现是为了指定泛型中的类型范围。通配符有 3 种形式。  lt;?gt;被称作无限定的通配符。  lt;? extends Tgt;被称作有上限的通配符。  lt;? super Tgt;被称作有下限的通配符。无限定通配符  lt;?gt;无限定通配符经常与容器类配合使用，它其中的 ? 其实代表的是未知类型，所以涉及到 ? 时的操作，一定与具体类型无关。12public void testWildCards(Collectionlt;?gt; collection)上面的代码中，方法内的参数是被无限定通配符修饰的 Collection 对象，它隐略地表达了一个意图或者可以说是限定，那就是 testWidlCards() 这个方法内部无需关注 Collection 中的真实类型，因为它是未知的。所以，你只能调用 Collection 中与类型无关的方法。我们可以看到，当 lt;?gt;存在时，Collection 对象丧失了 add() 方法的功能，编译器不通过。我们再看代码。12Listlt;?gt; wildlist = new ArrayListlt;Stringgt;();wildlist.add(123);// 编译不通过有人说，lt;?gt;提供了只读的功能，也就是它删减了增加具体类型元素的能力，只保留与具体类型无关的功能。它不管装载在这个容器内的元素是什么类型，它只关心元素的数量、容器是否为空？我想这种需求还是很常见的吧。有同学可能会想，lt;?gt;既然作用这么渺小，那么为什么还要引用它呢？ 个人认为，提高了代码的可读性，程序员看到这段代码时，就能够迅速对此建立极简洁的印象，能够快速推断源码作者的意图。extendslt;? extends Tgt;lt;?gt;代表着类型未知，但是我们的确需要对于类型的描述再精确一点，我们希望在一个范围内确定类别，比如类型 A 及 类型 A 的子类都可以。lt;? extends Tgt; 代表类型 T 及 T 的子类。1public void testSub(Collectionlt;? extends Basegt; para) 上面代码中，para 这个 Collection 接受 Base 及 Base 的子类的类型。 但是，它仍然丧失了写操作的能力。也就是说 java para.add(new Sub()); para.add(new Base()); 仍然编译不通过。 没有关系，我们不知道具体类型，但是我们至少清楚了类型的范围。superlt;? super Tgt; 这个和 lt;? extends Tgt;相对应，代表 T 及 T 的超类。1public void testSuper(Collectionlt;? super Subgt; para) lt;? super Tgt;神奇的地方在于，它拥有一定程度的写操作的能力。1234public void testSuper(Collectionlt;? super Subgt; para)     para.add(new Sub());//编译通过     para.add(new Base());//编译不通过 通配符与类型参数的区别 一般而言，通配符能干的事情都可以用类型参数替换。 比如1 public void testWildCards(Collectionlt;?gt; collection)可以被1public lt;Tgt; void test(Collectionlt;Tgt; collection)取代。值得注意的是，如果用泛型方法来取代通配符，那么上面代码中 collection 是能够进行写操作的。只不过要进行强制转换。1234public lt;Tgt; void test(Collectionlt;Tgt; collection)    collection.add((T)new Integer(12));    collection.add((T)123);需要特别注意的是，类型参数适用于参数之间的类别依赖关系，举例说明。1234public class Test2 lt;T,E extends Tgt;    T value1;    E value2;123public lt;D,S extends Dgt; void test(D d,S s)        E 类型是 T 类型的子类，显然这种情况类型参数更适合。有一种情况是，通配符和类型参数一起使用。123public lt;Tgt; void test(T t,Collectionlt;? extends Tgt; collection)    如果一个方法的返回类型依赖于参数的类型，那么通配符也无能为力。123public T test1(T t)    return value1;类型擦除泛型是 Java 1.5 版本才引进的概念，在这之前是没有泛型的概念的，但显然，泛型代码能够很好地和之前版本的代码很好地兼容。这是因为，泛型信息只存在于代码编译阶段，在进入 JVM 之前，与泛型相关的信息会被擦除掉，专业术语叫做类型擦除。通俗地讲，泛型类和普通类在 java 虚拟机内是没有什么特别的地方。回顾文章开始时的那段代码1234Listlt;Stringgt; l1 = new ArrayListlt;Stringgt;();Listlt;Integergt; l2 = new ArrayListlt;Integergt;();        System.out.println(l1.getClass() == l2.getClass());打印的结果为 true 是因为 List和 List在 jvm 中的 Class 都是 `List.class`。泛型信息被擦除了。可能同学会问，那么类型 String 和 Integer 怎么办？答案是泛型转译。123456public class Erasure lt;Tgt;T object;    public Erasure(T object)         this.object = object;    Erasure 是一个泛型类，我们查看它在运行时的状态信息可以通过反射。123Erasurelt;Stringgt; erasure = new Erasurelt;Stringgt;(hello);Class eclz = erasure.getClass();System.out.println(erasure class is:+eclz.getName());打印的结果是1erasure class is:com.frank.test.ErasureClass 的类型仍然是 Erasure 并不是 Erasure这种形式，那我们再看看泛型类中 T 的类型在 jvm 中是什么具体类型。1234Field[] fs = eclz.getDeclaredFields();for ( Field f:fs)     System.out.println(Field name +f.getName()+ type:+f.getType().getName());打印结果是1Field name object type:java.lang.Object那我们可不可以说，泛型类被类型擦除后，相应的类型就被替换成 Object 类型呢？这种说法，不完全正确。我们更改一下代码。12345678public class Erasure lt;T extends Stringgt;//    public class Erasure lt;Tgt;    T object;    public Erasure(T object)         this.object = object;    现在再看测试结果：1Field name object type:java.lang.String我们现在可以下结论了，在泛型类被类型擦除的时候，之前泛型类中的类型参数部分如果没有指定上限，如 则会被转译成普通的 Object 类型，如果指定了上限如 则类型参数就被替换成类型上限。所以，在反射中。123456789101112public class Erasure lt;Tgt;    T object;    public Erasure(T object)         this.object = object;            public void add(T object)            add() 这个方法对应的 Method 的签名应该是 Object.class。12345678Erasurelt;Stringgt; erasure = new Erasurelt;Stringgt;(hello);Class eclz = erasure.getClass();System.out.println(erasure class is:+eclz.getName());Method[] methods = eclz.getDeclaredMethods();for ( Method m:methods )    System.out.println( method:+m.toString());打印结果是1 method:public void com.frank.test.Erasure.add(java.lang.Object)也就是说，如果你要在反射中找到 add 对应的 Method，你应该调用 getDeclaredMethod(“add”,Object.class)否则程序会报错，提示没有这么一个方法，原因就是类型擦除的时候，T 被替换成 Object 类型了。类型擦除带来的局限性类型擦除，是泛型能够与之前的 java 版本代码兼容共存的原因。但也因为类型擦除，它会抹掉很多继承相关的特性，这是它带来的局限性。理解类型擦除有利于我们绕过开发当中可能遇到的雷区，同样理解类型擦除也能让我们绕过泛型本身的一些限制。比如正常情况下，因为泛型的限制，编译器不让最后一行代码编译通过，因为类似不匹配，但是，基于对类型擦除的了解，利用反射，我们可以绕过这个限制。123456public interface Listlt;Egt; extends Collectionlt;Egt;         boolean add(E e);上面是 List 和其中的 add() 方法的源码定义。因为 E 代表任意的类型，所以类型擦除时，add 方法其实等同于1boolean add(Object obj);那么，利用反射，我们绕过编译器去调用 add 方法。123456789101112131415161718192021222324252627282930313233343536373839public class ToolTest     public static void main(String[] args)         Listlt;Integergt; ls = new ArrayListlt;gt;();        ls.add(23);//        ls.add(text);        try             Method method = ls.getClass().getDeclaredMethod(add,Object.class);                                    method.invoke(ls,test);            method.invoke(ls,42.9f);         catch (NoSuchMethodException e)             // TODO Auto-generated catch block            e.printStackTrace();         catch (SecurityException e)             // TODO Auto-generated catch block            e.printStackTrace();         catch (IllegalAccessException e)             // TODO Auto-generated catch block            e.printStackTrace();         catch (IllegalArgumentException e)             // TODO Auto-generated catch block            e.printStackTrace();         catch (InvocationTargetException e)             // TODO Auto-generated catch block            e.printStackTrace();                        for ( Object o: ls)            System.out.println(o);                打印结果是：12323test42.9可以看到，利用类型擦除的原理，用反射的手段就绕过了正常开发中编译器不允许的操作限制。泛型中值得注意的地方泛型类或者泛型方法中，不接受 8 种基本数据类型。所以，你没有办法进行这样的编码。12Listlt;intgt; li = new ArrayListlt;gt;();Listlt;booleangt; li = new ArrayListlt;gt;();需要使用它们对应的包装类。12Listlt;Integergt; li = new ArrayListlt;gt;();Listlt;Booleangt; li1 = new ArrayListlt;gt;();对泛型方法的困惑123public lt;Tgt; T test(T t)    return null;有的同学可能对于连续的两个 T 感到困惑，其实 是为了说明类型参数，是声明,而后面的不带尖括号的 T 是方法的返回值类型。你可以相像一下，如果 test() 这样被调用1test(123);那么实际上相当于1public String test(String t);Java 不能创建具体类型的泛型数组这句话可能难以理解，代码说明。12Listlt;Integergt;[] li2 = new ArrayListlt;Integergt;[];Listlt;Booleangt; li3 = new ArrayListlt;Booleangt;[];这两行代码是无法在编译器中编译通过的。原因还是类型擦除带来的影响。List和 List在 jvm 中等同于List，所有的类型信息都被擦除，程序也无法分辨一个数组中的元素类型具体是 List类型还是 List类型。但是，123Listlt;?gt;[] li3 = new ArrayListlt;?gt;[10];li3[1] = new ArrayListlt;Stringgt;();Listlt;?gt; v = li3[1];借助于无限定通配符却可以，前面讲过 ？代表未知类型，所以它涉及的操作都基本上与类型无关，因此 jvm 不需要针对它对类型作判断，因此它能编译通过，但是，只提供了数组中的元素因为通配符原因，它只能读，不能写。比如，上面的 v 这个局部变量，它只能进行 get() 操作，不能进行 add() 操作，这个在前面通配符的内容小节中已经讲过。泛型，并不神奇我们可以看到，泛型其实并没有什么神奇的地方，泛型代码能做的非泛型代码也能做。而类型擦除，是泛型能够与之前的 java 版本代码兼容共存的原因。可量也正因为类型擦除导致了一些隐患与局限。但，我还是要建议大家使用泛型，如官方文档所说的，如果可以使用泛型的地方，尽量使用泛型。毕竟它抽离了数据类型与代码逻辑，本意是提高程序代码的简洁性和可读性，并提供可能的编译时类型转换安全检测功能。类型擦除不是泛型的全部，但是它却能很好地检测我们对于泛型这个概念的理解程度。我在文章开头将泛型比作是一个守门人，原因就是他本意是好的，守护我们的代码安全，然后在门牌上写着出入的各项规定，及“xxx 禁止出入”的提醒。但是同我们日常所遇到的那些门卫一般，他们古怪偏执，死板守旧，我们可以利用反射基于类型擦除的认识，来绕过泛型中某些限制，现实生活中，也总会有调皮捣蛋者能够基于对门卫们生活作息的规律，选择性地绕开他们的监视，另辟蹊径溜进或者溜出大门，然后扬长而去，剩下守卫者一个孤独的身影。"
} ,

{
"title"    : "dockerfile打成docker上传到linux后运行报sh脚本找不到的错误",
"category" : "",
"tags"     : "docker",
"url"      : "/blog/2020/07/20/dockerfile%E6%89%93%E6%88%90docker%E4%B8%8A%E4%BC%A0%E5%88%B0linux%E5%90%8E%E8%BF%90%E8%A1%8C%E6%8A%A5sh%E8%84%9A%E6%9C%AC%E6%89%BE%E4%B8%8D%E5%88%B0%E7%9A%84%E9%94%99%E8%AF%AF/",
"date"     : "2020-07-20 00:00:00 +0800",
"content"  : "之前遇到一个奇葩错误， 本地dockerfile打成docker上传到linux后(使用的是docker-maven-plugin)，docker run imageID 一直报 ./run.sh not found  的错误。dockerfile如下：1234567891011121314FROM openjdk:8-jdk-alpineRUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&#39; /etc/apk/repositoriesRUN apk update amp;amp; apk upgrade amp;amp; apk add netcat-openbsdRUN echo JAVA_HOMERUN cd /tmp/ amp;amp; #92;    wget http://download.oracle.com/otn-pub/java/jce/8/jce_policy-8.zip --header &#39;Cookie: oraclelicense=accept-securebackup-cookie&#39; amp;amp; #92;    unzip jce_policy-8.zip amp;amp; #92;    rm jce_policy-8.zip amp;amp; #92;    yes |cp -v /tmp/UnlimitedJCEPolicyJDK8/*.jar /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/ADD  project.build.finalName.jar /usr/local/configserver/ADD run.sh run.shRUN chmod +x run.shCMD cd /CMD ./run.shrun.sh明明存在，为什么一直not found。经过一番google，在stackoverflow上找到答案。文件的换行问题！！！windows上使用的是CRLF(#92;r#92;n，回车换行)，linux上使用的是LF(#92;n，换行)！！！修改方法："
} ,

{
"title"    : "shiro源码分析三",
"category" : "",
"tags"     : "shiro",
"url"      : "/blog/2020/04/24/shiro%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%89/",
"date"     : "2020-04-24 00:00:00 +0800",
"content"  : "这一篇主要通过2个流程，一个登录流程和一个登陆成功后的访问流程来看下 subject 的创建过程。因为subject 是 shiro 的核心，搞懂了它的创建过程，基本就搞懂了 shiro 的核心逻辑。(这一部分代码是基于传统表单验证的，不是基于JWT的)      登录流程    从前两篇得知，每次请求 shiro 都会创建一个 subject。我们直接从 shiroFilter 的 doFilterinternal()  开始  step into ，最终进到了 org.apache.shiro.mgt.DefaultSecurityManager#createSubject(org.apache.shiro.subject.SubjectContext)这个方法的大概逻辑是“启发式”的从 SubjectContext 中获取一些数据，用来构造 subject，构造成功后保存 subject。SubjectContext  本质上就是一个Map用来存放跟 subject 相关的一些数据，上下文嘛，差不多多这个意思。我们重点关注2步：org.apache.shiro.mgt.DefaultSecurityManager#resolveSession和 org.apache.shiro.mgt.DefaultSecurityManager#resolvePrincipals一个是解析session，一个是解析 principals。这2个都是 subject 中最重要的东西。此时 SubjectContext 中只有 3个 item，request、response、securityManager先看resolveSession，step into先进入第一个红框getSession() 就是从 context 中找 key 为 org.apache.shiro.subject.support.DefaultSubjectContext.SESSION ，类型为 org.apache.shiro.session.Session 的项，显然，此时context 中没有，返回null。接着从context中找key为org.apache.shiro.subject.support.DefaultSubjectContext.SUBJECT，类型为org.apache.shiro.subject.Subject的项，其实就是看有没有已存在的 subject，有的话直接从里面拿session。显然，此时也没有，返回null。第一个红框部分就结束了，没找到，进入第二个红框部分。它的注释意思是，从context中不能直接解析出session，看看能不能从 sessionManager 中解析。step intogetSessionKey 先从context中找key为org.apache.shiro.subject.support.DefaultSubjectContext.SESSION_ID的项作为sessionId，再加上request和response，构造出一个 WebSessionKey。显然，此时context中不存在sessionId,所以此时构造出来的 WebSessionKey 中的 sessionId 为 null。继续往下走，到了 getSession(key)，step into调用 sessionManager 的 getSession(key)，step into进到了 ServletContainerSessionManager#getSession，然后我们看到了熟悉的一句代码：request.getSession(false)shiro 将 tomcat 的 request 封装了一下，我们继续 step into一路跟进，最终进到了tomcat的源码后面的代码不再继续跟进，它属于servlet容器的范畴很显然，此时 request.getSession(false) 返回null，因为容器中还没有任何 session。那么 DefaultSecurityManager#createSubject(SubjectContext) 中的 resolveSession(context) 就结束了，“启发式”并没有获取到任何有关于session 的东西，不管是从 context 中获取，还是通过request从容器中获取。接下来到了 resolvePrincipals(context)，step intogetPrincipals 跟前面一样，从context获取，查找key为 org.apache.shiro.subject.support.DefaultSubjectContext.PRINCIPALS 的项，显然此时没有。然后它会分别从context中查找key为 AUTHENTICATION_INFO、SUBJECT、SESSION 的项，只要有一个存在，就从找到的对象中解析 principals。此时，上述key在 context 中都还不存在，所以最后返回null。然后我们进入实际的 subject 的创建方法，此时 context 中还是一开始的3个key，并没有“启发式”的获得其他数据。step into ，到了 DefaultWebSubjectFactory#createSubject可以看到最后是new了一个 WebDelegatingSubject 对象，我们重点关注一下其中最关键的 principals、authenticated、session 3个属性。其实它们的逻辑都跟前面的是一致的，先尝试从 context 中查找，有session的话再尝试从session中找。因为是第一次登录，最后的值都是null或false。至此 subject 已经构造完毕，后面的逻辑就跟第一篇讲的一样，因为/login路径配置的是 anon filter，所以直接放行，进入到我们的login方法中。此处就是构造一个UsernamePasswordToken，然后调用 subject 的 login 方法，若没抛出任何异常，说明登录成功。接下来我们看看登录过程做的一些事情，step into再step into当验证成功没有抛出异常后，会执行 createSubject(token, info, subject)，为什么这里又要创建一个 subject 呢？我们先不进去，先退回上一步看看这个创建出来的 subject 被用来干什么可以看到这个新创建出来的 subject 里面有 principals 和 session，用这2个属性来设置之前我们创建的 subject，所以此时的 subject 就变成了已认证的了。（这一部分在第二篇详细讲到。）回到 createSubject(token, info, subject)，看看它还做了些什么，step into可以看到新建了一个空的 SubjectContext，然后填充了 Authenticated、AuthenticationToken、AuthenticationInfo、SecurityManager、Subject 5个item我们知道 context 被用来做 ”启发式“ 的 subject 创建，所以这些东西在后面肯定用得着，继续 step into又回到了熟悉的 DefaultSecurityManager#createSubject(SubjectContext)这一次看看”启发式“的解析 session 和 principals 跟之前会不会不一样。代码就不再跟进了，这里口述一下。resolveSession(context) 先从context中找session，没有；接着找subject，有了，然后调用subject.getSession(false)，但是此时还没有session，返回null。resolvePrincipals(context) 先从context 中 principals，没有；接着找 AuthenticationInfo，有了，然后调用 info.getPrincipals() 得到 principals。然后又进入实际的创建过程 doCreateSubject(context)又重复了一遍上述的过程，最终创建出已认证的 subject此时进入了最后最重要的一块step into进入到了 DefaultSubjectDAO 的 save 方法，若 isSessionStorageEnabled ，则执行 saveToSession(subject)，把当前subject保存到session。这里叉一点，在基于 jwt 的应用中，一般把 isSessionStorageEnabled 设为 false，表示不采用session来保存 subject 。继续 step into可以看到它分为了两块，principals 和 authenticationState先看第一块先拿到 currentPrincipals，再通过 subject.getSession(false) 拿 session，明显此时没有，所以进入了红框逻辑。它的意思是当前请求没有会话，但是已经认证了，怎么办呢？肯定要为它创建一个会话嘛。所以我们看到了 session = subject.getSession() ，这句大家都很熟悉了吧，类似于 request.getSession(true)，这一块底层又是tomcat的东西。创建完session后，将 currentPrincipals 设到 session 里。同理，第二块会将  authenticated 设到 session里，它的值为true。至此，我们的session里就有了 principals 和 authenticated 2个属性，到这里便恍然大悟了，最终shiro是通过这2个属性来维护用户状态的。接下来我们进入第二个流程，验证一下上面说的，登陆成功后访问一个受保护路径看看 subject 的创建过程。完整流程不再贴，直接贴关键不同的部分在从 context 解析 session 的时候，context 中没找到(因为默认一开始只有3个key，上面有讲到)，从request.getSession(false)来找，此时有了。因为这次我们的请求是带着 sessionId 的 cookie 过来的，tomcat 通过这个sessionId 找到了关联的 session。我们可以再看下这个session里有什么东西，有没有上面我们设置进去的2个属性果然有！resolveSession(context) 这一步成功解析出了session，并把session设到了 SubjectContext 中，用于后续的”启发式“创建。同理， resolvePrincipals(context) 也成功解析到了，它是从session中解析出来的。最后创建出了一个已认证的 subject，所以简单来说，Shiro 是基于 filter，然后代理出自己的 filter 链，为每个请求创建一个 subject，基于session 来维护 会话状态。"
} ,

{
"title"    : "shiro源码分析二",
"category" : "",
"tags"     : "shiro",
"url"      : "/blog/2020/04/23/shiro%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%BA%8C/",
"date"     : "2020-04-23 00:00:00 +0800",
"content"  : "现在有个问题，当token认证通过以后，subject.isAuthenticated() = true 并且 subject.principal() 不为null，这一步操作是在哪里做的？回到源代码 JWTFilter 中，当我们带token访问时进行到这一步跟进去它就是把token取出来，构造出JWToken，然后调用subject.login(token) 方法进行登录。此时我们拿到的subject 是在 AbstractShiroFilter 中构造出来的subject，它的 isAuthenticated 为 false，principal 为null。跟进去到了 DelegatingSubject 的 login() 方法。此方法会先调用 securityManager 的 login() 方法进行登录，大概逻辑是调用我们自定义的 realm 中的 doGetAuthenticationInfo() 方法拿到AuthenticationInfo，与传进来的token中进行比较，相同则说明登录成功，其实也就是密码比对过程。若登录过程没有抛出任何异常，则代码继续往下执行，会看到上图中第三个红框部分。把当前对象也就是subject的pricipals 和 authenticated 进行了设值。至此，在该请求流程后面的任意时刻调用 subject.isAuthenticated() 都为true，subject.principals() 都不为null，即代表当前subject 已认证通过！"
} ,

{
"title"    : "shiro源码分析一",
"category" : "",
"tags"     : "shiro",
"url"      : "/blog/2020/04/23/shiro%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%E4%B8%80/",
"date"     : "2020-04-23 00:00:00 +0800",
"content"  : "shiro本质上是基于一系列filter的，根据不同的路径进入不同的filter，进行不同的逻辑处理，并决定是否要继续执行下一个filter。因此我们就可以控制整个应用程序的流转。（默认的在 DefaultFilter 中共12个）下面简单理一下shiro的执行逻辑：      因为shiro是基于filter的，我们先理一下它的filter的类结构。    在ShiroConfig中都会配置 ShiroFilterFactoryBean ，它是一个工厂bean，返回 的是SpringShiroFilter 。filter中最重要的自然是 doFilter() 方法，在 SpringShiroFilter 中查找这个方法，最终定位到 OncePerRequestFilter 。这个类我们可以把它当作整个shiro处理流程的基类。        我们先不加入自定义filter，用它自带的authc filter 来跟一下源码  123    Maplt;String, Stringgt; map = new LinkedHashMaplt;gt;();    map.put(/**, authc);    factory.setFilterChainDefinitionMap(map);再定义一个简单的api123456789GetMapping(/article)    public InvokeResultlt;Stringgt; article()         Subject subject = SecurityUtils.getSubject();        if (subject.isAuthenticated())             return InvokeResult.success(You are already logged in, null);         else             return InvokeResult.success(You are guest, null);            接下来访问/article，自然进入到了之前说过的 OncePerRequestFilter 中的 doFilter() 方法可以看到当前的对象是 ShiroFilterFactoryBean 中的 SpringShiroFilter。接着往下走，到了 doFilterInternal() 方法step into ,继续，进入到了 AbstractShiroFilter 中的 doFilterInternal() 。OncePerRequestFilter 有2个子类，一个是 AbstractShiroFilter，一个是 AdviceFilter。记住这个继承结构，很重要。简单来说 AbstractShiroFilter 用于执行初始化、配置、构造subject等事情，AdviceFilter 通过 preHandle()、postHandle() 方法执行真正的filter逻辑，并决定要不要继续执行下一个filter。继续往下走：可以看到先创建了一个subject，这个subject将会贯穿这个request的整个生命周期，每次请求进来shiro都会创建一个subject，具体创建过程后面再讲。接下来执行过滤链，也就是 executeChain() 方法。step into：第一步先获取过滤链，我们先看一下获取到的是什么东西获取到的是一个 ProxiedFilterChain，从名字可以看出来它是一个代理过滤链。它代理的是什么呢？从它的成员属性来看，代理的是原来的过滤链(orig)，再加上了一个filterList，这个filterList中看到了我们熟悉的 authc 字眼。所以这一块的作用就是将请求从容器中的 filter 代理到了 shiro 中的filter，接下来就可以执行我们的认证、授权逻辑了。  接着上一步，执行到了authc过滤器，也就是 FormAuthenticationFilter ，FormAuthenticationFilter 继承于 OncePerRequestFilter 下的 AdviceFilter，回想一下上面讲的继承结构，是不是感觉很清晰了。同样，执行 FormAuthenticationFilter 中的 doFilter() 方法，由于它本身没有定义，所以我们又回到了 OncePerRequestFilter 中的 doFilter() ，然后又到了 doFilterInternal()step into，由于这次是 FormAuthenticationFilter，所以这次的 doFilterInternal 进入到了 AdviceFilter中，记住这个类的继承结构，很重要。接着往下走，可以看到有preHandle、postHandle、continueChain方法，所以可以大概猜测到它的作用是执行一些业务逻辑来决定要不要继续执行filter过滤链。举个例子，反应到前台就是当没有认证时就不继续执行过滤链，跳转到登录页。接着往下走，我们step into preHandle() 方法进入到了 PathMatchingFilter 类中的 preHandle，PathMatchingFilter 是AdviceFilter 的直接子类。可以看一下 FormAuthenticationFilter 的 类图：PathMatchingFilter 顾名思义，就是根据当前请求的路径来判断是否要执行这个filter。如果没有匹配到，返回true，继续执行下一个filter。这里由于我们之前定义了 /**=authc，即所有路径都要过authc，所以这里匹配上了。进入到了 isFilterChainContinued() 方法到了 onPreHandle() 方法，这个方法在 AccessControlFilter 中实现，多回看上面的 FormAuthenticationFilter 类图。可以看到它的执行逻辑是先执行 isAccessAllowed()，为true，直接返回，说明该请求可以被放行，然后继续执行下一个filter。如果为false，则执行后面的onAccessDenied() 方法。接着step into isAccessAllowed()，到了 AuthenticatingFilter 中的 isAccessAllowed()，随时回看类结构图。它的执行逻辑是先执行父类的isAccessAllowed()，如果为false，再判断是否是登录请求，isPermissive() 是否满足。我们来看父类的isAccessAllowed()，也就是 AuthenticationFilter ，可以看到它的逻辑就是判断subject有没有被认证，subject的principal是否存在。subject来自于之前 AbstractShiroFilter 中创建的，不记得的回看上面。很明显，这里没有认证，返回false。所以我们进入到了 onAccessDenied()。onAccessDenied() 在 FormAuthenticationFilter 中被重写了：它的逻辑是先判断是不是登录请求，如果不是则保存当前请求(用于登陆成功后的跳转)然后跳转到登陆地址；如果是登录请求并且是post方法，则执行executeLogin() 方法，如果不是post方法，直接返回true，这时用户看到的是登陆页面(登陆地址+get请求嘛)。整个流程差不多就是这样子。      接下来我们加入jwt    首先创建一个JWTFilter，继承自 BasicHttpAuthenticationFilter 。看一下类结构图。  根据上面的流程，我们只要重写 isAccessAllowed() 方法控制是否允许通过，重写 onAccessDenied() 方法 控制不允许通过后的逻辑即可。先上代码：这里  isAccessAllowed() 直接返回false，以执行 onAccessDenied() 方法。在 onAccessDenied() 中，先判断是否是登录尝试，即请求头中是否带 Authorization 头如果请求头中是否带 Authorization 头，执行 executeLogin() 方法，会在自定义的Realm中对token进行校验。校验通过，返回true，接口得以正常访问。校验不通过，捕获抛出的异常，通过  sendResponse(response, e.getMessage()) 发送到客户端。如果不是登录请求，即请求头中没有带 Authorization 头，直接返回 false，并  sendResponse(response, “请携带token访问！”)。因为在前后端分离中，每次请求都必须带上token，每次请求都必须对token进行校验，就相当于每次请求都要进行一次登录操作，这样理解就可以了。"
} ,

{
"title"    : "log4j2打印彩色日志",
"category" : "",
"tags"     : "log",
"url"      : "/blog/2020/04/02/log4j2%E6%89%93%E5%8D%B0%E5%BD%A9%E8%89%B2%E6%97%A5%E5%BF%97/",
"date"     : "2020-04-02 00:00:00 +0800",
"content"  : "先贴一份简单的log4j2配置：使用 highlight 即可打印出彩色日志，但是自log4j 2.10版本以后该功能默认是关闭的，所以要给它打开。在 VM options 中添加 -Dlog4j.skipJansi = false即可。详见官方文档具体使用方法和默认值在 https://logging.apache.org/log4j/2.x/manual/layouts.html 中搜索 highlight 即可。如果要在独立控制台中启用彩色日志，即使用java -jar的方式也是可以的，只需要加入jansi包。12345lt;dependencygt;   lt;groupIdgt;org.fusesource.jansilt;/groupIdgt;   lt;artifactIdgt;jansilt;/artifactIdgt;   lt;versiongt;1.18lt;/versiongt;lt;/dependencygt;然后使用 java -Dlog4j.skipJansi=false -jar xxxx.jar 启动即可。"
} ,

{
"title"    : "实现一个类似纸张堆叠的效果",
"category" : "",
"tags"     : "css",
"url"      : "/blog/2020/01/29/%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%B1%BB%E4%BC%BC%E7%BA%B8%E5%BC%A0%E5%A0%86%E5%8F%A0%E7%9A%84%E6%95%88%E6%9E%9C/",
"date"     : "2020-01-29 00:00:00 +0800",
"content"  : "效果图如下：基本原理就是利用box-shadow。一般想到的是写2个baox-shadow，但出来的效果不好看，会导致每张纸的分界线不清晰。所以写2个阴影不透明的box-shadow 加上 3个阴影透明的box-shadow(有3条边界线)，总共5个。另外box-shadow的第4个值为阴影大小，可以设为负值，表示阴影缩小，这样就能制造出逐渐变窄的效果。代码如下：123456box-shadow:      0 1px 1px rgba(0, 0, 0, 0.2),      0 8px 0 -3px #f6f6f6,      0 9px 1px -3px rgba(0, 0, 0, 0.2),      0 16px 0 -6px #f6f6f6,      0 17px 2px -6px rgba(0, 0, 0, 0.2);"
} ,

{
"title"    : "一个密码验证的正则表达式",
"category" : "",
"tags"     : "正则",
"url"      : "/blog/2019/12/17/%E4%B8%80%E4%B8%AA%E5%AF%86%E7%A0%81%E9%AA%8C%E8%AF%81%E7%9A%84%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/",
"date"     : "2019-12-17 00:00:00 +0800",
"content"  : "要求：6-20个字符，字母、数字和特殊符号至少包含2种，不包括空格表达式：1/^(?![#92;d]+)(?![a-zA-Z]+)(?![^#92;da-zA-Z#92;s]+)#92;S6,20/说明：  ^ 表示行的起始位置  (?![#92;d]+)   不能全是数字  (?![a-zA-Z]+)    不能全是字母  (?![^#92;da-zA-Z#92;s]+)  不能全是特殊字符  #92;S 只能是非空白字符  6,20 长度6-20"
} ,

{
"title"    : "nodejs中graphql的服务端和客户端实现",
"category" : "",
"tags"     : "nodejs, graphql",
"url"      : "/blog/2019/12/10/nodejs%E4%B8%ADgraphql%E7%9A%84%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%9E%E7%8E%B0/",
"date"     : "2019-12-10 00:00:00 +0800",
"content"  : "先简单介绍一下GraphQL。GraphQL 是一门出自Facebook，用于api的查询语言，被称作是Restful的替代品，已经有越来越多的公司和系统使用GraphQL来代替Restful。它的几个主要特点是：  只返回你想要的数据。传统Restful中返回的是对象的所有字段，而往往我们需要的只是其中几个字段，这样无疑造成了很大的带宽浪费。GraphQL中由你定义你的查询请求，想要什么就只给你返回什么，所以GraphQL也被称作是由前端自由定义的查询语言。  一次请求中就可以获取多个资源，被称作数据聚合。传统Restful中经常会先发起一个请求获取某个资源，再根据这个资源的某个字段请求其他资源，最后由前端聚合成最终想要的数据。而GraphQL在一次请求中就可以拿到所有你想要的资源，并且遵循第一个特点，只包括你想要的字段。由于如今前后端分离越来越普遍，前端对于api的要求也越来越灵活和复杂，GraphQL正是下一代的API技术。服务端实现这里采用了Nest的Node.js框架。在nest中实现GraphQL服务端非常方便。它的官方文档也写得非常清楚。这里不再作说明。推荐采用代码优先的方式，我们只需使用它提供的各种装饰器即可生成相应的 GraphQL 架构。说白了就是各种注解。客户端这是本篇文章的重点。包括 GraphQL 的 Query、Mutations、Subscriptions 的 3 种操作。客户端使用的是 Apollo Client。它和各种UI框架作了很好的集成，如React 和 Vue ，可以直接将查询和变更操作绑定到组件上。但我们也可以在 nodejs 环境种单独使用，它其中的 Apollo-Link 模块就提供了这种方式。1、下面是针对 Query、Mutations 的示例代码：12345678910111213141516171819202122232425262728293031323334353637383940414243const  execute, makePromise  = require(&#39;apollo-link&#39;);const  HttpLink  = require(&#39;apollo-link-http&#39;);const gql = require(&#39;graphql-tag&#39;);const fetch = require(&#39;node-fetch&#39;);const query = gql`query  photos id,name,isPublished`;const mutation = gql`mutation(input:NewPhotoInput!)     addPhoto(newPhotoData:input)         id,        name,        isPublished    `;const variables =   &#39;input&#39;:     &#39;name&#39;: &#39;安东尼&#39;,    &#39;description&#39;: &#39;瓜哥&#39;,  ,;const uri = &#39;http://localhost:3000/graphql&#39;;const link = new HttpLink( uri, fetch );const operation =   query: mutation,  variables, //optional  // operationName: , //optional  // context: , //optional  // extensions: , //optional;// execute returns an Observable so it can be subscribed toexecute(link, operation).subscribe(  next: res =gt; console.log(res.data),  error: error =gt; console.log(`received error error`),  complete: () =gt; console.log(`complete`),);// For single execution operations, a Promise can be used// makePromise(execute(link, operation))//   .then(res =gt; console.log(res.data))//   .catch(error =gt; console.log(`received error error`));具体细节参考官方文档需要注意的一点是需要安装 node-fetch，在创建HttpLink的时候传递到它的选项对象中去。execute 和 makePromise 方法选一个就行。2、对于Subscriptions 操作就不能使用上面的 HttpLink 了。因为是订阅操作嘛，所以得使用 Websocket 连接。123456789101112131415161718192021const  WebSocketLink  = require(&#39;apollo-link-ws&#39;);const ws = require(&#39;ws&#39;);const  SubscriptionClient  = require(&#39;subscriptions-transport-ws&#39;);const  execute, makePromise  = require(&#39;apollo-link&#39;);const gql = require(&#39;graphql-tag&#39;);const GRAPHQL_ENDPOINT = &#39;ws://localhost:3000/graphql&#39;;const client = new SubscriptionClient(GRAPHQL_ENDPOINT,   reconnect: true,, ws);const link = new WebSocketLink(client);const operation =   query: gql`subscription photoAdded id,name,isPublished`,;execute(link, operation).subscribe(  next: res =gt; console.log(res.data),);不同之处在于使用的是 subscriptions-transport-ws 模块中的 SubscriptionClient  作为客户端，表示建立的是 Websocket 连接。另外建立的连接也不是 HttpLink，而是 apollo-link-ws 中的 WebSocketLink 。具体细节参考"
} ,

{
"title"    : "openresty使用uuid",
"category" : "",
"tags"     : "nginx",
"url"      : "/blog/2019/12/02/openresty%E4%BD%BF%E7%94%A8uuid/",
"date"     : "2019-12-02 00:00:00 +0800",
"content"  : "主要是使用到了 resty.jit-uuid 这个模块，这个模块并没有集成到 OpenResty 中，可以直接从 github 上下载 jit-uuid.lua 文件，放到 OpenResty 的安装目录下的 lualib/resty  目录里。"
} ,

{
"title"    : "记一次 centos7 + vue + nginx + uwsgi + django-rest + mysql 的部署过程",
"category" : "",
"tags"     : "python",
"url"      : "/blog/2019/12/01/%E8%AE%B0%E4%B8%80%E6%AC%A1-centos7-+-vue-+-nginx-+-uwsgi-+-django-rest-+-mysql-%E7%9A%84%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B/",
"date"     : "2019-12-01 00:00:00 +0800",
"content"  : "弄完了感觉也没那么复杂，但的确花了我2天的时间，主要是因为python版本和虚拟环境的问题。关键的关键是要在虚拟环境中启动uwsgi，它才能找到各种依赖包。安装python3因为centos7预装的是python2，但现在的程序都用的是pyhton3，所以要装上。下载python3.7.4源码包：1wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tgz解压缩：1tar -zxvf Python-3.7.4.tgz进入解压后的目录，执行下面命令手动编译：12./configure prefix=/usr/local/python3 make amp;amp; make install添加软链接：1ln -s /usr/local/python3/bin/python3.7 /usr/bin/python3python3自带了pip3，所以同样为pip3添加软链接：1ln -s /usr/local/python3/bin/pip3.7 /usr/bin/pip3最后python3 -V 和 pip3 -V检查版本对不对这样的话系统中就存在了两个版本的python和pip，想用哪个就用哪个。安装uwsgi注意得使用pip3安装，这样对应的版本才是python3。执行命令 pip3 install uwsgi ，会被安装到python3对应的lib/site-packages目录下。uwsgi –version 检查版本。创建虚拟环境这里我没有使用 virtualenv 命令，直接使用 python3 -m venv env 命令。从django项目中导出requirement.txt。激活虚拟环境：1source env/bin/activate安装依赖：1pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple -r requirement.txt。配置uwsgi配置文件1vim uwsgi.ini1234567socket = 127.0.0.1:3031chdir = /home/head/uwb/wsgi-file = uwb/wsgi.pyhome = /home/head/env/processes = 4threads = 2stats = 127.0.0.1:9191home 指向刚刚创建的虚拟环境目录，这很重要，否则会报各种找不到module的错误。其他配置项就不解释了，自行查阅。配置nginx安装过程就不讲了。首先配置前端代码。在Vue项目中使用 npm run build 命令生成dist目录，上传至nginx目录，添加如下配置：1234 location /             root   dist;            index  index.html;        接下来配置接口访问，也就是访问django应用。为了与静态资源区分开，将所有的接口访问都加上了前缀/api。我们不需改动我们的后端django代码，在nginx和uwsgi中配置可以达到这一效果。配置如下：12345 location /api         uwsgi_param SCRIPT_NAME /api;        include  uwsgi_params;        uwsgi_pass 127.0.0.1:3031;    这里 uwsgi_pass 和 uwsgi.ini 配置文件中的socket配置项一致，这就是nginx和uwsgi通讯的端口。同时在uwsgi.ini 配置文件中加入一行：route-run = fixpathinfo: ， 注意最后是冒号，表示传过来的请求都有一个固定前缀。启动uwsgi当然要先启动nginx、mysql。最好还是在虚拟环境中启动，避免出现一些莫名其妙的import error。执行命令：1uwsgi uwsgi.ini如果出现 mysql_config not found ，再安装以下东西：12sudo yum install mysql-devel gcc gcc-devel python-develsudo easy_install mysql-pythonOK，大功告成！"
} ,

{
"title"    : "Django 数据库迁移到MySQL",
"category" : "",
"tags"     : "python",
"url"      : "/blog/2019/11/28/Django-%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%81%E7%A7%BB%E5%88%B0MySQL/",
"date"     : "2019-11-28 00:00:00 +0800",
"content"  : "默认Django数据库采用的是sqlite3，想迁移到mysql数据库。创建Mysql数据库这没啥好说的，肯定你要先有个数据库吧。更改Django settings配置，修改为使用MySQL数据库123456789101112DATABASES =     &#39;default&#39;:         # &#39;ENGINE&#39;: &#39;django.db.backends.sqlite3&#39;,        # &#39;NAME&#39;: os.path.join(BASE_DIR, &#39;db.sqlite3&#39;),        &#39;ENGINE&#39;: &#39;django.db.backends.mysql&#39;,        &#39;NAME&#39;: &#39;xx&#39;,        &#39;USER&#39;: &#39;xx&#39;,        &#39;PASSWORD&#39;: &#39;xx&#39;,        &#39;HOST&#39;: &#39;xx&#39;,        &#39;PORT&#39;: &#39;xx&#39;    ​安装mysqlclient1pip install mysqlclient执行迁移命令这一步会把你应用 migrations 目录下的所有迁移文件应用到新的数据库中。12python manager.py makemigrationspython manager.py migrate导入数据上一步只是在新的数据库中生成了表结构，还没有数据。先把settings改为原来的配置，这样才能导出原来的数据。然后执行 python manager.py dumpdata gt; data.json      命令。再把settings改为mysql，执行 python manager.py loaddata data.json   命令，这样就完成了数据的迁移。"
} ,

{
"title"    : "axios跨域上传图片到openresty",
"category" : "",
"tags"     : "nginx",
"url"      : "/blog/2019/11/15/axios%E8%B7%A8%E5%9F%9F%E4%B8%8A%E4%BC%A0%E5%9B%BE%E7%89%87%E5%88%B0openresty/",
"date"     : "2019-11-15 00:00:00 +0800",
"content"  : "什么是跨域同源：当协议、域名、端口一致的时候2个域名是同源的。比如 aaa.cn 和 aaa.cn/xx/xxx 是同源的，但和 bbb.cn 就不是同源的。为什么要设置同源策略限制？假如你登陆了一个银行网站，在没有退出的情况下登录了另一个网站，这个网站悄悄的携带了你银行网站的cookie对银行网站发起了请求，这样就能冒充你做所有的操作，因为银行网站并不知道到底是不是你发起的请求，它看到了你的cookie就认为是你了。所以要有这个限制。这也是CSRF(cross site request forgery) 跨站请求伪造攻击。CORS(cross origin resource sharing) 跨域资源共享这是一个w3c标准，用于控制跨域请求。它定义了一些headers：  Access-Control-Allow-Origin​        响应首部中可以携带这个头部表示服务器允许哪些域可以访问该资源，其语法如下：​        Access-Control-Allow-Origin: lt;origingt; | *​        其中，origin 参数的值指定了允许访问该资源的外域 URI。对于不需要携带身份凭证的请求，服务器可以指定该字段的值为通配符，表示允许来自所有域的请求。  Access-Control-Allow-Methods​        该首部字段用于预检请求的响应，指明实际请求所允许使用的HTTP方法。其语法如下：​        Access-Control-Allow-Methods: lt;methodgt;[, lt;methodgt;]*  Access-Control-Allow-Headers​        该首部字段用于预检请求的响应。指明了实际请求中允许携带的首部字段。其语法如下：​        Access-Control-Allow-Headers: lt;field-namegt;[, lt;field-namegt;]*       我们就可以控制服务器上哪些资源可以被什么样的跨域请求访问，以达到跨域资源共享的目的。接下来进入代码部分axios部分：123456789101112var formData = new FormData()      var  action, onError, onSuccess, onProgress, filename, file  = data      formData.append(filename, file)      originalAxios.post(action, formData,         onUploadProgress: ( total, loaded ) =gt;           // 上传进度输出          onProgress( percent: Number(Math.round(loaded / total * 100).toFixed(2)) , file)              ).then(res =gt;         // 上传成功，触发handleChange。file的status会被设为done，同时将res.data赋给file.response        onSuccess(res.data, file)      ).catch(onError)这里用到了formData，axios的onUploadProgress(上传进度)。问题是在发post请求之前浏览器会自动先发一个options请求，用于确定服务器是否能接受这个请求，可以的话再发第二个真正的post请求。但是openresty中没有处理options请求的逻辑，会导致请求失败。所以需要在配置文件中加入以下这段代码：12345678910 location /upload             if (request_method = OPTIONS)               add_header Access-Control-Allow-Origin *;               add_header Access-Control-Allow-Method POST;               return 200;                        add_header Access-Control-Allow-Origin *;            default_type &#39;application/json&#39;;            content_by_lua_file lua/myupload.lua;         这里对options请求返回的响应头中加入了以上提到过的COSR头部：Access-Control-Allow-Origin、Access-Control-Allow-Method，然后返回200。在第二个post请求中用lua-resty-upload来真正处理上传，这里就不细表。"
} ,

{
"title"    : "flexbox改写的一个响应式网页布局实例",
"category" : "",
"tags"     : "css",
"url"      : "/blog/2019/09/16/flexbox%E6%94%B9%E5%86%99%E7%9A%84%E4%B8%80%E4%B8%AA%E5%93%8D%E5%BA%94%E5%BC%8F%E7%BD%91%E9%A1%B5%E5%B8%83%E5%B1%80%E5%AE%9E%E4%BE%8B/",
"date"     : "2019-09-16 00:00:00 +0800",
"content"  : "123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181lt;!DOCTYPE htmlgt;lt;!--[if lt IE 7]gt;      lt;html class=no-js lt-ie9 lt-ie8 lt-ie7gt; lt;![endif]--gt;lt;!--[if IE 7]gt;         lt;html class=no-js lt-ie9 lt-ie8gt; lt;![endif]--gt;lt;!--[if IE 8]gt;         lt;html class=no-js lt-ie9gt; lt;![endif]--gt;lt;!--[if gt IE 8]gt;lt;!--gt;lt;html class=no-jsgt;  lt;!--lt;![endif]--gt;  lt;headgt;    lt;meta charset=utf-8 /gt;    lt;meta http-equiv=X-UA-Compatible content=IE=edge /gt;    lt;titlegt;弹性盒子lt;/titlegt;    lt;meta name=description content= /gt;    lt;meta name=viewport content=width=device-width, initial-scale=1 /gt;    lt;link rel=stylesheet href= /gt;    lt;stylegt;      body         margin: 0;        background: #f1f1f1;            *         box-sizing: border-box;            .container         text-align: center;        display: flex;        flex-direction: column;        margin: 15px;            .header         background: #ffffff;            .header h1         font-size: 50px;            .header ul         list-style-type: none;        display: flex;        justify-content: space-between;        background: #333;        margin: 0;        padding: 0;            .header ul li         flex: 1;        display: flex;            .header ul li a         flex: 1;        text-decoration: none;        color: white;        padding: 15px;            .header ul li a:hover         background: #ddd;        color: black;            .body         display: flex;        align-items: flex-start;        text-align: center;        margin-top: 20px;        text-align: left;            .content         flex: 3.5;            .article         background: #ffffff;        padding: 20px;        margin-bottom: 20px;            .fakeImg         height: 200px;        background: #aaa;        padding: 20px;            .sider         flex: 1;        margin-left: 20px;            .sider gt; *         background: #ffffff;        margin-bottom: 15px;        padding: 20px;            .fakeImg2         height: 100px;        background: #aaa;        padding: 20px;            .footer         background: #ddd;        padding: 20px;            media all and (max-width: 800px)         .body           flex-direction: column;                .sider           width: 100;          margin-left: 0;                  lt;/stylegt;  lt;/headgt;  lt;bodygt;    lt;!--[if lt IE 7]gt;      lt;p class=browsehappygt;        You are using an lt;stronggt;outdatedlt;/stronggt; browser. Please        lt;a href=#gt;upgrade your browserlt;/agt; to improve your experience.      lt;/pgt;    lt;![endif]--gt;    lt;div class=containergt;      lt;div class=headergt;        lt;divgt;          lt;h1gt;我的网页lt;/h1gt;          lt;pgt;重置浏览器大小看效果lt;/pgt;        lt;/divgt;        lt;ulgt;          lt;div style=display: flexgt;            lt;ligt;lt;a href=#gt;链接lt;/agt;lt;/ligt;            lt;ligt;lt;a href=#gt;链接lt;/agt;lt;/ligt;            lt;ligt;lt;a href=#gt;链接lt;/agt;lt;/ligt;          lt;/divgt;          lt;divgt;            lt;ligt;lt;a href=#gt;链接lt;/agt;lt;/ligt;          lt;/divgt;        lt;/ulgt;      lt;/divgt;       lt;div class=bodygt;        lt;div class=contentgt;          lt;div class=articlegt;            lt;h2gt;文章标题lt;/h2gt;            lt;h5gt;2019 年 4 月 17日lt;/h5gt;            lt;div class=fakeImggt;图片lt;/divgt;            lt;pgt;一些文本. . .lt;/pgt;            lt;pgt;              菜鸟教程 - 学的不仅是技术，更是梦想！菜鸟教程 -              学的不仅是技术，更是梦想！菜鸟教程 -              学的不仅是技术，更是梦想！菜鸟教程 - 学的不仅是技术，更是梦想！            lt;/pgt;          lt;/divgt;          lt;div class=articlegt;            lt;h2gt;文章标题lt;/h2gt;            lt;h5gt;2019 年 4 月 17日lt;/h5gt;            lt;div class=fakeImggt;图片lt;/divgt;            lt;pgt;一些文本. . .lt;/pgt;            lt;pgt;              菜鸟教程 - 学的不仅是技术，更是梦想！菜鸟教程 -              学的不仅是技术，更是梦想！菜鸟教程 -              学的不仅是技术，更是梦想！菜鸟教程 - 学的不仅是技术，更是梦想！            lt;/pgt;          lt;/divgt;        lt;/divgt;         lt;div class=sidergt;          lt;divgt;            lt;h3 style=margin-top: 0gt;关于我lt;/h3gt;            lt;div class=fakeImg2gt;图片lt;/divgt;            lt;pgt;关于我的一些信息..lt;/pgt;          lt;/divgt;          lt;divgt;            lt;h3gt;热门文章lt;/h3gt;            lt;div class=fakeImg2gt;图片lt;/divgt;            lt;div class=fakeImg2gt;图片lt;/divgt;            lt;div class=fakeImg2gt;图片lt;/divgt;          lt;/divgt;          lt;divgt;            lt;h3gt;关注我lt;/h3gt;            lt;pgt;一些文本..lt;/pgt;          lt;/divgt;        lt;/divgt;      lt;/divgt;       lt;div class=footergt;lt;h2gt;底部区域lt;/h2gt;lt;/divgt;    lt;/divgt;     lt;script src= async defergt;lt;/scriptgt;  lt;/bodygt;lt;/htmlgt;整体效果如下，而且当屏幕宽度小于800px时右侧边栏变成垂直上下显示。"
} ,

{
"title"    : "网页布局样式小总结",
"category" : "",
"tags"     : "css",
"url"      : "/blog/2019/09/15/%E7%BD%91%E9%A1%B5%E5%B8%83%E5%B1%80%E6%A0%B7%E5%BC%8F%E5%B0%8F%E6%80%BB%E7%BB%93/",
"date"     : "2019-09-15 00:00:00 +0800",
"content"  : "      浮动：元素向左或右移动，直到碰到父容器边框或者下一个浮动元素        清除浮动：因为浮动元素会脱离文档流，所以会导致页面元素的重叠。而且如果一个父元素只包含浮动元素，会导致父元素高度崩塌（高度为0），比如ul内的所有li元素都浮动时，ul高度会变为0。解决办法是在ul上加上 overflow:hidden，具体原理这里不详述。        另外一种方式是使用 clear:both。具体做法是添加伪类 ，而且这个样式必须加在浮动元素层的最外层    12345.clearfix::after    clear:both;    content:&#39;&#39;,    display:table        表示在clearfix类元素后添加一个空的元素，该元素使用table块显示，并且不与浮动元素相邻，以达到清除浮动的效果。        inline：设置为行内显示。此时不能设置元素的width和height，且margin、padding的top和bottom都不能设置（设置了也无效），父元素的高度完全是由子元素的内容撑起来的。        box：设置为块显示。所有的边距和高度、宽度都能设置。宽度未设置时默认为100，即填满父容器。常用在li下的a标签上，可以让a标签充满整个li，这样的话只要在a上设置样式即可。        inline-box：与inline类似，但是所有的边距和高度、宽度都能设置。所以一般用inline-box。        box-sizing：css3新增， 默认为content-box，此时元素的实际宽高 = 元素内容宽高 + 内外边距 + border。        border-box：元素的实际宽高 = 设置的宽高。 元素实际内容的大小会自动用设置的宽高 - 内边距 - border 得到。(注意：不包括margin)。这种方式在布局时非常常用，比如将2个div放在一行内。  "
} ,

{
"title"    : "SpringMVC参数映射原理",
"category" : "",
"tags"     : "Spring",
"url"      : "/blog/2019/08/01/SpringMVC%E5%8F%82%E6%95%B0%E6%98%A0%E5%B0%84%E5%8E%9F%E7%90%86/",
"date"     : "2019-08-01 00:00:00 +0800",
"content"  : "方法参数带 RequestBody 注解1234 RequestMapping(/testRb)     public Person testRb(RequestBody Person p)          return p;     贴上 RequestBody 的说明：  Annotation indicating a method parameter should be bound to the body of the web request. The body of the request is passed through an HttpMessageConverter to resolve the method argument depending on the content type of the request. Optionally, automatic validation can be applied by annotating the argument with Valid.Supported for annotated handler methods.翻译一下：      该注解指示方法参数应通过请求的body来绑定；    根据请求头的 content-type 选择对应的 HttpMessageConverter 对body中的内容进行转换；  可以通过 Valid 注解对方法参数进行自动验证.这就很明显指出了几个重点，请求应该带body，content-type很重要。所以只要保证带有body体并且content-type能找到对应的HttpMessageConverter就能解析成功。接下来做几个测试。GET 请求方式，传递的值放到请求参数上结果是：提示缺少body体原因： 很明显，请求中没有带body体GET方式，请求中带body体结果： 请求没有body体原因： 我们已经带了body，为什么还报错？上面说过还有一个重要的因素：content-type。在这种方式下的content-type是multipart/form-data 。 而这个content-type并没有对应的 HttpMessageConverter，所有没有进行body解析，依然认为body体是空的。至于什么是multipart/form-data，可参考 multipart/form-data 。默认的HttpMessageConverter 有 10 个 ，每个converter都有自己支持的MediaType 。同理，content-type 为 application/x-www-form-urlencoded 时跟上面结果是一样的。GET方式，content-type 改为 application/json ，结果正确原因：因为application/json 的 content-type 对应到了 MappingJackson2HttpMessageConverter ，该converter将body中的内容转为了对应的实体对象。注：这有一个误区，get不能带body？上述证明，get是可以带body的，而且http规范中也没有规定get不可以带body，只是通常不这么做而已。上述方式改为POST，过程是一致的，只是报错信息变了先看一段代码：1234567    if (body == NO_VALUE)         if (httpMethod == null || !SUPPORTED_METHODS.contains(httpMethod) ||                (noContentType amp;amp; !message.hasBody()))             return null;                throw new HttpMediaTypeNotSupportedException(contentType, this.allSupportedMediaTypes);    在没有解析到body时走的都是这个逻辑，因为 SUPPORTED_METHODS 只包含 POST、PUT、PATCH，所以GET是return null，POST是抛出异常。RequestBody是如何被解析的。其实我们可以猜想一下，尤其是对于Spring这么优秀的框架，命名一定即规范又易懂。既然是解析方法参数，那么这种类大体应该叫MethodArgumentsResolver之类的，然后全局搜索一下，发现果然有这么个接口  HandlerMethodArgumentResolver ，再看下它的实现类，我们发现了 RequestResponseBodyMethodProcessor  。里面有几个方法：12345678910Override    public boolean supportsParameter(MethodParameter parameter)         return parameter.hasParameterAnnotation(RequestBody.class);        Overridepublic boolean supportsReturnType(MethodParameter returnType)     return (AnnotatedElementUtils.hasAnnotation(returnType.getContainingClass(), ResponseBody.class) ||            returnType.hasMethodAnnotation(ResponseBody.class));    一目了然，我们看到了 RequestBody 、ResponseBody 两个类，不就是参数上用的注解么。所以这个类表明它是用来处理带 RequestBody 注解 和 ResponseBody 的方法的。具体的解析逻辑在这个类的 resolveArgument 方法中。**上述原理对应源码流程是：RequestResponseBodyMethodProcessor.resolveArgument()   -gt;  ` readWithMessageConverters() `   -gt;  ` (父类)AbstractMessageConverterMethodArgumentResolver.readWithMessageConverters()` ** . 可自行查阅。参数中直接放实体对象1234RequestMapping(/testRb)    public Person testRb(Person p)         return p;    先说一下测试结论，测试过程就不再截图。这种方式不会报错，只是Person p会接收不到值，它的属性全部为null。能正常接收到的方式有：      GET +  查询参数    GET + body + ContentType(multipart/form-data)  POST + body + ContentType(multipart/form-data)  POST + body + ContentType(application/x-www-form-urlencoded)其他都不行，包括 POST + body + ContentType(application/json)。这一部分还想多说一点，干脆连springMVC整个的请求流程也简单说一下好了。先贴一个完整的调用栈，从中我们可以看到很多有意思的信息：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535454   testRb:24, DemoSpringApplication (io.lxh.demo)53   invoke:-1, GeneratedMethodAccessor32 (sun.reflect)52   invoke:43, DelegatingMethodAccessorImpl (sun.reflect)51   invoke:498, Method (java.lang.reflect)50   doInvoke:189, InvocableHandlerMethod (org.springframework.web.method.support)49   invokeForRequest:138, InvocableHandlerMethod (org.springframework.web.method.support)48   invokeAndHandle:102, ServletInvocableHandlerMethod (org.springframework.web.servlet.mvc.method.annotation)47   invokeHandlerMethod:892, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)46   handleInternal:797, RequestMappingHandlerAdapter (org.springframework.web.servlet.mvc.method.annotation)45   handle:87, AbstractHandlerMethodAdapter (org.springframework.web.servlet.mvc.method)44   doDispatch:1038, DispatcherServlet (org.springframework.web.servlet)43   doService:942, DispatcherServlet (org.springframework.web.servlet)42   processRequest:1005, FrameworkServlet (org.springframework.web.servlet)41   doGet:897, FrameworkServlet (org.springframework.web.servlet)40   service:634, HttpServlet (javax.servlet.http)39   service:882, FrameworkServlet (org.springframework.web.servlet)38   service:741, HttpServlet (javax.servlet.http)37   internalDoFilter:231, ApplicationFilterChain (org.apache.catalina.core)36   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)35   doFilter:53, WsFilter (org.apache.tomcat.websocket.server)34   internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)33   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)32   doFilterInternal:99, RequestContextFilter (org.springframework.web.filter)31   doFilter:107, OncePerRequestFilter (org.springframework.web.filter)30   internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)29   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)28   doFilterInternal:92, FormContentFilter (org.springframework.web.filter)27   doFilter:107, OncePerRequestFilter (org.springframework.web.filter)26   internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)25   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)24   doFilterInternal:93, HiddenHttpMethodFilter (org.springframework.web.filter)23   doFilter:107, OncePerRequestFilter (org.springframework.web.filter)22   internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)21   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)20   doFilterInternal:200, CharacterEncodingFilter (org.springframework.web.filter)19   doFilter:107, OncePerRequestFilter (org.springframework.web.filter)18   internalDoFilter:193, ApplicationFilterChain (org.apache.catalina.core)17   doFilter:166, ApplicationFilterChain (org.apache.catalina.core)16   invoke:200, StandardWrapperValve (org.apache.catalina.core)15   invoke:96, StandardContextValve (org.apache.catalina.core)14   invoke:490, AuthenticatorBase (org.apache.catalina.authenticator)13   invoke:139, StandardHostValve (org.apache.catalina.core)12   invoke:92, ErrorReportValve (org.apache.catalina.valves)11   invoke:74, StandardEngineValve (org.apache.catalina.core)10   service:343, CoyoteAdapter (org.apache.catalina.connector)9   service:408, Http11Processor (org.apache.coyote.http11)8   process:66, AbstractProcessorLight (org.apache.coyote)7   process:834, AbstractProtocolConnectionHandler (org.apache.coyote)6   doRun:1415, NioEndpointSocketProcessor (org.apache.tomcat.util.net)5   run:49, SocketProcessorBase (org.apache.tomcat.util.net)4   runWorker:1149, ThreadPoolExecutor (java.util.concurrent)3   run:624, ThreadPoolExecutorWorker (java.util.concurrent)2   run:61, TaskThreadWrappingRunnable (org.apache.tomcat.util.threads)1   run:748, Thread (java.lang)简单分析一下，这里面的流程分2个部分，一部分是tomcat的：org.apache.catalina；一部分是spring的：org.springframework。相信大家都知道，所有的web流程都是基于filter 和 servlet 的。可以看到tomcat的 ApplicationFilterChain 扮演了过滤链的角色，由它来决定filter链的执行。当所有filter执行完毕后开始执行servlet，也就是spring的 FrameworkServlet （39行）。然后到了大家众所周知的 DispatcherServlet （44行）执行 doDispatch 方法12// Actually invoke the handler.mv = ha.handle(processedRequest, response, mappedHandler.getHandler());从这开始真正的调用handler来处理请求。mv就是ModelAndView。这里先讲2个概念，Handler和HandlerAdapter。Handler 类型 是一个Object ，就是这么简单粗暴，它是为了保证扩展性，顾名思义它就是一个处理者的抽象，用来处理各种请求。HandlerAdapter 用来判断当前请求应该选哪一个Handler。众所周知的 RequestMappingHandlerAdapter 对应的 Handler 是 HandlerMethod。HandlerMethod 顾名思义就是对应一个方法的处理者。回到调用栈上就是第47行，RequestMappingHandlerAdapter.invokeHandlerMethod()，开始调用对应的HandlerMethod，也就是 ServletInvocableHandlerMethod (48行)，接着到了父类 InvocableHandlerMethod 调用 invokeForRequest() 开始真正处理请求。（讲了这么多终于到正题了）invokeForRequest的源码：123456789public Object invokeForRequest(NativeWebRequest request, Nullable ModelAndViewContainer mavContainer,            Object... providedArgs) throws Exception          Object[] args = getMethodArgumentValues(request, mavContainer, providedArgs);        if (logger.isTraceEnabled())             logger.trace(Arguments:  + Arrays.toString(args));                return doInvoke(args);    我们终于找到了解析方法参数的入口，getMethodArgumentValues里面的逻辑就大同小异了，找一个MethodArgumentsResolver 进行参数的解析。这里找到的是 ServletModelAttributeMethodProcessor ，我们再看一下它支持的参数类型12345Override    public boolean supportsParameter(MethodParameter parameter)         return (parameter.hasParameterAnnotation(ModelAttribute.class) ||                (this.annotationNotRequired amp;amp; !BeanUtils.isSimpleProperty(parameter.getParameterType())));    意思是要么是带 ModelAttribute  注解的 或者 不是简单类型的，比如String、Date或者Class对象、int类型等。接着看它的 resolveArgument 方法。首先它创建了一个WebDataBinder （Special DataBinder for data binding from web request parameters to JavaBean objects. ）即从request参数到JavaBean的绑定器。接下来的流程就很简单了，拿到请求参数反射调用对应字段的set方法完成对象构造。所以这种方式总是会返回一个对象的，只是对象里有没有值而已。下面对这部分开始测试的几种情况做下说明：      GET +  查询参数          对应的是RequestParamMethodArgumentResolver。本质就是Request.getParamValue(name)。所以直接取查询参数即可。            ContentType(multipart/form-data)          对于这种ContentType类型，tomcat直接取的是请求体中的内容，所以GET、POST方式都有效。            POST + body + ContentType(application/x-www-form-urlencoded)          tomcat只支持这种类型的POST方式。GET方式返回null。            没有提供对json类型的解析  直接传简单类型的参数，如String、Date等String类型的直接传就好了，这里主要说下Date类型。有以下方式可以完成日期字符串参数到Date类型的转换。  InitBinder1234  InitBinder      public void registerCustomDateEditor(WebDataBinder binder)           binder.registerCustomEditor(Date.class,new CustomDateEditor(new SimpleDateFormat(yyyy-MM-dd),false));      CustomDateEditor  是spring 提供的一个DateEditor。      ConversionService        万能解决方案，使用时间戳Timestamp  ​总结RequestBody 是取请求体中的内容RequestParam 或者 不带任何注解是直接取请求参数中的内容解析方法参数的接口是 MethodArgumentResolver，参数类型转换用的是MessageConverter 和 WebDataBinder 。"
} ,

{
"title"    : "mysql 分组后将某列的值合并成一行",
"category" : "",
"tags"     : "sql, mysql",
"url"      : "/blog/2019/07/24/mysql-%E5%88%86%E7%BB%84%E5%90%8E%E5%B0%86%E6%9F%90%E5%88%97%E7%9A%84%E5%80%BC%E5%90%88%E5%B9%B6%E6%88%90%E4%B8%80%E8%A1%8C/",
"date"     : "2019-07-24 00:00:00 +0800",
"content"  : "有这么个表            ID      NAME                  1      张三              1      李四              2      王五              2      小六      要得到以下结果：            ID      NAME                  1      张三,李四              2      王五,小六      SQL:1select t.id,GROUP_CONCAT(t.name SEPARATOR &#39;,&#39;) from t group by t.id;注意  GROUP_CONCAT  中只有一个参数，是一个完整字符串。假如要在分组中根据某个条件进行过滤，比如原表            ID      NAME      AGE                  1      张三      18              1      李四      25              1      王五      28              2      小六      18              2      小七      22      要得到以下结果：            ID      lt;20的NAME      gt;20的NAME                  1      张三      李四,王五              2      小六      小七      SQL：123456select     t.id,    GROUP_CONCAT(CASE WHEN t.age lt; 20 THEN t.name END SEPARATOR &#39;,&#39; ) ,    GROUP_CONCAT(CASE WHEN t.age gt; 20 THEN t.name END SEPARATOR &#39;,&#39; ) from t group by t.id;"
} ,

{
"title"    : "mysql group by with rollup",
"category" : "",
"tags"     : "mysql, sql",
"url"      : "/blog/2019/07/10/mysql-group-by-with-rollup/",
"date"     : "2019-07-10 00:00:00 +0800",
"content"  : "rollup用于在group by的基础上再进行一次相同的统计源数据：1234567891011mysqlgt; SELECT * FROM employee_tbl;+----+--------+---------------------+--------+| id | name   | date                | singin |+----+--------+---------------------+--------+|  1 | 小明 | 2016-04-22 15:25:33 |      1 ||  2 | 小王 | 2016-04-20 15:25:47 |      3 ||  3 | 小丽 | 2016-04-19 15:26:02 |      2 ||  4 | 小王 | 2016-04-07 15:26:14 |      4 ||  5 | 小明 | 2016-04-11 15:26:40 |      4 ||  6 | 小明 | 2016-04-04 15:26:54 |      2 |+----+--------+---------------------+--------+group by：123456789mysqlgt; SELECT name, COUNT(*) FROM   employee_tbl GROUP BY name;+--------+----------+| name   | COUNT(*) |+--------+----------+| 小丽 |        1 || 小明 |        3 || 小王 |        2 |+--------+----------+3 rows in set (0.01 sec)在上面的基础上再进行with rollup：12345678910mysqlgt; SELECT name, SUM(singin) as singin_count FROM  employee_tbl GROUP BY name WITH ROLLUP;+--------+--------------+| name   | singin_count |+--------+--------------+| 小丽 |            2 || 小明 |            7 || 小王 |            7 || NULL   |           16 |+--------+--------------+"
} ,

{
"title"    : "byte数组转十六进制字符串",
"category" : "",
"tags"     : "java",
"url"      : "/blog/2019/07/10/byte%E6%95%B0%E7%BB%84%E8%BD%AC%E5%8D%81%E5%85%AD%E8%BF%9B%E5%88%B6%E5%AD%97%E7%AC%A6%E4%B8%B2/",
"date"     : "2019-07-10 00:00:00 +0800",
"content"  : "1234567891011121314private static String byteArrayToHex(byte[] byteArray)         // 首先初始化一个字符数组，用来存放每个16进制字符        char[] hexDigits = &#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;, &#39;7&#39;, &#39;8&#39;, &#39;9&#39;, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;;        // new一个字符数组，这个就是用来组成结果字符串的（解释一下：一个byte是八位二进制，也就是2位十六进制字符（2的8次方等于16的2次方））        char[] resultCharArray = new char[byteArray.length * 2];        // 遍历字节数组，通过位运算（位运算效率高），转换成字符放到字符数组中去        int index = 0;        for (byte b : byteArray)             resultCharArray[index++] = hexDigits[b gt;gt;gt; 4 amp; 0xf];            resultCharArray[index++] = hexDigits[b amp; 0xf];                // 字符数组组合成字符串返回        return new String(resultCharArray);    1个byte8位，1个16进制4位。b右移4位得到高4位也就是第一个16进制，b amp; 0xf 得到低4位也就是第二个16进制。"
} ,

{
"title"    : "计算不同单位的时间差",
"category" : "",
"tags"     : "java",
"url"      : "/blog/2019/07/07/%E8%AE%A1%E7%AE%97%E4%B8%8D%E5%90%8C%E5%8D%95%E4%BD%8D%E7%9A%84%E6%97%B6%E9%97%B4%E5%B7%AE/",
"date"     : "2019-07-07 00:00:00 +0800",
"content"  : "12345678910111213141516171819202122232425262728293031public static String getTimeGap(Date startTime, Date endTime)         Duration duration = Duration.between(Instant.ofEpochMilli(startTime.getTime()), Instant.ofEpochMilli(endTime.getTime()));        Period period = Period.ofDays((int) duration.toDays());         long yearGap = period.getYears();        if (yearGap != 0)             return yearGap + 年前;                long monthGap = period.getMonths();        if (monthGap != 0)             return monthGap + 个月前;                long dayGap = period.getDays();        long weekGap = dayGap / 7;        if (weekGap != 0)             return weekGap + 周前;                if (dayGap != 0)             return dayGap + 天前;                long hourGap = duration.toHours();        if (hourGap != 0)             return hourGap + 小时前;                long minuteGap = duration.toMinutes();        if (minuteGap != 0)             return minuteGap + 分钟前;                long secondGap = duration.getSeconds();        return secondGap + 秒前;    Duration：基于时间的时间量，如34.5sPeriod：基于日期的时间量，如2年3个月4天"
} ,

{
"title"    : "SpringBoot项目中使用Swagger2和Spring rest docs 生成Rest API 文档",
"category" : "",
"tags"     : "Swagger, springboot",
"url"      : "/blog/2019/01/29/SpringBoot%E9%A1%B9%E7%9B%AE%E4%B8%AD%E4%BD%BF%E7%94%A8Swagger2%E5%92%8CSpring-rest-docs-%E7%94%9F%E6%88%90Rest-API-%E6%96%87%E6%A1%A3/",
"date"     : "2019-01-29 00:00:00 +0800",
"content"  : "  Swagger官方示例  spring-restdocs官方示例在线文档引入Swagger2的依赖1234567891011121314151617lt;!--swagger相关--gt;        lt;dependencygt;            lt;groupIdgt;io.springfoxlt;/groupIdgt;            lt;artifactIdgt;springfox-swagger2lt;/artifactIdgt;            lt;versiongt;springfox.versionlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;io.springfoxlt;/groupIdgt;            lt;artifactIdgt;springfox-swagger-uilt;/artifactIdgt;            lt;versiongt;springfox.versionlt;/versiongt;        lt;/dependencygt;        lt;dependencygt;            lt;groupIdgt;io.springfoxlt;/groupIdgt;            lt;artifactIdgt;springfox-bean-validatorslt;/artifactIdgt;            lt;versiongt;springfox.versionlt;/versiongt;        lt;/dependencygt;当然也要引入相关的spring-boot-starter，包括test。定义一个Swagger Config：123456789101112131415161718192021222324252627282930313233ConfigurationEnableSwagger2Import(BeanValidatorPluginsConfiguration.class)public class SwaggerConfig     Bean    public Docket restApi()         return new Docket(DocumentationType.SWAGGER_2)                .apiInfo(apiInfo())                .securitySchemes(asList(                        new OAuth(                                petstore_auth,                                asList(new AuthorizationScope(write_pets, modify pets in your account),                                        new AuthorizationScope(read_pets, read your pets)),                                singletonList(new ImplicitGrant(new LoginEndpoint(http://petstore.swagger.io/api/oauth/dialog), tokenName))                        ),                        new ApiKey(api_key, api_key, header)                ))                .select()                .apis(RequestHandlerSelectors.basePackage(head.swagger2markup.controller))                .build();        private ApiInfo apiInfo()         return new ApiInfoBuilder()                .title(Petstore API)                .description(Petstore API在线文档)                .contact(new Contact(head, null, 452228391qq.com))                .license(Apache 2.0)                .licenseUrl(http://www.apache.org/licenses/LICENSE-2.0.html)                .version(1.0.0)                .build();    注意一定要加上EnableSwagger2注解表示启用Swagger2。它的作用是定义Api的overview部分，title、description、contact等等。然后生成一个Docket，定义了需要扫描的API包路径，还加入了安全验证，这里提供了OAuth和ApiKey两种方式，这部分还没好好研究，等以后有时间再看。然后在代码中加入swagger的注解以提供相关说明信息，比如：以Api开头的注解就是swagger相关注解，在类、方法、参数上都有对应注解以提供说明信息。这时访问 http://localhost:8080/swagger-ui.html 就能看到在线文档了，提供了api说明和model实体类说明，点击try it out 还能进行在线测试。Swagger2还提供了对JSR303验证的支持，比如：12 Size(min = 1, max = 100)    private String name;引入：12345        lt;dependencygt;            lt;groupIdgt;io.springfoxlt;/groupIdgt;            lt;artifactIdgt;springfox-bean-validatorslt;/artifactIdgt;            lt;versiongt;springfox.versionlt;/versiongt;        lt;/dependencygt;在SwaggerConfig上加入Import(BeanValidatorPluginsConfiguration.class)，即可在文档中看到相关约束，效果如下：离线文档比如html和pdf格式，spring官方文档就是基于这种格式（asciidoc）。swagger提供了一个url：http://localhost:8080/v2/api-docs ，返回了所有api的信息。      读取这个url，将结果存储到swagger.json：    123456789101112131415    Test    public void createSpringfoxSwaggerJson() throws Exception //        String outputDir = System.getProperty(io.springfox.staticdocs.outputDir);        String outputDir = target/swagger;        MvcResult result = mockMvc.perform(get(/v2/api-docs).accept(MediaType.APPLICATION_JSON))                .andExpect(status().isOk())                .andReturn();          MockHttpServletResponse response = result.getResponse();        String swaggerJson = response.getContentAsString();        Files.createDirectories(Paths.get(outputDir));        try (BufferedWriter writer = Files.newBufferedWriter(Paths.get(outputDir, swagger.json), StandardCharsets.UTF_8))             writer.write(swaggerJson);                        利用swagger2markup-maven-plugin插件读取swagger.json，生成一系列adoc文件。        写一个index.adoc文件，这个文件的内容基本是固定的。    1include::generated/overview.adoc[] include::manual_content1.adoc[] include::manual_content2.adoc[] include::generated/paths.adoc[] include::generated/security.adoc[] include::generated/definitions.adoc[]        它就是用来包含上一步生成的adoc文件的，其中 manual_content1.adoc 和 manual_content2.adoc 是自定义的，采用asciidoc格式，manual_content1.adoc如下：    1234567== 自定义章节 1  This is some dummy text  === 子章节  Dummy text of sub chapter            最后利用asciidoctor-maven-plugin插件将index.adoc文件转成html或pdf。  最终效果如下：此时各个API是没有请求和响应示例的，需要用到spring-restdocs生成的snippets。snippets就是一段代码示例，通过springMvc Test生成。swagger2markup-maven-plugin 会将生成的snippets添加到adoc文件中。步骤如下：加入依赖：1234        lt;dependencygt;            lt;groupIdgt;org.springframework.restdocslt;/groupIdgt;            lt;artifactIdgt;spring-restdocs-mockmvclt;/artifactIdgt;        lt;/dependencygt;测试代码中添加对api的测试，比如这里测试addPet：123456789101112131415AutoConfigureMockMvcAutoConfigureRestDocs(outputDir = target/asciidoc/snippets)SpringBootTestRunWith(SpringRunner.class)public class Swagger2MarkupTest     Autowired    private MockMvc mockMvc;    Test    public void addANewPetToTheStore() throws Exception         this.mockMvc.perform(post(/pets/).content(createPet())                .contentType(MediaType.APPLICATION_JSON))                .andDo(document(addPetUsingPOST, preprocessResponse(prettyPrint())))                .andExpect(status().isOk());    document方法就是生成这个api对应的snippet。最后mvn clean test，因为插件都是绑定到test上的，当然你可以绑定到其他生命周期。效果如下：可以看到请求和响应示例已经添加到最终的文档中了。well,it’s done."
} ,

{
"title"    : "ERR hash value is not an integer",
"category" : "",
"tags"     : "redis",
"url"      : "/blog/2019/01/21/ERR-hash-value-is-not-an-integer/",
"date"     : "2019-01-21 00:00:00 +0800",
"content"  : "在用redis hash结构的increment功能时报了这个错。一般hash结构都是采用JsonSerialize或JdkSerialize来对对象进行序列化，而有时我们不光想存对象，也会存一些简单的值。increment操作不会反序列化而是直接对存储的值进行操作，此时该值经过Json或Jdk序列化后不能直接进行加减操作，因为不能转换为对应的类型如integer、double、long。最简单的办法就是重写JsonSerialize的serialize方法，加上一个判断，如果是String类型，就直接返回String.getBytes()，对象还是走原来的序列化方式。这样既能用序列化方式存储对象，也能用字节存储简单值从而进行加减操作。"
} ,

{
"title"    : "MySQL时区问题",
"category" : "",
"tags"     : "mysql",
"url"      : "/blog/2019/01/16/MySQL%E6%97%B6%E5%8C%BA%E9%97%AE%E9%A2%98/",
"date"     : "2019-01-16 00:00:00 +0800",
"content"  : "相差14或8个小时解决方案：在mysql连接字符串后加上 serverTimezone=GMT2b8  ，表示使用 GMT+8北京时间1jdbc:mysql://ip:3306/seckiluseUnicode=trueamp;characterEncoding=utf8amp;useSSL=falseamp;serverTimezone=GMT2b8或者，进入mysql，执行  show variables like ‘time_zome’   查看当前时区123456+------------------+--------+| Variable_name    | Value  |+------------------+--------+| system_time_zone | CST    || time_zone        | SYSTEM |+------------------+--------+修改time_zoneset global time_zone=’+08:00’;set time_zone = ‘+08:00’;再次查看：123456+------------------+--------+| Variable_name    | Value  |+------------------+--------+| system_time_zone | CST    || time_zone        | +08:00 |+------------------+--------+ResponseBody返回对象时时间字段时结果会相差14或8个小时解决方案：spring boot使用Jackson进行对象的序列化，需要我们手动配置Jackson的时区12spring.jackson.date-format=yyyy-MM-dd HH:mm:ssspring.jackson.time-zone=GMT+8时区概念：  GMT：Greenwich Mean Time 格林威治时间  CST: China Standard Time 中国标准时间  UTC: 世界标准时间GMT+8=UTC+8=CST"
} ,

{
"title"    : "JMeter中cookie未随请求一起发送",
"category" : "",
"tags"     : "jmeter, 测试",
"url"      : "/blog/2019/01/16/JMeter%E4%B8%ADcookie%E6%9C%AA%E9%9A%8F%E8%AF%B7%E6%B1%82%E4%B8%80%E8%B5%B7%E5%8F%91%E9%80%81/",
"date"     : "2019-01-16 00:00:00 +0800",
"content"  : "可看到 Cookie Policy那里有很多种策略可选，默认是standard，我这里换成了netscape才生效，反正发现不起作用时各种换着试一试。下面是各种策略的说明：      Compatibility          这种兼容性设计要求是适应尽可能多的不同的服务器，尽管不是完全按照标准来实现的。如果你遇到了解析Cookies的问题，你就可能要用到这一个规范。有太多的web站点是用CGI脚本去实现的，而导致只有将所有的Cookies都放入Request header才可以正常的工作。这种情况下最好设置http.protocol.single-cookie-header参数为true。            RFC2965          RFC2965定义了版本2并且尝试去弥补在版本1中Cookie的RFC2109标准的缺点。RFC2965是，并规定RFC2965最终取代RFC2109.发送RFC2965标准Cookies的服务端，将会使用Set-Cookie2 header添加到Set-Cookie Header信息中，RFC2965 Cookies是区分端口的。            Ignore Cookies          此规格忽略所有Cookie 。被用来防止HttpClient接受和发送的Cookie。            Netscape          Netscape是最原始的Cookies规范，同时也是RFC2109的基础。尽管如此，还是在很多重要的方面与RFC2109不同，可能需要特定服务器才可以兼容。            RFC2109          RFC2109是W3C组织第一次推出的官方Cookies标准。理论上，所有使用版本1Cookies的服务端都应该使用此标准。HttpClient已经将此标准设定为默认。      遗憾的是，许多服务端不正确的实现了标准或者仍然使用Netscape标准。所以有时感到此标准太多于严格。RFC2109是HttpClient使用的默认Cookies协议。      "
} 

]